{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkkr1HUBPOcL"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.983 · Aprenentage per reforç</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2024-1 · Màster universitari en Ciència de dades (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis d'Informàtica, Multimèdia i Telecomunicació</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Nom i cognoms:</strong> Xavier Maltas Tarridas\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFR_J-YYXLx-"
   },
   "source": [
    "# PAC 1 - Solucions tabulars\n",
    "\n",
    "En aquesta pràctica implementarem els diferents mètodes d'aprenentatge per reforç estudiats als Blocs I i II del curs. En concret, ens centrarem en la definició d'un entorn i implementarem els diferents mètodes per buscar una solució òptima del problema.\n",
    "\n",
    "**<u>Important</u>: El lliurament s'ha de fer en format notebook i en format html on es vegi el codi, els resultats i comentaris de cada exercici. És a dir, s'han de lliurar dos fitxers: un amb extensió .ipynb i un altre .html. Per exportar el notebook a html pot fer-se des del menú File $\\to$ Download as $\\to$ HTML. A més s'han de lliurar els 2 arxius que es modificaran dins de la carpeta <code>\\gym_gridworlds</code>, els arxius <code>gridworld.py</code> i <code>\\_\\_init\\_\\_.py</code>.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyFUy1qlDGku"
   },
   "source": [
    "## 1. L'entorn __Gym-Gridworlds__ (1 punt)\n",
    "\n",
    "L'entorn __Gym-Gridworlds__ és un entorn de tercers que es troba referenciat a la web de Gymnasium, concretament a l'apartat 'ENVIRONMENTS/External Environments'.\n",
    "\n",
    "L'entorn consisteix en un agent que es mou en una quadrícula de dimensions configurables. La classe per defecte `Gridworld` implementa una tasca \"anar a l'objectiu\" on l'agent té cinc accions (esquerra, dreta, amunt, avall, romandre al lloc) i una funció de transició predeterminada (seleccionar l'acció \"romandre al lloc\" als estats objectiu acaba l'episodi).\n",
    "\n",
    "El codi i la documentació es troba a https://github.com/sparisi/gym_gridworlds\n",
    "\n",
    "Es recomana llegir atentament la pàgina per familiaritzar-se amb l'entorn.\n",
    "\n",
    "**<u>Nota</u>: L'entorn utilitzat té un mode de renderitzat <code>render\\_mode = \"human\"</code> que permet observar la quadrícula i el moviment de l'agent en una pantalla adicional utiltzant el paquet <code>pygame</code> de python. Aquest mode de renderitzat només funciona en local. Per a poder executar el codi sense problemes es proporciona un arxiu <code>environment.yml</code> perquè pugueu crear un entorn virtual amb els paquets necessaris.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBkO2FuFJdH-"
   },
   "source": [
    "Començarem carregant un dels múltiples entorns pre-dissenyats i veurem les seves principals característiques, executant un episodi de prova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBrIWvZmDB6H"
   },
   "source": [
    "### 1.1. Càrrega de dades (0.5 punts)\n",
    "\n",
    "El següent codi carrega els paquets necessaris per a l'exemple.\n",
    "\n",
    "Comencem instal·lant Gymnasium (això només s'ha de fer una vegada si no ho teniu instal·lat ja)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7035,
     "status": "ok",
     "timestamp": 1729206552620,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "qMuA4JjZXLx_",
    "outputId": "99100ad3-caf9-4cfe-9636-d02536659505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium==1.0.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La versió de Gymnasium utilitzada en aquesta PAC és la 1.0.0, comprova-ho a continuació\n",
      "Gymnasium Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(\"La versió de Gymnasium utilitzada en aquesta PAC és la 1.0.0, comprova-ho a continuació\")\n",
    "print(\"Gymnasium Version:\", gym.__version__) # 0.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuació clonem el repositori de l'entorn (això només s'ha de fer una vegada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1729206566707,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "81RCuLEwej1O",
    "outputId": "f665789d-788c-4505-a597-cdef0d02e924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'gym_gridworlds' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sparisi/gym_gridworlds.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrem dins del directori `gym_gridworlds` i l'instal·lem en mode editable (això només s'ha de fer una vegada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1729206575365,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "7ne58OSTe2Sa",
    "outputId": "ac759fe7-3aa5-4afa-80b9-c4ed9492dae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavim\\Desktop\\myRepos\\ReinforcementLearning\\PAC1\\gym_gridworlds\n",
      "Obtaining file:///C:/Users/xavim/Desktop/myRepos/ReinforcementLearning/PAC1/gym_gridworlds\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: gymnasium in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from Gym-Gridworlds==1.0) (1.0.0)\n",
      "Requirement already satisfied: pygame in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from Gym-Gridworlds==1.0) (2.6.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium->Gym-Gridworlds==1.0) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium->Gym-Gridworlds==1.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium->Gym-Gridworlds==1.0) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium->Gym-Gridworlds==1.0) (0.0.4)\n",
      "Installing collected packages: Gym-Gridworlds\n",
      "  Attempting uninstall: Gym-Gridworlds\n",
      "    Found existing installation: Gym-Gridworlds 1.0\n",
      "    Uninstalling Gym-Gridworlds-1.0:\n",
      "      Successfully uninstalled Gym-Gridworlds-1.0\n",
      "  Running setup.py develop for Gym-Gridworlds\n",
      "Successfully installed Gym-Gridworlds-1.0\n"
     ]
    }
   ],
   "source": [
    "%cd gym_gridworlds\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7016,
     "status": "ok",
     "timestamp": 1729206587859,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "R3t9AqNLZ549",
    "outputId": "d3c79fd1-df15-46fc-8ad3-843090119b33"
   },
   "source": [
    "Importem els paquets necessaris per la resta de la PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1729206594496,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "h5y5sIqBPPxr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import time\n",
    "import gym_gridworlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem l'entorn mitjançant la instanciació d'un objecte de classe `Gridworld` (en executar la instrucció `env = gym.make(\"Gym-Gridworlds/Full-4x5-v0\", render_mode=\"human\")`) i imprimim per pantalla el tipus de l'espai d'accions i de l'espai d'observacions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1729206611289,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "ej5qlR-OaTLi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(5) \n",
      "Observation space is Discrete(20) \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Full-4x5-v0\", render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entorn generat té el següent aspecte (comprovar-ho a la finestra emergent de <code>pygame</code> que es genera a l'executar <code>env.render()</code>):\n",
    "<br><br>\n",
    "<img src=\"images/Gym-Gridworld_4x5.png\" alt=\"Mi imagen\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "En aquest entorn podem veure una sèrie de caselles i figures que es descriuen a continuació:\n",
    "\n",
    "<ul>\n",
    "  <li>Les caselles negres són buides.</li>\n",
    "  <li>Les caselles negres amb fletxes grises són caselles on l'agent només es pot moure en una direcció (les altres accions fallaran).</li>\n",
    "  <li>Les caselles vermelles donen recompenses negatives (com més brillants, més negatives).</li>\n",
    "  <li>Les caselles verdes donen recompenses positives (com més brillants, més altes).</li>\n",
    "  <li>Les caselles grogues són arenes movedisses, on totes les accions fallaran amb un 90% de probabilitat.</li>\n",
    "  <li>L'agent és el cercle blau.</li>\n",
    "</ul>\n",
    "\n",
    "A més, hi ha altres tipus de caselles:\n",
    "\n",
    "<ul>\n",
    "  <li>Les caselles blanques són pous (caminar-hi comporta una gran recompensa negativa i l'episodi acaba).</li>\n",
    "  <li>Les caselles porpres són parets (l'agent no hi pot passar).</li>\n",
    "  <li>Una fletxa taronja indica l'última acció de l'agent.</li>\n",
    "  <li>Un punt taronja indica que l'agent no va intentar moure's amb la seva última acció.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 1.1</strong> (0.5 pts)\n",
    "\n",
    "Analitzar la [documentació](https://github.com/sparisi/gym_gridworlds) i el codi de l'entorn que es troba a <code>\\gym_gridworlds\\gridworld.py</code> per respondre a les següents preguntes:\n",
    "<ul>    \n",
    "    <li>Descriure l'espai d'accions de l'entorn: quantes accions hi ha? a què correspòn cadascuna d'elles?</li>\n",
    "    <li>Quina és la casella inicial? Es pot canviar?</li>\n",
    "</ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br>\n",
    "    \n",
    "    \n",
    "<h2>Espai d'accions de l'entorn</h2>\n",
    "<p><strong>Quantes accions hi ha?</strong></p>\n",
    "<p>L'espai d'accions és discret amb un total de 5 accions, representades per nombres enters en el rang <code>{0, 1, 2, 3, 4}</code>.</p>\n",
    "\n",
    "<p><strong>A què correspon cadascuna d'elles?</strong></p>\n",
    "<ul>\n",
    "  <li><code>0</code>: Moure's a l'esquerra (LEFT)</li>\n",
    "  <li><code>1</code>: Moure's cap avall (DOWN)</li>\n",
    "  <li><code>2</code>: Moure's a la dreta (RIGHT)</li>\n",
    "  <li><code>3</code>: Moure's cap amunt (UP)</li>\n",
    "  <li><code>4</code>: Quedar-se quiet (STAY)</li>\n",
    "</ul>\n",
    "\n",
    "    \n",
    "    \n",
    "<h2>Casella inicial</h2>\n",
    "<p><strong>Quina és la casella inicial?</strong></p>\n",
    "<p>Per defecte, en aquest entorn, la casella inicial és la cantonada superior esquerra <code>(0, 0)</code>.</p>\n",
    "\n",
    "<p><strong>Es pot canviar?</strong></p>\n",
    "<p>Sí, és possible canviar la casella inicial. Pots utilitzar classes predefinides com <code>GridworldMiddleStart</code> per començar al centre, o <code>GridworldRandomStart</code> per una posició aleatòria.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXpl0ErhE_I1"
   },
   "source": [
    "### 1.2. Execució d'un episodi (0.5 punts)\n",
    "\n",
    "A continuació, realitzarem l'execució d'un episodi de l'entorn *Gym-Gridworlds/Full-4x5-v0* utilitzant un agent que selecciona les accions de forma aleatòria.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "84mPbgBGE1zz",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs inicial: 0 \n",
      "Step 1. Action: Left -> Obs: 0 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 2. Action: Right -> Obs: 1 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 3. Action: Down -> Obs: 6 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 4. Action: Right -> Obs: 7 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 5. Action: Right -> Obs: 8 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 6. Action: Up -> Obs: 3 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 7. Action: Stay -> Obs: 3 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 8. Action: Stay -> Obs: 3 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 9. Action: Right -> Obs: 4 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 10. Action: Up -> Obs: 4 and reward: 0. Terminated False. Truncated False.\n",
      "Step 11. Action: Up -> Obs: 4 and reward: 0. Terminated False. Truncated False.\n",
      "Step 12. Action: Up -> Obs: 4 and reward: 0. Terminated False. Truncated False.\n",
      "Step 13. Action: Up -> Obs: 4 and reward: 0. Terminated False. Truncated False.\n",
      "Step 14. Action: Left -> Obs: 3 and reward: 0. Terminated False. Truncated False.\n",
      "Step 15. Action: Up -> Obs: 3 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 16. Action: Down -> Obs: 8 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 17. Action: Down -> Obs: 13 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 18. Action: Up -> Obs: 8 and reward: 0. Terminated False. Truncated False.\n",
      "Step 19. Action: Up -> Obs: 3 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 20. Action: Up -> Obs: 3 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 21. Action: Left -> Obs: 2 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 22. Action: Down -> Obs: 7 and reward: 0. Terminated False. Truncated False.\n",
      "Step 23. Action: Stay -> Obs: 7 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 24. Action: Right -> Obs: 8 and reward: -10.0. Terminated False. Truncated False.\n",
      "Step 25. Action: Down -> Obs: 13 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 26. Action: Down -> Obs: 18 and reward: 0. Terminated False. Truncated False.\n",
      "Step 27. Action: Up -> Obs: 13 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 28. Action: Right -> Obs: 14 and reward: 0. Terminated False. Truncated False.\n",
      "Step 29. Action: Down -> Obs: 19 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 30. Action: Down -> Obs: 19 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 31. Action: Right -> Obs: 19 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 32. Action: Up -> Obs: 14 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 33. Action: Up -> Obs: 9 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 34. Action: Stay -> Obs: 9 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 35. Action: Up -> Obs: 4 and reward: 0.0. Terminated False. Truncated False.\n",
      "Step 36. Action: Stay -> Obs: 4 and reward: 1.0. Terminated True. Truncated False.\n",
      "Episode finished after 36 timesteps and reward was -99.0 \n"
     ]
    }
   ],
   "source": [
    "# Inicialitzem l'entorn\n",
    "obs, info = env.reset()\n",
    "t, total_reward, terminated, truncated = 0, 0, False, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"Left\",\n",
    "        1: \"Down\",\n",
    "        2: \"Right\",\n",
    "        3: \"Up\",\n",
    "        4: \"Stay\",\n",
    "    }\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    # Triar una acció aleatòria (aquesta és la implementació de l'agent)\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Executar l'acció i esperar la resposta de l'entorn\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    # Imprimir time-step\n",
    "    print(\"Step {}. Action: {} -> Obs: {} and reward: {}. Terminated {}. Truncated {}.\".format(t+1, switch_action[action], new_obs, reward, terminated, truncated))\n",
    "\n",
    "    # Actualitzar variables\n",
    "    obs = new_obs\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    time.sleep(0.5) #S'afegeix per alentir el renderitzat i poder apreciar els moviments de l'agent\n",
    "\n",
    "print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es recomana analitzar el codi anterior, executar-lo vàries vegades i observar el renderitzat en la finestra emergent externa abans de contestar el següent exercici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 1.2</strong> (0.5 pts)\n",
    "\n",
    "<!-- Una vegada executat l'episodi, i en base a l'anàlisi de la documentació i del codi de l'entorn respondre a les següents preguntes: -->\n",
    "<ul> \n",
    "    <li>Descriure l'espai d'estats: en què consisteixen els estats? com es codifiquen per defecte?</li>\n",
    "    <li>Descriure el senyal de recompensa: quins valors pot prendre? en quines situacions es rep cada valor?</li>\n",
    "    <li>Quan finalitza un episodi?</li>\n",
    "</ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br>\n",
    "\n",
    "    \n",
    "<h2>Espai d'estats</h2>\n",
    "<p><strong>En què consisteixen els estats</strong></p>\n",
    "<p>L'espai d'estats en l'entorn \"Gym-Gridworlds/Full-4x5-v0\" es basa en la posició de l'agent dins d'una graella de 4x5. Cada estat es troba representat segons la posició de l'agent a la graella, la qual es descriu mitjançant un conjunt de coordenades <code>(x,y)</code>, on <code>x</code> és la fila i <code>y</code> és la columna de la graella.</p>\n",
    "\n",
    "<p><strong>Com es codifiquen per defecte?</strong></p>\n",
    "    <p>Cada estat es codifica mitjançant una parella coordenades representades per nombres enters <code>(x,y)</code>. La <code>x</code> és la fila (entre 0 i 3) i <code>y</code> és la columna (entre 0 i 4) de la graella. Això permet representar fins a 20 possibles estats (4 files * 5 columnes).</p>\n",
    "    \n",
    "<h2>Senyal de recompensa</h2>\n",
    "<p><strong>Quins valors pot prendre?</strong></p>\n",
    "<ul>\n",
    "  <li><strong>+1</strong>: Recompensa màxima per arribar a la casella d'objectiu (<code>GOOD</code>) i executar l'acció <code>STAY</code>.</li>\n",
    "  <li><strong>+0.1</strong>: Recompensa menor per arribar a una casella de distracció (<code>GOOD_SMALL</code>) i executar l'acció <code>STAY</code>.</li>\n",
    "  <li><strong>-10</strong>: Penalització per moure's a una casella de penalització gran (<code>BAD</code>).</li>\n",
    "  <li><strong>-0.1</strong>: Penalització menor per moure's a una casella de penalització menor (<code>BAD_SMALL</code>).</li>\n",
    "  <li><strong>-100</strong>: Penalització severa per caure en una casella de <forat (<code>PIT</code>).</li>\n",
    "  <li><strong>0</strong>: Recompensa nul·la en caselles buides (<code>EMPTY</code>) o si l'agent xoca amb una <em>paret</em> (<code>WALL</code>).</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>En quines situacions es rep cada valor?</strong></p>\n",
    "<ul>\n",
    "  <li><strong>+1</strong>: Quan l'agent arriba a l'objectiu principal (<code>GOOD</code>) i tria quedar-se quiet (<code>STAY</code>).</li>\n",
    "  <li><strong>+0.1</strong>: Quan l'agent arriba a un objectiu distractor(<code>GOOD_SMALL</code>) i tria quedar-se quiet (<code>STAY</code>).</li>\n",
    "  <li><strong>-10</strong>: Quan l'agent entra en una casella amb penalització gran (<code>BAD</code>).</li>\n",
    "  <li><strong>-0.1</strong>: Quan l'agent es mou cap a una casella amb penalització menor (<code>BAD_SMALL</code>).</li>\n",
    "  <li><strong>-100</strong>: Quan l'agent cau en una casella de forat (<code>PIT</code>), finalitzant l'episodi.</li>\n",
    "  <li><strong>0</strong>: Quan l'agent es mou a una casella buida (<code>EMPTY</code>) o xoca amb una paret (<code>WALL</code>), o si no es queda quiet després d'arribar a un objectiu (<code>GOOD</code> o <code>GOOD_SMALL</code>).</li>\n",
    "</ul>\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "<h2>Finalització de l'episodi</h2>\n",
    "<p><strong>Quan finalitza un episodi?</strong></p>\n",
    "<p>\n",
    "  L'episodi es completa en les situacions següents:\n",
    "  <ul>\n",
    "    <li>Quan l'agent arriba a la casella d'objectiu (<em>GOOD</em>) i executa l'acció <em>STAY</em>, rebent una recompensa de +1.</li>\n",
    "    <li>Quan l'agent cau en una casella de forat (<em>PIT</em>), rebent una penalització de -100.</li>\n",
    "  </ul>\n",
    "</p>\n",
    "\n",
    "\n",
    "<h2>Finalització de l'episodi</h2>\n",
    "<p><strong>Quan finalitza un episodi?</strong></p>\n",
    "<ul>\n",
    "  <li>L'episodi es finalitza immediatament si l'agent rep una recompensa de <strong>+1</strong> en arribar a la casella objectiu principal (<code>GOOD</code>) i executar l'acció <code>STAY</code>.</li>\n",
    "  <li>També finalitza si l'agent cau en una casella de forat (<code>PIT</code>) i rep una recompensa de <strong>-100</strong>.</li>\n",
    "</ul>\n",
    "    \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanquem l'entorn perquè als següents apartats en crearem un de nou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc_f31JOkh2d"
   },
   "source": [
    "## 2. Creació d'un entorn propi (1,5 punts)\n",
    "\n",
    "L'entorn *Gym-Gridworld* té diversos arguments que poden ser modificats:\n",
    "\n",
    "* La dimensió de la quadrícula.\n",
    "* El tipus de cada casella.\n",
    "* La posició de les caselles de sortida i d'arribada.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WniTr5sdXLyC"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.1</strong> (0,75 punts)\n",
    "\n",
    "Crear un entorn nou com el que es descriu a continuació:\n",
    "\n",
    "<ul>\n",
    "  <li>Graella 4x4</li>\n",
    "  <li>Les 4 caselles centrals son de tipus paret (color porpra).</li>\n",
    "  <li>Casella d'inici a dalt a l'esquerra (posició per defecte).</li>\n",
    "  <li>Casella final (color verd brillant) a baix a la dreta.</li>\n",
    "  <li>La casella de la dreta del tot de la primera fila que sigui amb penalització gran (color vermell brillant).</li>\n",
    "  <li>La resta de caselles, en comptes de ser normals (color negre) posar-les amb una petita penalització (color vermell fosc).</li>\n",
    "</ul>\n",
    "\n",
    "Per a realitzar l'entorn:\n",
    "<ul>\n",
    "  <li>Heu d'afegir la graella al diccionari <code>GRIDS</code> de l'arxiu <code>\\gym_gridworlds\\gridworld.py</code> (utilitzeu com a clau de l'entrada al diccionari <code>\"4X4_Ex2\"</code>.)</li>\n",
    "  <li>Heu de registrar l'entorn a l'arxiu <code>\\gym_gridworlds\\__init__.py</code> amb les següents dades:</li>\n",
    "      <ul>\n",
    "          <li><code>id=\"Gym-Gridworlds/Ex2-4x4-v0\"</code></li>\n",
    "          <li><code>entry_point=\"gym_gridworlds.gridworld:Gridworld\"</code></li>\n",
    "          <li><code>max_episode_steps=100</code></li>\n",
    "          <li><code>kwargs={\"grid\": \"4X4_Ex2\",}</code></li>\n",
    "       </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vegada afegit i registrat el nou entorn re-inicieu el kernel del Notebook per a que es pugui carregar i executar.\n",
    "\n",
    "Si tot ha anat bé, després d'executar el següent codi, hauríeu d'obtenir un entorn amb el següent aspecte:\n",
    "<br><br>\n",
    "<img src=\"images/Gym-Gridworld_Ex2_4x4.png\" alt=\"Mi imagen\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZIKcFYi-IX-C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(5) \n",
      "Observation space is Discrete(16) \n"
     ]
    }
   ],
   "source": [
    "import gym_gridworlds\n",
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#gridworld.py\n",
    "#new grid\n",
    "\n",
    "GRIDS = {\n",
    "\n",
    "    \"4X4_Ex2\": [\n",
    "        [BAD_SMALL, BAD_SMALL, BAD_SMALL, BAD],\n",
    "        [BAD_SMALL, WALL, WALL, BAD_SMALL],\n",
    "        [BAD_SMALL, WALL, WALL, BAD_SMALL],\n",
    "        [BAD_SMALL, BAD_SMALL, BAD_SMALL, GOOD],\n",
    "    ],\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#__init__.py\n",
    "#Register env\n",
    "\n",
    "register(\n",
    "    id=\"Gym-Gridworlds/Ex2-4x4-v0\",\n",
    "    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n",
    "    max_episode_steps=100,\n",
    "    kwargs={\"grid\": \"4X4_Ex2\"},\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AVvZVSqIggI"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.2</strong> (0,25 punts)\n",
    "\n",
    "A continuació, implementar un agent que dugui a terme una política aleatòria. Comprovar que les caselles visitades i les recompenses rebudes es corresponen amb les accions i l'entorn programat.\n",
    "\n",
    "Mostrar la trajectòria seguida per l'agent. No cal graficar-la, tan sols mostrar les accions i les caselles visitades en ordre i les recompenses rebudes.\n",
    "\n",
    "Mostrar també el nombre de *steps* i la recompensa total acumulada.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ksk8N63OHpEO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs inicial: 0 \n"
     ]
    }
   ],
   "source": [
    "# Inicialitzem l'entorn\n",
    "obs, info = env.reset()\n",
    "t, total_reward, terminated, truncated = 0, 0, False, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"Left\",\n",
    "        1: \"Down\",\n",
    "        2: \"Right\",\n",
    "        3: \"Up\",\n",
    "        4: \"Stay\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Q1B0qogIJB2z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Action: Right, Obs: 1, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 2: Action: Right, Obs: 2, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 3: Action: Stay, Obs: 2, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 4: Action: Down, Obs: 2, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 5: Action: Right, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 6: Action: Right, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 7: Action: Down, Obs: 7, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 8: Action: Stay, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 9: Action: Stay, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 10: Action: Up, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 11: Action: Right, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 12: Action: Stay, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 13: Action: Down, Obs: 7, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 14: Action: Right, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 15: Action: Right, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 16: Action: Up, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 17: Action: Up, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 18: Action: Stay, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 19: Action: Stay, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 20: Action: Down, Obs: 7, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 21: Action: Down, Obs: 11, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 22: Action: Stay, Obs: 11, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 23: Action: Up, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 24: Action: Stay, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 25: Action: Up, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 26: Action: Up, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 27: Action: Stay, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 28: Action: Up, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 29: Action: Up, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 30: Action: Right, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 31: Action: Stay, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 32: Action: Right, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 33: Action: Right, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 34: Action: Stay, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 35: Action: Up, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 36: Action: Left, Obs: 2, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 37: Action: Right, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 38: Action: Left, Obs: 2, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 39: Action: Right, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 40: Action: Up, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 41: Action: Down, Obs: 7, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 42: Action: Up, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 43: Action: Left, Obs: 2, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 44: Action: Left, Obs: 1, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 45: Action: Left, Obs: 0, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 46: Action: Right, Obs: 1, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 47: Action: Stay, Obs: 1, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 48: Action: Right, Obs: 2, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 49: Action: Right, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 50: Action: Right, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 51: Action: Stay, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 52: Action: Up, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 53: Action: Stay, Obs: 3, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 54: Action: Left, Obs: 2, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 55: Action: Down, Obs: 2, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 56: Action: Right, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 57: Action: Down, Obs: 7, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 58: Action: Stay, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 59: Action: Up, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 60: Action: Left, Obs: 2, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 61: Action: Stay, Obs: 2, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 62: Action: Stay, Obs: 2, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 63: Action: Down, Obs: 2, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 64: Action: Right, Obs: 3, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 65: Action: Down, Obs: 7, Reward: -10.0, Terminated: False, Truncated: False\n",
      "Step 66: Action: Right, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 67: Action: Right, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 68: Action: Stay, Obs: 7, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 69: Action: Down, Obs: 11, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 70: Action: Left, Obs: 11, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 71: Action: Stay, Obs: 11, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 72: Action: Right, Obs: 11, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 73: Action: Down, Obs: 15, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 74: Action: Left, Obs: 14, Reward: 0, Terminated: False, Truncated: False\n",
      "Step 75: Action: Stay, Obs: 14, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 76: Action: Stay, Obs: 14, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 77: Action: Left, Obs: 13, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 78: Action: Right, Obs: 14, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 79: Action: Right, Obs: 15, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 80: Action: Right, Obs: 15, Reward: 0, Terminated: False, Truncated: False\n",
      "Step 81: Action: Stay, Obs: 15, Reward: 1.0, Terminated: True, Truncated: False\n",
      "\n",
      "--- Episode Summary ---\n",
      "Total number of steps: 81\n",
      "Total reward: -323.60000000000036\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "import gymnasium as gym\n",
    "\n",
    "# List to store the trajectory\n",
    "trajectory = []\n",
    "\n",
    "# Interaction loop with the environment\n",
    "while not (terminated or truncated):\n",
    "    # Select a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Execute the action and get new observations and rewards\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Store the information of the performed action\n",
    "    trajectory.append({\n",
    "        \"step\": t + 1,\n",
    "        \"action\": switch_action[action],\n",
    "        \"obs\": next_obs,\n",
    "        \"reward\": reward\n",
    "    })\n",
    "    \n",
    "    # Update reward variables and step count\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    \n",
    "    # Display information of the current step\n",
    "    print(f\"Step {t}: Action: {switch_action[action]}, Obs: {next_obs}, Reward: {reward}, Terminated: {terminated}, Truncated: {truncated}\")\n",
    "    time.sleep(0.25)\n",
    "    \n",
    "# Display the final summary\n",
    "print(\"\\n--- Episode Summary ---\")\n",
    "print(f\"Total number of steps: {t}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVDVy2tIXLyD"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Pregunta</strong>\n",
    "Quin pot ser l'ojectiu d'utilitzar caselles de collor vermell fosc (amb una petita penalització) en comptes de caselles negres (sense penalització) en l'entrenament dels agents?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttFYwsVtXLyE"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "\n",
    "1. **Eficiència**: A l'utilitzar penalitzacions petites es fomenta que l'agent eviti trajectòries ineficients. En lloc de moure's aleatòriament sense cap conseqüència, l'agent aprèn a minimitzar el cost acumulat per arribar a la meta/objectiu trobant la ruta més curta i eficient. \n",
    "\n",
    "    \n",
    "\n",
    "2. **Simular entorn més realista**: En molts escenaris del món real, moure's per un espai té un cost associat (consum d'energia, temps o altres recursos). En fer ús de petites penalitzacions per a les caselles normals pot ajudar a simular aquestes condicions, fent que l'agent aprengui a prendre decisions més realistes.\n",
    "\n",
    "    \n",
    "\n",
    "3. **Millorar l'aprenentatge**: Les petites penalitzacions poden proporcionar un senyal d'aprenentatge de major riquesa, ajudant l'agent a diferenciar entre accions lleugerament millors o pitjors. Això pot accelerar el procés d'aprenentatge i millorar la qualitat  de les polítiques apreses.\n",
    "\n",
    "    \n",
    "\n",
    "4. **Exploració controlada**: Fer ús de penalitzacions petites pot ajudar a controlar l'exploració de l'agent, fent que eviti certes àrees de l'entorn que no són òptimes. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVDVy2tIXLyD"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.3</strong> (0,5 punts)\n",
    "\n",
    "A continuació, implementar un agent que dugui a terme la política òptima determinista, és a dir, que partint de la casella inicial [0,0] arribi a la casella final (de color verd brillant) amb la màxima recompensa acumulada. Quin és el valor del nombre de passos mínims? Quin és el retorn obtingut?\n",
    "\n",
    "Mostra la trajectòria seguida per l'agent i el retorn obtingut. No cal graficar-la, tan sols mostrar les accions i les caselles visitades en ordre i el retorn (recompensa total acumulada).\n",
    "\n",
    "Comenta els resultats.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "07kPrBwtXLyD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs inicial: 0 \n"
     ]
    }
   ],
   "source": [
    "# Inicialitzem l'entorn\n",
    "obs, info = env.reset()\n",
    "t, total_reward, terminated, truncated = 0, 0, False, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"Left\",\n",
    "        1: \"Down\",\n",
    "        2: \"Right\",\n",
    "        3: \"Up\",\n",
    "        4: \"Stay\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FkX27sGtXLyD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: 0 \n",
      "Step 1: Action: Down, Obs: 4, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 2: Action: Down, Obs: 8, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 3: Action: Down, Obs: 12, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 4: Action: Right, Obs: 13, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 5: Action: Right, Obs: 14, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 6: Action: Right, Obs: 15, Reward: -0.1, Terminated: False, Truncated: False\n",
      "Step 7: Action: Stay, Obs: 15, Reward: 1.0, Terminated: True, Truncated: False\n",
      "\n",
      "--- Episode Summary ---\n",
      "Total number of steps: 7\n",
      "Total reward: 0.4\n",
      "-----------------------\n",
      "\n",
      "Step 1: Action: Down, Observation: 4, Reward: -0.1\n",
      "Step 2: Action: Down, Observation: 8, Reward: -0.1\n",
      "Step 3: Action: Down, Observation: 12, Reward: -0.1\n",
      "Step 4: Action: Right, Observation: 13, Reward: -0.1\n",
      "Step 5: Action: Right, Observation: 14, Reward: -0.1\n",
      "Step 6: Action: Right, Observation: 15, Reward: -0.1\n",
      "Step 7: Action: Stay, Observation: 15, Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "import gymnasium as gym\n",
    "\n",
    "# Optimal deterministic policy to reach the final cell with maximum reward\n",
    "optimal_policy = [\n",
    "    (1, \"Down\"), (1, \"Down\"),(1, \"Down\"), \n",
    "    (2, \"Right\"), (2, \"Right\"), (2, \"Right\"), \n",
    "    (4, \"Stay\")\n",
    "]\n",
    "\n",
    "print(\"Initial observation: {} \".format(obs))\n",
    "\n",
    "# List to store the trajectory\n",
    "trajectory = []\n",
    "\n",
    "# Execute the optimal policy\n",
    "for action_code, action_name in optimal_policy:\n",
    "    # Execute the action and get new observations and rewards\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action_code)\n",
    "    \n",
    "    # Store the information of the performed action\n",
    "    trajectory.append({\n",
    "        \"step\": t + 1,\n",
    "        \"action\": action_name,\n",
    "        \"obs\": next_obs,\n",
    "        \"reward\": reward\n",
    "    })\n",
    "    \n",
    "    # Update reward variables and step count\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Display information of the current step\n",
    "    print(f\"Step {t}: Action: {action_name}, Obs: {next_obs}, Reward: {reward}, Terminated: {terminated}, Truncated: {truncated}\")\n",
    "    \n",
    "# Display the final summary\n",
    "print(\"\\n--- Episode Summary ---\")\n",
    "print(f\"Total number of steps: {t}\")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(\"-----------------------\\n\")\n",
    "\n",
    "# Display the trajectory\n",
    "for step_info in trajectory:\n",
    "    print(f\"Step {step_info['step']}: Action: {step_info['action']}, Observation: {step_info['obs']}, Reward: {step_info['reward']}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttFYwsVtXLyE"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "\n",
    "### Interpretació dels resultats\n",
    "    \n",
    "1. **Nombre de passos**: L'agent ha trigat 7 passos per arribar a la casella final amb la màxima recompensa. El resultat ve donat, ja que l'entorn és una graella de 4x4 amb parets centrals que obliguen l'agent a seguir una ruta més llarga rodejant les parets. \n",
    "    \n",
    "2. **Recompenses** : \n",
    "    - **Penalització lleu**: L'agent ha passat per 6 caselles amb penalització lleu (`BAD_SMALL`). Cada una amb una penalització de `-0.1`.\n",
    "    - **Casella final (`GOOD`)**: L'agent arriba a la casella final i es queda a la posició (`STAY`), obté una recompensa positiva de `+1.0`.\n",
    "    \n",
    "    \n",
    "3. **Recompensa total acumulada**:\n",
    "    - Penalitzacions lleus acumulades : `6 * (-0.1) = -0.6`\n",
    "    - Recompensa final : `+1.0`\n",
    "    - Recompensa total acumulada : `1.0 - 0.6 = 0.4`\n",
    "    \n",
    "L'agent ha aconseguit arribar a la casella final amb una recompensa total acumulada positiva, tot i les penalitzacions lleus trobades pel camí. A la vegada, ha evitat la casella amb una penalització severa (`BAD` amb `-10`) a la zona superior dreta. \n",
    " \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I9svxF9XLyG"
   },
   "source": [
    "## 3. Mètodes de Montecarlo (2 punts)\n",
    "\n",
    "L'objectiu d'aquest apartat és realitzar una estimació de la política òptima mitjançant els mètodes de Montecarlo. En concret estudiarem l'algoritme *On-policy first-visit MC control (per a polítiques $\\epsilon$-soft)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8GHrBhaXLyG"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.1</strong> (1 punt)\n",
    "\n",
    "Implementar l'Algorisme 3 explicat en el mòdul \"Mètodes de Montecarlo\": *On-policy first-visit MC control (per a polítiques $\\epsilon$-soft)* utilitzant els següents paràmetres:\n",
    "    \n",
    "<ul>\n",
    "  <li>Nombre d'episodis = 50.000</li>\n",
    "  <li>Epsilon inicial = 0,5</li>\n",
    "  <li>Factor de decaïment d'epsilon (<it>epsilon decay</it>) = 0,999</li>\n",
    "  <li>Mínim valor d'epsilon (<it>epsilon_min</it>) = 0,05</li>\n",
    "  <li>Actualitzar epsilon segons:  $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, \\epsilon_{\\textrm{min}})$$</li>\n",
    "  <li>Factor de descompte = 1</li>\n",
    "</ul>\n",
    "<b>Nota: als entrenaments dels agents es recomana utilitzar els entorns amb <code>render_mode = None</code> per a agilitzar l'execució.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PzULGiJpXLyG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/50000 completed.\n",
      "Episode 2000/50000 completed.\n",
      "Episode 3000/50000 completed.\n",
      "Episode 4000/50000 completed.\n",
      "Episode 5000/50000 completed.\n",
      "Episode 6000/50000 completed.\n",
      "Episode 7000/50000 completed.\n",
      "Episode 8000/50000 completed.\n",
      "Episode 9000/50000 completed.\n",
      "Episode 10000/50000 completed.\n",
      "Episode 11000/50000 completed.\n",
      "Episode 12000/50000 completed.\n",
      "Episode 13000/50000 completed.\n",
      "Episode 14000/50000 completed.\n",
      "Episode 15000/50000 completed.\n",
      "Episode 16000/50000 completed.\n",
      "Episode 17000/50000 completed.\n",
      "Episode 18000/50000 completed.\n",
      "Episode 19000/50000 completed.\n",
      "Episode 20000/50000 completed.\n",
      "Episode 21000/50000 completed.\n",
      "Episode 22000/50000 completed.\n",
      "Episode 23000/50000 completed.\n",
      "Episode 24000/50000 completed.\n",
      "Episode 25000/50000 completed.\n",
      "Episode 26000/50000 completed.\n",
      "Episode 27000/50000 completed.\n",
      "Episode 28000/50000 completed.\n",
      "Episode 29000/50000 completed.\n",
      "Episode 30000/50000 completed.\n",
      "Episode 31000/50000 completed.\n",
      "Episode 32000/50000 completed.\n",
      "Episode 33000/50000 completed.\n",
      "Episode 34000/50000 completed.\n",
      "Episode 35000/50000 completed.\n",
      "Episode 36000/50000 completed.\n",
      "Episode 37000/50000 completed.\n",
      "Episode 38000/50000 completed.\n",
      "Episode 39000/50000 completed.\n",
      "Episode 40000/50000 completed.\n",
      "Episode 41000/50000 completed.\n",
      "Episode 42000/50000 completed.\n",
      "Episode 43000/50000 completed.\n",
      "Episode 44000/50000 completed.\n",
      "Episode 45000/50000 completed.\n",
      "Episode 46000/50000 completed.\n",
      "Episode 47000/50000 completed.\n",
      "Episode 48000/50000 completed.\n",
      "Episode 49000/50000 completed.\n",
      "Episode 50000/50000 completed.\n",
      "\n",
      "Optimal policy learned.\n",
      "Q-values for each state-action pair:\n",
      "State 0: [ 0.10574534  0.36327212 -0.43898305 -0.1259434  -0.03606299]\n",
      "State 1: [ 0.09672414 -1.92916667 -9.99583333 -3.65909091 -3.925     ]\n",
      "State 4: [ 0.18433735  0.47244794  0.01007407 -0.12003155  0.15479876]\n",
      "State 8: [0.34364207 0.57940423 0.38814103 0.25669782 0.38708054]\n",
      "State 13: [0.50182371 0.65411585 0.79264535 0.59359375 0.64900662]\n",
      "State 12: [0.45945946 0.3741784  0.68606354 0.34382022 0.50881459]\n",
      "State 2: [ -5.46        -9.92307692 -12.15833333 -11.11111111  -4.45      ]\n",
      "State 3: [-17.425 -11.5   -19.6   -19.6   -21.3  ]\n",
      "State 14: [0.6600639  0.74319328 0.89763515 0.73125    0.70719875]\n",
      "State 15: [0.86993671 0.94019608 0.98009119 0.75445378 1.        ]\n",
      "State 11: [-0.83846154  0.89634551  0.77058824 -3.84117647  0.76666667]\n",
      "State 7: [  0.1          0.71304348  -8.34       -17.6         -5.96      ]\n",
      "\n",
      "Optimal policy (ordered by states):\n",
      "State 0: Action 1\n",
      "State 1: Action 0\n",
      "State 2: Action 4\n",
      "State 3: Action 1\n",
      "State 4: Action 1\n",
      "State 7: Action 1\n",
      "State 8: Action 1\n",
      "State 11: Action 1\n",
      "State 12: Action 2\n",
      "State 13: Action 2\n",
      "State 14: Action 2\n",
      "State 15: Action 4\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ COMPLETA ###########################\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "# Parameters\n",
    "num_episodes = 50000\n",
    "initial_epsilon = 0.5\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.05\n",
    "gamma = 1.0  # Discount factor\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=None)\n",
    "\n",
    "# Initialize epsilon\n",
    "epsilon = initial_epsilon\n",
    "\n",
    "# Initialize Q-table (action-value function)\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Initialize returns for state-action pairs\n",
    "returns_sum = defaultdict(float)\n",
    "returns_count = defaultdict(float)\n",
    "\n",
    "# Function to generate an episode following epsilon-soft policy\n",
    "def generate_episode(env, Q, epsilon):\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # epsilon-soft policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return episode\n",
    "\n",
    "# On-policy first-visit MC control algorithm for epsilon-soft policies\n",
    "for episode_num in range(1, num_episodes + 1):\n",
    "    # Generate a new episode\n",
    "    episode = generate_episode(env, Q, epsilon)\n",
    "    \n",
    "    # Initialize variables to compute cumulative reward\n",
    "    seen_state_action_pairs = set()\n",
    "    G = 0  # Cumulative reward initialized to 0\n",
    "    \n",
    "    # Process the episode in reverse order to calculate G\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + gamma * G  # Update cumulative reward\n",
    "        \n",
    "        # First-visit check for state-action pair\n",
    "        if (state, action) not in seen_state_action_pairs:\n",
    "            seen_state_action_pairs.add((state, action))\n",
    "            \n",
    "            # Update returns and Q-value for the state-action pair\n",
    "            returns_sum[(state, action)] += G\n",
    "            returns_count[(state, action)] += 1\n",
    "            Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "    \n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    \n",
    "    # Print progress every 1000 episodes\n",
    "    if episode_num % 1000 == 0:\n",
    "        print(f\"Episode {episode_num}/{num_episodes} completed.\")\n",
    "\n",
    "# Extract the optimal policy from the Q-table\n",
    "optimal_policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "\n",
    "print(\"\\nOptimal policy learned.\")\n",
    "print(\"Q-values for each state-action pair:\")\n",
    "for state, actions in Q.items():\n",
    "    print(f\"State {state}: {actions}\")\n",
    "    \n",
    "print(\"\\nOptimal policy (ordered by states):\")\n",
    "for state in sorted(optimal_policy.keys()):\n",
    "    print(f\"State {state}: Action {optimal_policy[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Q-table----------\n",
      "State 0:\n",
      "  Action 0: Q-value = 0.11\n",
      "  Action 1: Q-value = 0.36\n",
      "  Action 2: Q-value = -0.44\n",
      "  Action 3: Q-value = -0.13\n",
      "  Action 4: Q-value = -0.04\n",
      "State 1:\n",
      "  Action 0: Q-value = 0.10\n",
      "  Action 1: Q-value = -1.93\n",
      "  Action 2: Q-value = -10.00\n",
      "  Action 3: Q-value = -3.66\n",
      "  Action 4: Q-value = -3.92\n",
      "State 4:\n",
      "  Action 0: Q-value = 0.18\n",
      "  Action 1: Q-value = 0.47\n",
      "  Action 2: Q-value = 0.01\n",
      "  Action 3: Q-value = -0.12\n",
      "  Action 4: Q-value = 0.15\n",
      "State 8:\n",
      "  Action 0: Q-value = 0.34\n",
      "  Action 1: Q-value = 0.58\n",
      "  Action 2: Q-value = 0.39\n",
      "  Action 3: Q-value = 0.26\n",
      "  Action 4: Q-value = 0.39\n",
      "State 13:\n",
      "  Action 0: Q-value = 0.50\n",
      "  Action 1: Q-value = 0.65\n",
      "  Action 2: Q-value = 0.79\n",
      "  Action 3: Q-value = 0.59\n",
      "  Action 4: Q-value = 0.65\n",
      "State 12:\n",
      "  Action 0: Q-value = 0.46\n",
      "  Action 1: Q-value = 0.37\n",
      "  Action 2: Q-value = 0.69\n",
      "  Action 3: Q-value = 0.34\n",
      "  Action 4: Q-value = 0.51\n",
      "State 2:\n",
      "  Action 0: Q-value = -5.46\n",
      "  Action 1: Q-value = -9.92\n",
      "  Action 2: Q-value = -12.16\n",
      "  Action 3: Q-value = -11.11\n",
      "  Action 4: Q-value = -4.45\n",
      "State 3:\n",
      "  Action 0: Q-value = -17.43\n",
      "  Action 1: Q-value = -11.50\n",
      "  Action 2: Q-value = -19.60\n",
      "  Action 3: Q-value = -19.60\n",
      "  Action 4: Q-value = -21.30\n",
      "State 14:\n",
      "  Action 0: Q-value = 0.66\n",
      "  Action 1: Q-value = 0.74\n",
      "  Action 2: Q-value = 0.90\n",
      "  Action 3: Q-value = 0.73\n",
      "  Action 4: Q-value = 0.71\n",
      "State 15:\n",
      "  Action 0: Q-value = 0.87\n",
      "  Action 1: Q-value = 0.94\n",
      "  Action 2: Q-value = 0.98\n",
      "  Action 3: Q-value = 0.75\n",
      "  Action 4: Q-value = 1.00\n",
      "State 11:\n",
      "  Action 0: Q-value = -0.84\n",
      "  Action 1: Q-value = 0.90\n",
      "  Action 2: Q-value = 0.77\n",
      "  Action 3: Q-value = -3.84\n",
      "  Action 4: Q-value = 0.77\n",
      "State 7:\n",
      "  Action 0: Q-value = 0.10\n",
      "  Action 1: Q-value = 0.71\n",
      "  Action 2: Q-value = -8.34\n",
      "  Action 3: Q-value = -17.60\n",
      "  Action 4: Q-value = -5.96\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Optimal policy based on Q_mc (ordered by states)\n",
      "\n",
      "State 0: Action 1\n",
      "State 1: Action 0\n",
      "State 2: Action 4\n",
      "State 3: Action 1\n",
      "State 4: Action 1\n",
      "State 7: Action 1\n",
      "State 8: Action 1\n",
      "State 11: Action 1\n",
      "State 12: Action 2\n",
      "State 13: Action 2\n",
      "State 14: Action 2\n",
      "State 15: Action 4\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "Q_mc = Q #Computed Q-table from MC algorithm\n",
    "\n",
    "#Q-table print\n",
    "print(\"---------Q-table----------\")\n",
    "for state, actions in Q_mc.items():\n",
    "    print(f\"State {state}:\")\n",
    "    for action, q_value in enumerate(actions):\n",
    "        print(f\"  Action {action}: Q-value = {q_value:.2f}\")\n",
    "print(\"\\n----------------------------\")\n",
    "\n",
    "# Optimal policy sorted by states\n",
    "optimal_policy = {state: np.argmax(actions) for state, actions in Q_mc.items()}\n",
    "print(\"\\nOptimal policy based on Q_mc (ordered by states)\\n\")\n",
    "for state in sorted(optimal_policy.keys()):\n",
    "    print(f\"State {state}: Action {optimal_policy[state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z-VxeHfXLyH"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.2</strong> (0.5 punts)\n",
    "Implementar una funció que imprimeixi per pantalla la política trobada per a cada casella a partir de la funció Q (aplicant una política <i>greedy</i>) i executar-la amb la funció Q obtinguda de l'entrenament del mètode de Montecarlo.\n",
    "Es tracta de la política òptima?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rhrFr8xtXLyK"
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "# Action mapping dictionary\n",
    "switch_action = {\n",
    "    0: \"Left\",\n",
    "    1: \"Down\",\n",
    "    2: \"Right\",\n",
    "    3: \"Up\",\n",
    "    4: \"Stay\",\n",
    "}\n",
    "\n",
    "def print_policy(Q, width, height):\n",
    "    print(\"Optimal Policy based on Q-values\\n\")\n",
    "    \n",
    "    # Iterate over each cell of the grid\n",
    "    for row in range(height):\n",
    "        row_policy = []\n",
    "        for col in range(width):\n",
    "            state = row * width + col  # Coordinates (row, col) to a linear index\n",
    "            \n",
    "            # Check if the state exists in the Q-table\n",
    "            if state in Q:\n",
    "                best_action = np.argmax(Q[state])  # Take action with highest Q value\n",
    "                row_policy.append(switch_action.get(best_action, \"None\"))  # Append action\n",
    "            else:\n",
    "                row_policy.append(\"None\")  # If the state does not exist, add \"None\"\n",
    "        \n",
    "        # Print the policy for the row\n",
    "        print(\" \".join(row_policy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "i22S8zaNXLyK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy based on Q-values\n",
      "\n",
      "Down Left Stay Down\n",
      "Down None None Down\n",
      "Down None None Down\n",
      "Right Right Right Stay\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "print_policy(Q_mc, width=4, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "\n",
    "Els resultats mostren la política òptima per al tauler de 4x4 a partir dels valors de Q obtinguts mitjançant l'algorisme de Montecarlo. La política que s'ha obtingut sembla ser una bona aproximació de la política òptima per al problema, tot i que caldria revisar els estats no explorats i assegurar-se que la política cobreixi tot el tauler de manera eficaç. La presència de 'None' en algunes àrees del tauler indica que cal més exploració i més entrenament per millorar la política. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPvEnKWHXLyL"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.3</strong> (0.5 punts)\n",
    "Executar un episodi amb la política trobada i mostrar la trajectòria de l'agent i el retorn obtingut. Comentar els resultats.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yYSTtAGjXLyL"
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "def execute_episode(Q, env):\n",
    "    #Init state by resetting the env\n",
    "    state, _ = env.reset() \n",
    "    done = False #Termination episode flag to false\n",
    "    trajectory = []  # Store agent's trajectory\n",
    "    total_reward = 0  # Init total reward\n",
    "\n",
    "    while not done:\n",
    "        # Get best action based on Qtable \n",
    "        best_action = np.argmax(Q[state])\n",
    "        \n",
    "        # take chosen action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(best_action)\n",
    "        \n",
    "        # Store state,action,reward\n",
    "        trajectory.append((state, best_action, reward))\n",
    "        \n",
    "        # Update reward and state\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if the episode it is finished\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    # Display agent's trajectory\n",
    "    print(\"\\nAgent's trajectory:\")\n",
    "    for step in trajectory:\n",
    "        state, action, reward = step\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
    "    \n",
    "    # Disply total reward\n",
    "    print(f\"\\nTotal return for the episode: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WM2JvlXCXLyL",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent's trajectory:\n",
      "State: 0, Action: 1, Reward: -0.1\n",
      "State: 4, Action: 1, Reward: -0.1\n",
      "State: 8, Action: 1, Reward: -0.1\n",
      "State: 12, Action: 2, Reward: -0.1\n",
      "State: 13, Action: 2, Reward: -0.1\n",
      "State: 14, Action: 2, Reward: -0.1\n",
      "State: 15, Action: 4, Reward: 1.0\n",
      "\n",
      "Total return for the episode: 0.4\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "execute_episode(Q,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "El total acumulat de la recompensa és de 0.4, el qual correspon a la suma de les recompenses obtingudes en cada pas del trajecte. Inicialment, tenim diverses recompenses lleus negatives (-0.1) i una recompensa positiva (1.0) en arribar a la casella final. Analitzant la trajectòria seguida, podem veure que el retorn total de l'episodi és relativament baix, però correspon al valor màxim possible donat l'entorn en el qual ens trobem. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzg9iWfjXLyL"
   },
   "source": [
    "## 4. Mètodes d'Diferència Temporal (3.5 punts)\n",
    "\n",
    "L'objectiu d'aquest apartat és realitzar una estimació de la política òptima mitjançant els mètodes de Diferència Temporal en l'entorn *Gridworld* creat anteriorment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Mètode SARSA (2 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3xqC9vCXLyL"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.1.1</strong> (1 pt)\n",
    "\n",
    "Implementar l'algoritme *SARSA* explicat al mòdul 6 \"Aprenentatge per Diferència Temporal\" y executar-lo utilizant els següents paràmetres:\n",
    "<ul>    \n",
    "    <li>Número d'episodis = 10.000</li>\n",
    "    <li>learning rate = 0,2</li>\n",
    "    <li>discount factor = 1</li>\n",
    "    <li>epsilon = 0,5</li>\n",
    "    <li>epsilon decay = 0,9</li>\n",
    "    <li>mínim valor d'epsilon = 0,05</li>\n",
    "</ul>\n",
    "Actualitzar el valor d'epsilon segons: $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, \\epsilon_{\\textrm{min}})$$\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=None)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "Q_sarsa, deltas = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4.1.2</strong> (0.5 pts)\n",
    "Imprimir una gràfica amb l'evolució del més gran error TD de cada episodi. Atès que l'error té molta variància, imprimiu també la mitjana mòbil amb una finestra temporal de 100 episodis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.1.3</strong> (0.25 pts)\n",
    "Imprimir la política trobada amb el mètode SARSA per a cada estat (podeu re-utilitzar la funció creada a l'apartat anterior dels mètodes MC). Es tracta d'una política òptima?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "print_policy(Q_sarsa,?,?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "Escriure aquí els vostres comentaris.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4.1.4</strong> (0.25 punts)\n",
    "Executar un episodi amb la política trobada i mostrar la trajectòria de l'agent i el retorn obtingut. Comentar els resultats.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "execute_episode(?,?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "Escriure aquí els vostres comentaris.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Mètode Q-Learning (1.5 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3xqC9vCXLyL"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.2.1</strong> (0.75 pts)\n",
    "\n",
    "Implementar l'algoritme *Q-learning* explicat al mòdul 6 \"Aprenentatge per Diferència Temporal\" y executar-lo utilizant els següents paràmetres:\n",
    "<ul>    \n",
    "    <li>Número de episodios = 5.000</li>\n",
    "    <li>learning rate = 0,4</li>\n",
    "    <li>discount factor = 1</li>\n",
    "    <li>epsilon = 0,5</li>\n",
    "    <li>epsilon decay = 0,9</li>\n",
    "    <li>mínim valor d'epsilon = 0,05</li>\n",
    "</ul>\n",
    "Actualitzar el valor d'epsilon segons: $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, \\epsilon_{\\textrm{min}})$$\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuY9efT4XLyL"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=None)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "Q_qlearning, deltas = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4.2.2</strong> (0.25 pts)\n",
    "Imprimir una gràfica amb l'evolució del més gran error TD de cada episodi. Atès que l'error té molta variància, imprimiu també la mitjana mòbil amb una finestra temporal de 100 episodis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.2.3</strong> (0.25 pts)\n",
    "Imprimir la política trobada amb el mètode Q-learning per a cada estat (podeu re-utilitzar la funció creada a l'apartat anterior). Es tracta d'una política òptima?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "print_policy(Q_qlearning,?,?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "Escriure aquí els vostres comentaris.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4.2.4</strong> (0.25 punts)\n",
    "Executar un episodi amb la política trobada i mostrar la trajectòria de l'agent i el retorn obtingut. Comentar els resultats.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "execute_episode(?,?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "Escriure aquí els vostres comentaris.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjWsuHJEXLyN"
   },
   "source": [
    "## 5. Comparativa dels algoritmes (0.5 punts)\n",
    "\n",
    "En aquest apartat farem una petita comparativa dels mètodes programats en els apartats anteriors.\n",
    "\n",
    "Compararem el comportament dels algoritmes en termens de la política assolida, la duració de l'entrenament i el factor de descompte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9sS5ijiXLyO"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 5.1 - Anàlisi de les polítiques obtingudes i del temps de convergència</strong> (0,25 punts)\n",
    "\n",
    "Realitzar un estudi de les polítiques obtingudes responent a les següents preguntes:\n",
    "<ul>\n",
    "  <li>Tots els algoritmes aconsegueixen arribar a la política òptima?</li>\n",
    "  <li>Triguen el mateix temps en convergir?</li>\n",
    "  <li>A què poden ser degudes les diferències?</li>\n",
    "</ul>\n",
    "<b>Nota: Es recomana executar cada algoritme diverses vegades per extreure unes conclusions més consistents.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBncS-4SXLyO"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONS:</strong>\n",
    "<br><br>\n",
    "Escriure aquí els vostres comentaris.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMobYk_5XLyO"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 5.2 - Influència del factor de descompte (discount factor)</strong> (0,25 punts)\n",
    "\n",
    "Tots els agents s'han entrenat amb recompenses sense descomptar (factor de descompte = 1). A què creus que es deu aquesta elecció? Creus que millorarien els resultats si s'utilitza un factor de descompte diferent? Per què? En cas afirmatiu, selecciona un nou factor de descompte i testeja'l a algun dels algortimes (per exemple a Q-learning).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS2P1vMYXLyP"
   },
   "outputs": [],
   "source": [
    "#SOLUCIÓ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlNW-nJYXLyP"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONS:</strong>\n",
    "<br><br>\n",
    "Escriure aquí els vostres comentaris.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Diferència de polítiques entre SARSA i Q-Learning (1.5 punts)\n",
    "\n",
    "En aquest darrer apartat dissenyarem un entorn una mica més complicat i compararem les diferents polítiques que assoleixen els mètodes SARSA i Q-learning.\n",
    "\n",
    "Per a això es demana:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 6.1</strong> (0,5 punts)\n",
    "\n",
    "Crear un entorn nou amb una graella 5x5 com la que s'observa a continuació (utilitzeu <code>\"5X5_Ex6\"</code> com a clau de l'entrada al diccionari <code>GRIDS</code>.):\n",
    "<br><br>\n",
    "<img src=\"images/Gym-Gridworld_Ex6_5x5.png\" alt=\"Mi imagen\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "<br>\n",
    "Fixeu-vos que la casella inicial no es troba a dalt a l'esquerra si no a la meitat de la primera columna. Per a aconseguir això heu de crear una sub-classe de la classe <code>Gridworld</code> i afegir-la al final de l'arxiu <code>\\gym_gridworlds\\gridworld.py</code> (en el mateix arxiu teniu algun exemple).\n",
    "\n",
    "Enrecordeu-vos de registrar l'entorn a l'arxiu <code>\\gym_gridworlds\\\\\\_\\_init\\_\\_.py</code> amb els paràmetres pertinents (fixeu-vos en altres registres com es crida a una sub-classe), reiniciar el kernel del Notebook i tornar a importar els paquets.\n",
    "\n",
    "L'entorn s'ha de poder cridar amb el següent codi:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Ex6-5x5-v0\", render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 6.2 - Entrenament i comparativa dels agents</strong> (1 punt)\n",
    "\n",
    "<ol>\n",
    "  <li>Entrena els 2 agents TD (SARSA i Q-learning) en el nou entorn. Ajusta els diferents paràmetres fins que els algoritmes convergeixin.</li>\n",
    "  <li>Imprimeix per pantalla la política assolida per cada agent.</li>\n",
    "  <li>Executa un episodi amb cada política i mostra els resultats (accions, recorregut i recompensa total).</li>\n",
    "  <li>Comenta els resultats. Són els resultats esperats?</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Ex6-5x5-v0\", render_mode=None)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "######################## SOLUCIÓ ###########################\n",
    "#Entrenament SARSA\n",
    "Q_sarsa, deltas = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "######################## SOLUCIÓ ###########################\n",
    "#Entrenament Q-learning\n",
    "Q_qlearning, deltas = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "#Política SARSA\n",
    "print_policy(Q_sarsa,?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "#Execució episodi SARSA\n",
    "execute_episode(?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "#Política O-learning\n",
    "print_policy(Q_qlearning,?,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "#Execució episodi Q-learning\n",
    "execute_episode(?,?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONS:</strong>\n",
    "<br><br>\n",
    "Escriure aquí els vostres comentaris.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
