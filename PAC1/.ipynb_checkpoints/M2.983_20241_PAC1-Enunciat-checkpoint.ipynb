{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkkr1HUBPOcL"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.983 · Aprenentage per reforç</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2024-1 · Màster universitari en Ciència de dades (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis d'Informàtica, Multimèdia i Telecomunicació</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Nom i cognoms:</strong> Xavier Maltas Tarridas\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFR_J-YYXLx-"
   },
   "source": [
    "# PAC 1 - Solucions tabulars\n",
    "\n",
    "En aquesta pràctica implementarem els diferents mètodes d'aprenentatge per reforç estudiats als Blocs I i II del curs. En concret, ens centrarem en la definició d'un entorn i implementarem els diferents mètodes per buscar una solució òptima del problema.\n",
    "\n",
    "**<u>Important</u>: El lliurament s'ha de fer en format notebook i en format html on es vegi el codi, els resultats i comentaris de cada exercici. És a dir, s'han de lliurar dos fitxers: un amb extensió .ipynb i un altre .html. Per exportar el notebook a html pot fer-se des del menú File $\\to$ Download as $\\to$ HTML. A més s'han de lliurar els 2 arxius que es modificaran dins de la carpeta <code>\\gym_gridworlds</code>, els arxius <code>gridworld.py</code> i <code>\\_\\_init\\_\\_.py</code>.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyFUy1qlDGku"
   },
   "source": [
    "## 1. L'entorn __Gym-Gridworlds__ (1 punt)\n",
    "\n",
    "L'entorn __Gym-Gridworlds__ és un entorn de tercers que es troba referenciat a la web de Gymnasium, concretament a l'apartat 'ENVIRONMENTS/External Environments'.\n",
    "\n",
    "L'entorn consisteix en un agent que es mou en una quadrícula de dimensions configurables. La classe per defecte `Gridworld` implementa una tasca \"anar a l'objectiu\" on l'agent té cinc accions (esquerra, dreta, amunt, avall, romandre al lloc) i una funció de transició predeterminada (seleccionar l'acció \"romandre al lloc\" als estats objectiu acaba l'episodi).\n",
    "\n",
    "El codi i la documentació es troba a https://github.com/sparisi/gym_gridworlds\n",
    "\n",
    "Es recomana llegir atentament la pàgina per familiaritzar-se amb l'entorn.\n",
    "\n",
    "**<u>Nota</u>: L'entorn utilitzat té un mode de renderitzat <code>render\\_mode = \"human\"</code> que permet observar la quadrícula i el moviment de l'agent en una pantalla adicional utiltzant el paquet <code>pygame</code> de python. Aquest mode de renderitzat només funciona en local. Per a poder executar el codi sense problemes es proporciona un arxiu <code>environment.yml</code> perquè pugueu crear un entorn virtual amb els paquets necessaris.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBkO2FuFJdH-"
   },
   "source": [
    "Començarem carregant un dels múltiples entorns pre-dissenyats i veurem les seves principals característiques, executant un episodi de prova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBrIWvZmDB6H"
   },
   "source": [
    "### 1.1. Càrrega de dades (0.5 punts)\n",
    "\n",
    "El següent codi carrega els paquets necessaris per a l'exemple.\n",
    "\n",
    "Comencem instal·lant Gymnasium (això només s'ha de fer una vegada si no ho teniu instal·lat ja)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7035,
     "status": "ok",
     "timestamp": 1729206552620,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "qMuA4JjZXLx_",
    "outputId": "99100ad3-caf9-4cfe-9636-d02536659505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium==1.0.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La versió de Gymnasium utilitzada en aquesta PAC és la 1.0.0, comprova-ho a continuació\n",
      "Gymnasium Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "print(\"La versió de Gymnasium utilitzada en aquesta PAC és la 1.0.0, comprova-ho a continuació\")\n",
    "print(\"Gymnasium Version:\", gym.__version__) # 0.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuació clonem el repositori de l'entorn (això només s'ha de fer una vegada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1729206566707,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "81RCuLEwej1O",
    "outputId": "f665789d-788c-4505-a597-cdef0d02e924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gym_gridworlds'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sparisi/gym_gridworlds.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrem dins del directori `gym_gridworlds` i l'instal·lem en mode editable (això només s'ha de fer una vegada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1729206575365,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "7ne58OSTe2Sa",
    "outputId": "ac759fe7-3aa5-4afa-80b9-c4ed9492dae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavim\\Desktop\\myRepos\\ReinforcementLearning\\PAC1\\gym_gridworlds\n",
      "Obtaining file:///C:/Users/xavim/Desktop/myRepos/ReinforcementLearning/PAC1/gym_gridworlds\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: gymnasium in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from Gym-Gridworlds==1.0) (1.0.0)\n",
      "Requirement already satisfied: pygame in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from Gym-Gridworlds==1.0) (2.6.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium->Gym-Gridworlds==1.0) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium->Gym-Gridworlds==1.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium->Gym-Gridworlds==1.0) (4.7.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium->Gym-Gridworlds==1.0) (0.0.4)\n",
      "Installing collected packages: Gym-Gridworlds\n",
      "  Running setup.py develop for Gym-Gridworlds\n",
      "Successfully installed Gym-Gridworlds-1.0\n"
     ]
    }
   ],
   "source": [
    "%cd gym_gridworlds\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7016,
     "status": "ok",
     "timestamp": 1729206587859,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "R3t9AqNLZ549",
    "outputId": "d3c79fd1-df15-46fc-8ad3-843090119b33"
   },
   "source": [
    "Importem els paquets necessaris per la resta de la PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1729206594496,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "h5y5sIqBPPxr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import time\n",
    "import gym_gridworlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem l'entorn mitjançant la instanciació d'un objecte de classe `Gridworld` (en executar la instrucció `env = gym.make(\"Gym-Gridworlds/Full-4x5-v0\", render_mode=\"human\")`) i imprimim per pantalla el tipus de l'espai d'accions i de l'espai d'observacions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1729206611289,
     "user": {
      "displayName": "Luis Esteve Elfau",
      "userId": "04659768458370690763"
     },
     "user_tz": -120
    },
    "id": "ej5qlR-OaTLi"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Full-4x5-v0\", render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entorn generat té el següent aspecte (comprovar-ho a la finestra emergent de <code>pygame</code> que es genera a l'executar <code>env.render()</code>):\n",
    "<br><br>\n",
    "<img src=\"images/Gym-Gridworld_4x5.png\" alt=\"Mi imagen\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "En aquest entorn podem veure una sèrie de caselles i figures que es descriuen a continuació:\n",
    "\n",
    "<ul>\n",
    "  <li>Les caselles negres són buides.</li>\n",
    "  <li>Les caselles negres amb fletxes grises són caselles on l'agent només es pot moure en una direcció (les altres accions fallaran).</li>\n",
    "  <li>Les caselles vermelles donen recompenses negatives (com més brillants, més negatives).</li>\n",
    "  <li>Les caselles verdes donen recompenses positives (com més brillants, més altes).</li>\n",
    "  <li>Les caselles grogues són arenes movedisses, on totes les accions fallaran amb un 90% de probabilitat.</li>\n",
    "  <li>L'agent és el cercle blau.</li>\n",
    "</ul>\n",
    "\n",
    "A més, hi ha altres tipus de caselles:\n",
    "\n",
    "<ul>\n",
    "  <li>Les caselles blanques són pous (caminar-hi comporta una gran recompensa negativa i l'episodi acaba).</li>\n",
    "  <li>Les caselles porpres són parets (l'agent no hi pot passar).</li>\n",
    "  <li>Una fletxa taronja indica l'última acció de l'agent.</li>\n",
    "  <li>Un punt taronja indica que l'agent no va intentar moure's amb la seva última acció.</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 1.1</strong> (0.5 pts)\n",
    "\n",
    "Analitzar la [documentació](https://github.com/sparisi/gym_gridworlds) i el codi de l'entorn que es troba a <code>\\gym_gridworlds\\gridworld.py</code> per respondre a les següents preguntes:\n",
    "<ul>    \n",
    "    <li>Descriure l'espai d'accions de l'entorn: quantes accions hi ha? a què correspòn cadascuna d'elles?</li>\n",
    "    <li>Quina és la casella inicial? Es pot canviar?</li>\n",
    "</ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br>\n",
    "    \n",
    "    \n",
    "<h2>Espai d'accions de l'entorn</h2>\n",
    "<p><strong>Quantes accions hi ha?</strong></p>\n",
    "<p>L'espai d'accions és discret amb un total de 5 accions, representades per nombres enters en el rang <code>{0, 1, 2, 3, 4}</code>.</p>\n",
    "\n",
    "<p><strong>A què correspon cadascuna d'elles?</strong></p>\n",
    "<ul>\n",
    "  <li><code>0</code>: Moure's a l'esquerra (LEFT)</li>\n",
    "  <li><code>1</code>: Moure's cap avall (DOWN)</li>\n",
    "  <li><code>2</code>: Moure's a la dreta (RIGHT)</li>\n",
    "  <li><code>3</code>: Moure's cap amunt (UP)</li>\n",
    "  <li><code>4</code>: Quedar-se quiet (STAY)</li>\n",
    "</ul>\n",
    "\n",
    "    \n",
    "    \n",
    "<h2>Casella inicial</h2>\n",
    "<p><strong>Quina és la casella inicial?</strong></p>\n",
    "<p>Per defecte, en aquest entorn, la casella inicial és la cantonada superior esquerra <code>(0, 0)</code>.</p>\n",
    "\n",
    "<p><strong>Es pot canviar?</strong></p>\n",
    "<p>Sí, és possible canviar la casella inicial. Pots utilitzar classes predefinides com <code>GridworldMiddleStart</code> per començar al centre, o <code>GridworldRandomStart</code> per una posició aleatòria.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXpl0ErhE_I1"
   },
   "source": [
    "### 1.2. Execució d'un episodi (0.5 punts)\n",
    "\n",
    "A continuació, realitzarem l'execució d'un episodi de l'entorn *Gym-Gridworlds/Full-4x5-v0* utilitzant un agent que selecciona les accions de forma aleatòria.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84mPbgBGE1zz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inicialitzem l'entorn\n",
    "obs, info = env.reset()\n",
    "t, total_reward, terminated, truncated = 0, 0, False, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"Left\",\n",
    "        1: \"Down\",\n",
    "        2: \"Right\",\n",
    "        3: \"Up\",\n",
    "        4: \"Stay\",\n",
    "    }\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    # Triar una acció aleatòria (aquesta és la implementació de l'agent)\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Executar l'acció i esperar la resposta de l'entorn\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    # Imprimir time-step\n",
    "    print(\"Step {}. Action: {} -> Obs: {} and reward: {}. Terminated {}. Truncated {}.\".format(t+1, switch_action[action], new_obs, reward, terminated, truncated))\n",
    "\n",
    "    # Actualitzar variables\n",
    "    obs = new_obs\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    time.sleep(0.5) #S'afegeix per alentir el renderitzat i poder apreciar els moviments de l'agent\n",
    "\n",
    "print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es recomana analitzar el codi anterior, executar-lo vàries vegades i observar el renderitzat en la finestra emergent externa abans de contestar el següent exercici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 1.2</strong> (0.5 pts)\n",
    "\n",
    "<!-- Una vegada executat l'episodi, i en base a l'anàlisi de la documentació i del codi de l'entorn respondre a les següents preguntes: -->\n",
    "<ul> \n",
    "    <li>Descriure l'espai d'estats: en què consisteixen els estats? com es codifiquen per defecte?</li>\n",
    "    <li>Descriure el senyal de recompensa: quins valors pot prendre? en quines situacions es rep cada valor?</li>\n",
    "    <li>Quan finalitza un episodi?</li>\n",
    "</ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br>\n",
    "\n",
    "    \n",
    "<h2>Espai d'estats</h2>\n",
    "<p><strong>En què consisteixen els estats</strong></p>\n",
    "<p>L'espai d'estats en l'entorn \"Gym-Gridworlds/Full-4x5-v0\" es basa en la posició de l'agent dins d'una graella de 4x5. Cada estat es troba representat segons la posició de l'agent a la graella, la qual es descriu mitjançant un conjunt de coordenades <code>(x,y)</code>, on <code>x</code> és la fila i <code>y</code> és la columna de la graella.</p>\n",
    "\n",
    "<p><strong>Com es codifiquen per defecte?</strong></p>\n",
    "    <p>Cada estat es codifica mitjançant una parella coordenades representades per nombres enters <code>(x,y)</code>. La <code>x</code> és la fila (entre 0 i 3) i <code>y</code> és la columna (entre 0 i 4) de la graella. Això permet representar fins a 20 possibles estats (4 files * 5 columnes).</p>\n",
    "    \n",
    "<h2>Senyal de recompensa</h2>\n",
    "<p><strong>Quins valors pot prendre?</strong></p>\n",
    "<ul>\n",
    "  <li><strong>+1</strong>: Recompensa màxima per arribar a la casella d'objectiu (<code>GOOD</code>) i executar l'acció <code>STAY</code>.</li>\n",
    "  <li><strong>+0.1</strong>: Recompensa menor per arribar a una casella de distracció (<code>GOOD_SMALL</code>) i executar l'acció <code>STAY</code>.</li>\n",
    "  <li><strong>-10</strong>: Penalització per moure's a una casella de penalització gran (<code>BAD</code>).</li>\n",
    "  <li><strong>-0.1</strong>: Penalització menor per moure's a una casella de penalització menor (<code>BAD_SMALL</code>).</li>\n",
    "  <li><strong>-100</strong>: Penalització severa per caure en una casella de <forat (<code>PIT</code>).</li>\n",
    "  <li><strong>0</strong>: Recompensa nul·la en caselles buides (<code>EMPTY</code>) o si l'agent xoca amb una <em>paret</em> (<code>WALL</code>).</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>En quines situacions es rep cada valor?</strong></p>\n",
    "<ul>\n",
    "  <li><strong>+1</strong>: Quan l'agent arriba a l'objectiu principal (<code>GOOD</code>) i tria quedar-se quiet (<code>STAY</code>).</li>\n",
    "  <li><strong>+0.1</strong>: Quan l'agent arriba a un objectiu distractor(<code>GOOD_SMALL</code>) i tria quedar-se quiet (<code>STAY</code>).</li>\n",
    "  <li><strong>-10</strong>: Quan l'agent entra en una casella amb penalització gran (<code>BAD</code>).</li>\n",
    "  <li><strong>-0.1</strong>: Quan l'agent es mou cap a una casella amb penalització menor (<code>BAD_SMALL</code>).</li>\n",
    "  <li><strong>-100</strong>: Quan l'agent cau en una casella de forat (<code>PIT</code>), finalitzant l'episodi.</li>\n",
    "  <li><strong>0</strong>: Quan l'agent es mou a una casella buida (<code>EMPTY</code>) o xoca amb una paret (<code>WALL</code>), o si no es queda quiet després d'arribar a un objectiu (<code>GOOD</code> o <code>GOOD_SMALL</code>).</li>\n",
    "</ul>\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "<h2>Finalització de l'episodi</h2>\n",
    "<p><strong>Quan finalitza un episodi?</strong></p>\n",
    "<p>\n",
    "  L'episodi es completa en les situacions següents:\n",
    "  <ul>\n",
    "    <li>Quan l'agent arriba a la casella d'objectiu (<em>GOOD</em>) i executa l'acció <em>STAY</em>, rebent una recompensa de +1.</li>\n",
    "    <li>Quan l'agent cau en una casella de forat (<em>PIT</em>), rebent una penalització de -100.</li>\n",
    "  </ul>\n",
    "</p>\n",
    "\n",
    "\n",
    "<h2>Finalització de l'episodi</h2>\n",
    "<p><strong>Quan finalitza un episodi?</strong></p>\n",
    "<ul>\n",
    "  <li>L'episodi es finalitza immediatament si l'agent rep una recompensa de <strong>+1</strong> en arribar a la casella objectiu principal (<code>GOOD</code>) i executar l'acció <code>STAY</code>.</li>\n",
    "  <li>També finalitza si l'agent cau en una casella de forat (<code>PIT</code>) i rep una recompensa de <strong>-100</strong>.</li>\n",
    "</ul>\n",
    "    \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanquem l'entorn perquè als següents apartats en crearem un de nou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc_f31JOkh2d"
   },
   "source": [
    "## 2. Creació d'un entorn propi (1,5 punts)\n",
    "\n",
    "L'entorn *Gym-Gridworld* té diversos arguments que poden ser modificats:\n",
    "\n",
    "* La dimensió de la quadrícula.\n",
    "* El tipus de cada casella.\n",
    "* La posició de les caselles de sortida i d'arribada.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WniTr5sdXLyC"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.1</strong> (0,75 punts)\n",
    "\n",
    "Crear un entorn nou com el que es descriu a continuació:\n",
    "\n",
    "<ul>\n",
    "  <li>Graella 4x4</li>\n",
    "  <li>Les 4 caselles centrals son de tipus paret (color porpra).</li>\n",
    "  <li>Casella d'inici a dalt a l'esquerra (posició per defecte).</li>\n",
    "  <li>Casella final (color verd brillant) a baix a la dreta.</li>\n",
    "  <li>La casella de la dreta del tot de la primera fila que sigui amb penalització gran (color vermell brillant).</li>\n",
    "  <li>La resta de caselles, en comptes de ser normals (color negre) posar-les amb una petita penalització (color vermell fosc).</li>\n",
    "</ul>\n",
    "\n",
    "Per a realitzar l'entorn:\n",
    "<ul>\n",
    "  <li>Heu d'afegir la graella al diccionari <code>GRIDS</code> de l'arxiu <code>\\gym_gridworlds\\gridworld.py</code> (utilitzeu com a clau de l'entrada al diccionari <code>\"4X4_Ex2\"</code>.)</li>\n",
    "  <li>Heu de registrar l'entorn a l'arxiu <code>\\gym_gridworlds\\__init__.py</code> amb les següents dades:</li>\n",
    "      <ul>\n",
    "          <li><code>id=\"Gym-Gridworlds/Ex2-4x4-v0\"</code></li>\n",
    "          <li><code>entry_point=\"gym_gridworlds.gridworld:Gridworld\"</code></li>\n",
    "          <li><code>max_episode_steps=100</code></li>\n",
    "          <li><code>kwargs={\"grid\": \"4X4_Ex2\",}</code></li>\n",
    "       </ul>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vegada afegit i registrat el nou entorn re-inicieu el kernel del Notebook per a que es pugui carregar i executar.\n",
    "\n",
    "Si tot ha anat bé, després d'executar el següent codi, hauríeu d'obtenir un entorn amb el següent aspecte:\n",
    "<br><br>\n",
    "<img src=\"images/Gym-Gridworld_Ex2_4x4.png\" alt=\"Mi imagen\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIKcFYi-IX-C"
   },
   "outputs": [],
   "source": [
    "import gym_gridworlds\n",
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#gridworld.py\n",
    "#new grid\n",
    "\n",
    "GRIDS = {\n",
    "\n",
    "    \"4X4_Ex2\": [\n",
    "        [BAD_SMALL, BAD_SMALL, BAD_SMALL, BAD],\n",
    "        [BAD_SMALL, WALL, WALL, BAD_SMALL],\n",
    "        [BAD_SMALL, WALL, WALL, BAD_SMALL],\n",
    "        [BAD_SMALL, BAD_SMALL, BAD_SMALL, GOOD],\n",
    "    ],\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#__init__.py\n",
    "#Register env\n",
    "\n",
    "register(\n",
    "    id=\"Gym-Gridworlds/Ex2-4x4-v0\",\n",
    "    entry_point=\"gym_gridworlds.gridworld:Gridworld\",\n",
    "    max_episode_steps=100,\n",
    "    kwargs={\"grid\": \"4X4_Ex2\"},\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AVvZVSqIggI"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.2</strong> (0,25 punts)\n",
    "\n",
    "A continuació, implementar un agent que dugui a terme una política aleatòria. Comprovar que les caselles visitades i les recompenses rebudes es corresponen amb les accions i l'entorn programat.\n",
    "\n",
    "Mostrar la trajectòria seguida per l'agent. No cal graficar-la, tan sols mostrar les accions i les caselles visitades en ordre i les recompenses rebudes.\n",
    "\n",
    "Mostrar també el nombre de *steps* i la recompensa total acumulada.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksk8N63OHpEO"
   },
   "outputs": [],
   "source": [
    "# Inicialitzem l'entorn\n",
    "obs, info = env.reset()\n",
    "t, total_reward, terminated, truncated = 0, 0, False, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"Left\",\n",
    "        1: \"Down\",\n",
    "        2: \"Right\",\n",
    "        3: \"Up\",\n",
    "        4: \"Stay\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1B0qogIJB2z"
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "import gymnasium as gym\n",
    "\n",
    "# List to store the trajectory\n",
    "trajectory = []\n",
    "\n",
    "# Interaction loop with the environment\n",
    "while not (terminated or truncated):\n",
    "    # Select a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Execute the action and get new observations and rewards\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Store the information of the performed action\n",
    "    trajectory.append({\n",
    "        \"step\": t + 1,\n",
    "        \"action\": switch_action[action],\n",
    "        \"obs\": next_obs,\n",
    "        \"reward\": reward\n",
    "    })\n",
    "    \n",
    "    # Update reward variables and step count\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    \n",
    "    # Display information of the current step\n",
    "    print(f\"Step {t}: Action: {switch_action[action]}, Obs: {next_obs}, Reward: {reward}, Terminated: {terminated}, Truncated: {truncated}\")\n",
    "    time.sleep(0.25)\n",
    "    \n",
    "# Display the final summary\n",
    "print(\"\\n--- Episode Summary ---\")\n",
    "print(f\"Total number of steps: {t}\")\n",
    "print(f\"Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVDVy2tIXLyD"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Pregunta</strong>\n",
    "Quin pot ser l'ojectiu d'utilitzar caselles de collor vermell fosc (amb una petita penalització) en comptes de caselles negres (sense penalització) en l'entrenament dels agents?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttFYwsVtXLyE"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "\n",
    "1. **Eficiència**: A l'utilitzar penalitzacions petites es fomenta que l'agent eviti trajectòries ineficients. En lloc de moure's aleatòriament sense cap conseqüència, l'agent aprèn a minimitzar el cost acumulat per arribar a la meta/objectiu trobant la ruta més curta i eficient. \n",
    "\n",
    "    \n",
    "\n",
    "2. **Simular entorn més realista**: En molts escenaris del món real, moure's per un espai té un cost associat (consum d'energia, temps o altres recursos). En fer ús de petites penalitzacions per a les caselles normals pot ajudar a simular aquestes condicions, fent que l'agent aprengui a prendre decisions més realistes.\n",
    "\n",
    "    \n",
    "\n",
    "3. **Millorar l'aprenentatge**: Les petites penalitzacions poden proporcionar un senyal d'aprenentatge de major riquesa, ajudant l'agent a diferenciar entre accions lleugerament millors o pitjors. Això pot accelerar el procés d'aprenentatge i millorar la qualitat  de les polítiques apreses.\n",
    "\n",
    "    \n",
    "\n",
    "4. **Exploració controlada**: Fer ús de penalitzacions petites pot ajudar a controlar l'exploració de l'agent, fent que eviti certes àrees de l'entorn que no són òptimes. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVDVy2tIXLyD"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.3</strong> (0,5 punts)\n",
    "\n",
    "A continuació, implementar un agent que dugui a terme la política òptima determinista, és a dir, que partint de la casella inicial [0,0] arribi a la casella final (de color verd brillant) amb la màxima recompensa acumulada. Quin és el valor del nombre de passos mínims? Quin és el retorn obtingut?\n",
    "\n",
    "Mostra la trajectòria seguida per l'agent i el retorn obtingut. No cal graficar-la, tan sols mostrar les accions i les caselles visitades en ordre i el retorn (recompensa total acumulada).\n",
    "\n",
    "Comenta els resultats.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07kPrBwtXLyD"
   },
   "outputs": [],
   "source": [
    "# Inicialitzem l'entorn\n",
    "obs, info = env.reset()\n",
    "t, total_reward, terminated, truncated = 0, 0, False, False\n",
    "\n",
    "print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "switch_action = {\n",
    "        0: \"Left\",\n",
    "        1: \"Down\",\n",
    "        2: \"Right\",\n",
    "        3: \"Up\",\n",
    "        4: \"Stay\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkX27sGtXLyD"
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "import gymnasium as gym\n",
    "\n",
    "# Optimal deterministic policy to reach the final cell with maximum reward\n",
    "optimal_policy = [\n",
    "    (1, \"Down\"), (1, \"Down\"),(1, \"Down\"), \n",
    "    (2, \"Right\"), (2, \"Right\"), (2, \"Right\"), \n",
    "    (4, \"Stay\")\n",
    "]\n",
    "\n",
    "print(\"Initial observation: {} \".format(obs))\n",
    "\n",
    "# List to store the trajectory\n",
    "trajectory = []\n",
    "\n",
    "# Execute the optimal policy\n",
    "for action_code, action_name in optimal_policy:\n",
    "    # Execute the action and get new observations and rewards\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action_code)\n",
    "    \n",
    "    # Store the information of the performed action\n",
    "    trajectory.append({\n",
    "        \"step\": t + 1,\n",
    "        \"action\": action_name,\n",
    "        \"obs\": next_obs,\n",
    "        \"reward\": reward\n",
    "    })\n",
    "    \n",
    "    # Update reward variables and step count\n",
    "    total_reward += reward\n",
    "    t += 1\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Display information of the current step\n",
    "    print(f\"Step {t}: Action: {action_name}, Obs: {next_obs}, Reward: {reward}, Terminated: {terminated}, Truncated: {truncated}\")\n",
    "    \n",
    "# Display the final summary\n",
    "print(\"\\n--- Episode Summary ---\")\n",
    "print(f\"Total number of steps: {t}\")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(\"-----------------------\\n\")\n",
    "\n",
    "# Display the trajectory\n",
    "for step_info in trajectory:\n",
    "    print(f\"Step {step_info['step']}: Action: {step_info['action']}, Observation: {step_info['obs']}, Reward: {step_info['reward']}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttFYwsVtXLyE"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "\n",
    "### Interpretació dels resultats\n",
    "    \n",
    "1. **Nombre de passos**: L'agent ha trigat 7 passos per arribar a la casella final amb la màxima recompensa. El resultat ve donat, ja que l'entorn és una graella de 4x4 amb parets centrals que obliguen l'agent a seguir una ruta més llarga rodejant les parets. \n",
    "    \n",
    "2. **Recompenses** : \n",
    "    - **Penalització lleu**: L'agent ha passat per 6 caselles amb penalització lleu (`BAD_SMALL`). Cada una amb una penalització de `-0.1`.\n",
    "    - **Casella final (`GOOD`)**: L'agent arriba a la casella final i es queda a la posició (`STAY`), obté una recompensa positiva de `+1.0`.\n",
    "    \n",
    "    \n",
    "3. **Recompensa total acumulada**:\n",
    "    - Penalitzacions lleus acumulades : `6 * (-0.1) = -0.6`\n",
    "    - Recompensa final : `+1.0`\n",
    "    - Recompensa total acumulada : `1.0 - 0.6 = 0.4`\n",
    "    \n",
    "L'agent ha aconseguit arribar a la casella final amb una recompensa total acumulada positiva, tot i les penalitzacions lleus trobades pel camí. A la vegada, ha evitat la casella amb una penalització severa (`BAD` amb `-10`) a la zona superior dreta. \n",
    " \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I9svxF9XLyG"
   },
   "source": [
    "## 3. Mètodes de Montecarlo (2 punts)\n",
    "\n",
    "L'objectiu d'aquest apartat és realitzar una estimació de la política òptima mitjançant els mètodes de Montecarlo. En concret estudiarem l'algoritme *On-policy first-visit MC control (per a polítiques $\\epsilon$-soft)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8GHrBhaXLyG"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.1</strong> (1 punt)\n",
    "\n",
    "Implementar l'Algorisme 3 explicat en el mòdul \"Mètodes de Montecarlo\": *On-policy first-visit MC control (per a polítiques $\\epsilon$-soft)* utilitzant els següents paràmetres:\n",
    "    \n",
    "<ul>\n",
    "  <li>Nombre d'episodis = 50.000</li>\n",
    "  <li>Epsilon inicial = 0,5</li>\n",
    "  <li>Factor de decaïment d'epsilon (<it>epsilon decay</it>) = 0,999</li>\n",
    "  <li>Mínim valor d'epsilon (<it>epsilon_min</it>) = 0,05</li>\n",
    "  <li>Actualitzar epsilon segons:  $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, \\epsilon_{\\textrm{min}})$$</li>\n",
    "  <li>Factor de descompte = 1</li>\n",
    "</ul>\n",
    "<b>Nota: als entrenaments dels agents es recomana utilitzar els entorns amb <code>render_mode = None</code> per a agilitzar l'execució.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzULGiJpXLyG"
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ COMPLETA ###########################\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "# Parameters\n",
    "num_episodes = 50000\n",
    "initial_epsilon = 0.5\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.05\n",
    "gamma = 1.0  # Discount factor\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=None)\n",
    "\n",
    "# Initialize epsilon\n",
    "epsilon = initial_epsilon\n",
    "\n",
    "# Initialize Q-table (action-value function)\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Initialize returns for state-action pairs\n",
    "returns_sum = defaultdict(float)\n",
    "returns_count = defaultdict(float)\n",
    "\n",
    "# Function to generate an episode following epsilon-soft policy\n",
    "def generate_episode(env, Q, epsilon):\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # epsilon-soft policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return episode\n",
    "\n",
    "# On-policy first-visit MC control algorithm for epsilon-soft policies\n",
    "for episode_num in range(1, num_episodes + 1):\n",
    "    # Generate a new episode\n",
    "    episode = generate_episode(env, Q, epsilon)\n",
    "    \n",
    "    # Initialize variables to compute cumulative reward\n",
    "    seen_state_action_pairs = set()\n",
    "    G = 0  # Cumulative reward initialized to 0\n",
    "    \n",
    "    # Process the episode in reverse order to calculate G\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = reward + gamma * G  # Update cumulative reward\n",
    "        \n",
    "        # First-visit check for state-action pair\n",
    "        if (state, action) not in seen_state_action_pairs:\n",
    "            seen_state_action_pairs.add((state, action))\n",
    "            \n",
    "            # Update returns and Q-value for the state-action pair\n",
    "            returns_sum[(state, action)] += G\n",
    "            returns_count[(state, action)] += 1\n",
    "            Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "    \n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    \n",
    "    # Print progress every 1000 episodes\n",
    "    if episode_num % 1000 == 0:\n",
    "        print(f\"Episode {episode_num}/{num_episodes} completed.\")\n",
    "\n",
    "# Extract the optimal policy from the Q-table\n",
    "optimal_policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "\n",
    "print(\"\\nOptimal policy learned.\")\n",
    "print(\"Q-values for each state-action pair:\")\n",
    "for state, actions in Q.items():\n",
    "    print(f\"State {state}: {actions}\")\n",
    "    \n",
    "print(\"\\nOptimal policy (ordered by states):\")\n",
    "for state in sorted(optimal_policy.keys()):\n",
    "    print(f\"State {state}: Action {optimal_policy[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "Q_mc = Q #Computed Q-table from MC algorithm\n",
    "\n",
    "#Q-table print\n",
    "print(\"---------Q-table----------\")\n",
    "for state, actions in Q_mc.items():\n",
    "    print(f\"State {state}:\")\n",
    "    for action, q_value in enumerate(actions):\n",
    "        print(f\"  Action {action}: Q-value = {q_value:.2f}\")\n",
    "print(\"\\n----------------------------\")\n",
    "\n",
    "# Optimal policy sorted by states\n",
    "optimal_policy = {state: np.argmax(actions) for state, actions in Q_mc.items()}\n",
    "print(\"\\nOptimal policy based on Q_mc (ordered by states)\\n\")\n",
    "for state in sorted(optimal_policy.keys()):\n",
    "    print(f\"State {state}: Action {optimal_policy[state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z-VxeHfXLyH"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.2</strong> (0.5 punts)\n",
    "Implementar una funció que imprimeixi per pantalla la política trobada per a cada casella a partir de la funció Q (aplicant una política <i>greedy</i>) i executar-la amb la funció Q obtinguda de l'entrenament del mètode de Montecarlo.\n",
    "Es tracta de la política òptima?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rhrFr8xtXLyK"
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "# Action mapping dictionary\n",
    "switch_action = {\n",
    "    0: \"Left\",\n",
    "    1: \"Down\",\n",
    "    2: \"Right\",\n",
    "    3: \"Up\",\n",
    "    4: \"Stay\",\n",
    "}\n",
    "\n",
    "def print_policy(Q, width, height):\n",
    "    print(\"Optimal Policy based on Q-values\\n\")\n",
    "    \n",
    "    # Iterate over each cell of the grid\n",
    "    for row in range(height):\n",
    "        row_policy = []\n",
    "        for col in range(width):\n",
    "            state = row * width + col  # Coordinates (row, col) to a linear index\n",
    "            \n",
    "            # Check if the state exists in the Q-table\n",
    "            if state in Q:\n",
    "                best_action = np.argmax(Q[state])  # Take action with highest Q value\n",
    "                row_policy.append(switch_action.get(best_action, \"None\"))  # Append action\n",
    "            else:\n",
    "                row_policy.append(\"None\")  # If the state does not exist, add \"None\"\n",
    "        \n",
    "        # Print the policy for the row\n",
    "        print(\" \".join(row_policy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i22S8zaNXLyK"
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "print_policy(Q_mc, width=4, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "\n",
    "Els resultats mostren la política òptima per al tauler de 4x4 a partir dels valors de Q obtinguts mitjançant l'algorisme de Montecarlo. La política que s'ha obtingut sembla ser una bona aproximació de la política òptima per al problema, tot i que caldria revisar els estats no explorats i assegurar-se que la política cobreixi tot el tauler de manera eficaç. La presència de 'None' en algunes àrees del tauler indica que cal més exploració i més entrenament per millorar la política. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPvEnKWHXLyL"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.3</strong> (0.5 punts)\n",
    "Executar un episodi amb la política trobada i mostrar la trajectòria de l'agent i el retorn obtingut. Comentar els resultats.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yYSTtAGjXLyL"
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "def execute_episode(Q, env):\n",
    "    #Init state by resetting the env\n",
    "    state, _ = env.reset() \n",
    "    done = False #Termination episode flag to false\n",
    "    trajectory = []  # Store agent's trajectory\n",
    "    total_reward = 0  # Init total reward\n",
    "\n",
    "    while not done:\n",
    "        # Get best action based on Qtable \n",
    "        best_action = np.argmax(Q[state])\n",
    "        \n",
    "        # take chosen action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(best_action)\n",
    "        \n",
    "        # Store state,action,reward\n",
    "        trajectory.append((state, best_action, reward))\n",
    "        \n",
    "        # Update reward and state\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if the episode it is finished\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    # Display agent's trajectory\n",
    "    print(\"\\nAgent's trajectory:\")\n",
    "    for step in trajectory:\n",
    "        state, action, reward = step\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
    "    \n",
    "    # Disply total reward\n",
    "    print(f\"\\nTotal return for the episode: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WM2JvlXCXLyL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "execute_episode(Q_mc,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "El total acumulat de la recompensa és de 0.4, el qual correspon a la suma de les recompenses obtingudes en cada pas del trajecte. Inicialment, tenim diverses recompenses lleus negatives (-0.1) i una recompensa positiva (1.0) en arribar a la casella final. Analitzant la trajectòria seguida, podem veure que el retorn total de l'episodi és relativament baix, però correspon al valor màxim possible donat l'entorn en el qual ens trobem. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzg9iWfjXLyL"
   },
   "source": [
    "## 4. Mètodes d'Diferència Temporal (3.5 punts)\n",
    "\n",
    "L'objectiu d'aquest apartat és realitzar una estimació de la política òptima mitjançant els mètodes de Diferència Temporal en l'entorn *Gridworld* creat anteriorment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Mètode SARSA (2 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3xqC9vCXLyL"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.1.1</strong> (1 pt)\n",
    "\n",
    "Implementar l'algoritme *SARSA* explicat al mòdul 6 \"Aprenentatge per Diferència Temporal\" y executar-lo utilizant els següents paràmetres:\n",
    "<ul>    \n",
    "    <li>Número d'episodis = 10.000</li>\n",
    "    <li>learning rate = 0,2</li>\n",
    "    <li>discount factor = 1</li>\n",
    "    <li>epsilon = 0,5</li>\n",
    "    <li>epsilon decay = 0,9</li>\n",
    "    <li>mínim valor d'epsilon = 0,05</li>\n",
    "</ul>\n",
    "Actualitzar el valor d'epsilon segons: $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, \\epsilon_{\\textrm{min}})$$\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=None)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "#SARSA Algorithm\n",
    "def sarsaAlgorithm(env, num_episodes, learning_rate, discount_factor, epsilon, epsilon_decay, epsilon_min):\n",
    "    #Init Qtalbe\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    #store deltas\n",
    "    deltas = []\n",
    "\n",
    "    #Generate an episode following an epsilon-soft policy\n",
    "    def generate_episode(Q, epsilon):\n",
    "        episode = []\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        # Choose an initial action following the epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() #Random action\n",
    "        else:\n",
    "            action = np.argmax(Q[state]) #Greedy action -> max Q value\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            #store the episode (state, action, reward)\n",
    "            episode.append((state, action, reward))\n",
    "            \n",
    "            #Update state and action\n",
    "            state = next_state\n",
    "            if np.random.rand() < epsilon:\n",
    "                next_action = env.action_space.sample() # Random action\n",
    "            else:\n",
    "                next_action = np.argmax(Q[state]) #greedy action -> max Q value\n",
    "            \n",
    "            action = next_action #update the action\n",
    "            \n",
    "            done = terminated or truncated\n",
    "        \n",
    "        return episode\n",
    "    \n",
    "    #loop per each episode\n",
    "    for episode_num in range(1, num_episodes + 1):\n",
    "        #new episode\n",
    "        episode = generate_episode(Q, epsilon)\n",
    "        \n",
    "        #update qtable for each step in the episode\n",
    "        for t in range(len(episode) - 1):\n",
    "            state, action, reward = episode[t]\n",
    "            next_state, next_action, _ = episode[t + 1]\n",
    "            \n",
    "            #store qvalues before compute delta\n",
    "            old_q_value = Q[state][action]\n",
    "            \n",
    "            #Update Qvaluw using SARSA rules\n",
    "            Q[state][action] += learning_rate * (reward + discount_factor * Q[next_state][next_action] - old_q_value)\n",
    "            \n",
    "            #Compute TD error and append\n",
    "            td_error = abs(Q[state][action] - old_q_value)\n",
    "            deltas.append(td_error)\n",
    "        \n",
    "        #update epsilon\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        #Print progressx\n",
    "        if episode_num % 1000 == 0:\n",
    "            print(f\"Episode {episode_num}/{num_episodes} completed.\")\n",
    "\n",
    "    #optimal policy based on Qtable\n",
    "    optimal_policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "\n",
    "    print(\"\\nOptimal policy based on SARSA:\")\n",
    "    for state, action in optimal_policy.items():\n",
    "        print(f\"State {state}: Action {action}\")\n",
    "\n",
    "    return Q, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "num_episodes = 10000\n",
    "learning_rate = 0.2\n",
    "gamma = 1.0  #discount factor\n",
    "initial_epsilon = 0.5\n",
    "epsilon_decay = 0.9\n",
    "epsilon_min = 0.05\n",
    "#Init epsilon\n",
    "epsilon = initial_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "Q_sarsa, deltas = sarsaAlgorithm(env, num_episodes, learning_rate, gamma, epsilon, epsilon_decay, epsilon_min)\n",
    "\n",
    "# Qsarsa (uncomment to see)\n",
    "# print(\"\\nQ-values from SARSA:\")\n",
    "# for state, actions in Q_sarsa.items():\n",
    "#     print(f\"State {state}: {actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4.1.2</strong> (0.5 pts)\n",
    "Imprimir una gràfica amb l'evolució del més gran error TD de cada episodi. Atès que l'error té molta variància, imprimiu també la mitjana mòbil amb una finestra temporal de 100 episodis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Moving average 100 episodes \n",
    "moving_average = np.convolve(deltas, np.ones(100)/100, mode='valid')\n",
    "\n",
    "#TD error evolution plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(deltas, label='TD Error per Step', color='blue')\n",
    "plt.plot(moving_average, label='Moving Average (100 episodes)', color='red', linestyle='dashed')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('TD Error')\n",
    "plt.title('TD Error Evolution and Moving Average (SARSA)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.1.3</strong> (0.25 pts)\n",
    "Imprimir la política trobada amb el mètode SARSA per a cada estat (podeu re-utilitzar la funció creada a l'apartat anterior dels mètodes MC). Es tracta d'una política òptima?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "print_policy(Q_sarsa, width=4, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "\n",
    "Reutilitzem la funció ```print_policy()``` anterior però en aquest cas utilitzant el paràmetre ```Q_sarsa```. La política trobada amb SARSA és coherent amb els objectius del problema i pot considerar-se òptima o propera a l'òptima. Això dependrà de factors com el nombre d'episodis, la taxa d'aprenentatge i l'estratègia d'exploració. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4.1.4</strong> (0.25 punts)\n",
    "Executar un episodi amb la política trobada i mostrar la trajectòria de l'agent i el retorn obtingut. Comentar els resultats.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "execute_episode(Q_sarsa,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "\n",
    "Reutilitzem la funció ```execute_episode()``` anterior però en aquest cas utilitzant el paràmetre ```Q_sarsa```. \n",
    "\n",
    "El resultat és idèntic a l'obtingut utilitzant el mètode de Montecarlo. L'agent ha seguit la política apresa per assolir l'estat objectiu amb un retorn acumulat positiu. Aquesta trajectòria i retorn demostren que fent ús de SARSA ha estat capaç de trobar una política que sembla ser òptima o propera a òptima. \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Mètode Q-Learning (1.5 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3xqC9vCXLyL"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.2.1</strong> (0.75 pts)\n",
    "\n",
    "Implementar l'algoritme *Q-learning* explicat al mòdul 6 \"Aprenentatge per Diferència Temporal\" y executar-lo utilizant els següents paràmetres:\n",
    "<ul>    \n",
    "    <li>Número de episodios = 5.000</li>\n",
    "    <li>learning rate = 0,4</li>\n",
    "    <li>discount factor = 1</li>\n",
    "    <li>epsilon = 0,5</li>\n",
    "    <li>epsilon decay = 0,9</li>\n",
    "    <li>mínim valor d'epsilon = 0,05</li>\n",
    "</ul>\n",
    "Actualitzar el valor d'epsilon segons: $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, \\epsilon_{\\textrm{min}})$$\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuY9efT4XLyL"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=None)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "# Q-learning Algorithm\n",
    "def qLearning(env, num_episodes, learning_rate, discount_factor, epsilon, epsilon_decay, epsilon_min):\n",
    "    #init Qtable\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    #Store deltas\n",
    "    deltas = []\n",
    "\n",
    "    #Loop though episodes\n",
    "    for episode_num in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()  #reset env\n",
    "        done = False\n",
    "        episode_td_error = [] #current episode td error\n",
    "\n",
    "        #run episode\n",
    "        while not done:\n",
    "            #Select action using epsilon-greedy policy\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample() #Random action\n",
    "            else:\n",
    "                action = np.argmax(Q[state]) #Greedy action \n",
    "            \n",
    "            #Execute action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            #Updata Qvalue using qLearning rule\n",
    "            old_q_value = Q[state][action] #Save qvalue to compute delta\n",
    "            best_next_action = np.argmax(Q[next_state]) #Take best action for the next state\n",
    "            Q[state][action] += learning_rate * (reward + discount_factor * Q[next_state][best_next_action] - old_q_value)\n",
    "            \n",
    "            #Calculate and store the TD error\n",
    "            td_error = abs(Q[state][action] - old_q_value)\n",
    "            episode_td_error.append(td_error)\n",
    "            \n",
    "            #update current state\n",
    "            state = next_state\n",
    "            \n",
    "            #Check if episode completed\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        #Append max TD error for this episode\n",
    "        deltas.append(np.max(episode_td_error))\n",
    "        \n",
    "        #update epsilon\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "        \n",
    "        #Print progress\n",
    "        if episode_num % 1000 == 0:\n",
    "            print(f\"Episode {episode_num}/{num_episodes} completed.\")\n",
    "\n",
    "    #optimal policy based on Qtable\n",
    "    print(\"\\nOptimal policy based on Q-learning:\")\n",
    "    optimal_policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "    for state, action in optimal_policy.items():\n",
    "        print(f\"State {state}: Action {action}\")\n",
    "    \n",
    "    return Q, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "# Parameters\n",
    "num_episodes = 5000\n",
    "learning_rate = 0.4\n",
    "gamma = 1.0  #discount factor\n",
    "initial_epsilon = 0.5\n",
    "epsilon_decay = 0.9\n",
    "epsilon_min = 0.05\n",
    "\n",
    "#init epsilon\n",
    "epsilon = initial_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "Q_qlearning, deltas = qLearning(env, num_episodes, learning_rate, gamma, epsilon, epsilon_decay, epsilon_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4.2.2</strong> (0.25 pts)\n",
    "Imprimir una gràfica amb l'evolució del més gran error TD de cada episodi. Atès que l'error té molta variància, imprimiu també la mitjana mòbil amb una finestra temporal de 100 episodis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Moving average 100 episodes \n",
    "window_size = 100\n",
    "moving_avg_td_errors = np.convolve(deltas, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "#TD error evolution plot and moving average\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(deltas, label='Max TD Error per Episode')\n",
    "plt.plot(np.arange(window_size - 1, num_episodes), moving_avg_td_errors, label=f'Moving Average (window={window_size})', color='red', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('TD Error')\n",
    "plt.title('TD Error Evolution in Q-learning')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio 4.2.3</strong> (0.25 pts)\n",
    "Imprimir la política trobada amb el mètode Q-learning per a cada estat (podeu re-utilitzar la funció creada a l'apartat anterior). Es tracta d'una política òptima?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "print_policy(Q_qlearning, width=4, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "    \n",
    "La política trobada sembla coherent amb un entorn en què l'agent ha après a moure's cap a les zones més favorables. Tot i que la política sembla correcta per a molts estats, aquesta depenent de l'execució va variant i en alguns casos obtenim una ```optimal_policy``` que dista del que un pot pensar de primeres en alguns dels estats. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4.2.4</strong> (0.25 punts)\n",
    "Executar un episodi amb la política trobada i mostrar la trajectòria de l'agent i el retorn obtingut. Comentar els resultats.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "execute_episode(Q_qlearning,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "\n",
    "En executar l'episodi (múltiples vegades) amb la política trobada, desemboquem en el resultat que hem obtingut utilitzant tots els altres mètodes. Recompensa total acumulada de l'episodi és un valor positiu de ```0.4```.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjWsuHJEXLyN"
   },
   "source": [
    "## 5. Comparativa dels algoritmes (0.5 punts)\n",
    "\n",
    "En aquest apartat farem una petita comparativa dels mètodes programats en els apartats anteriors.\n",
    "\n",
    "Compararem el comportament dels algoritmes en termens de la política assolida, la duració de l'entrenament i el factor de descompte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9sS5ijiXLyO"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 5.1 - Anàlisi de les polítiques obtingudes i del temps de convergència</strong> (0,25 punts)\n",
    "\n",
    "Realitzar un estudi de les polítiques obtingudes responent a les següents preguntes:\n",
    "<ul>\n",
    "  <li>Tots els algoritmes aconsegueixen arribar a la política òptima?</li>\n",
    "  <li>Triguen el mateix temps en convergir?</li>\n",
    "  <li>A què poden ser degudes les diferències?</li>\n",
    "</ul>\n",
    "<b>Nota: Es recomana executar cada algoritme diverses vegades per extreure unes conclusions més consistents.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBncS-4SXLyO"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONS:</strong>\n",
    "\n",
    "### 1. Política òptima\n",
    "\n",
    "Anàlisi realitzat després d'executar diverses vegades cada algorisme.\n",
    "\n",
    "#### 1.1 SARSA \n",
    "\n",
    "Algorisme basat en el control d'estratègies per aprendre la política òptima, però depèn de la política d'exploració (epsilon-greedy). \n",
    "\n",
    "La quantitat més gran de vegades s'arriba a una política òptima. En alguna de les execucions, alguns estats podien tenir alguna variació. Es pot quedar atrapat en una política subòptima a causa de les accions seleccionades durant la fase d'explotació.\n",
    "\n",
    "#### 1.2 Q-Learning\n",
    "\n",
    "Algorisme basat en la política de valor que utilitza el valor màxim de la funció ```Q``` en l'estat següent per actualitzar la taula de ```Q-values```. No depèn de l'estratègia seguida en l'exploració de l'entorn. Igual que en el cas de SARSA, la major quantitat de vegades arriba a una política òptima, tot i que en algunes execucions, alguns estats podien tenir alguna variació.\n",
    "\n",
    "### 2. Convergència\n",
    "\n",
    "#### 2.1 SARSA \n",
    "\n",
    "SARSA actualitza la taula Q a mesura que l'agent segueix una política específica (epsilon-greedy), la qual cosa significa que, en gran part del temps, l'agent explora l'entorn. Això desemboca en què SARSA necessiti un nombre d'episodis més gran per convergir completament. \n",
    "\n",
    "#### 2.2 Q-Learning\n",
    "\n",
    "Q-Learning, és més directe en el càlcul de la política òptima, ja que sempre utilitza l'acció que maximitza el valor Q en l'estat següent. Això fa que, Q-Learning convergeixi més ràpidament a la política  òptima en comparació amb SARSA.\n",
    " \n",
    "\n",
    "### 3. Diferències\n",
    "\n",
    "#### Exploració vs Explotació\n",
    "\n",
    "SARSA utilització una política ```epsilon-greedy```, la qual cosa implica que l'agent explora accions subòptimes durant l'aprenentatge. Per altra banda, Q-learning fa una explotació més directa a través de la selecció de la millor acció en cada estat. El fa més eficient en termes de convergència cap a la política òptima.\n",
    "\n",
    "#### Èpsilon i Decaïment\n",
    "\n",
    "La taxa de canvi d'èpsilon té un gran impacte en el temps de convergència pels dos mètodes. \n",
    "\n",
    "SARSA és més sensible a la configuració d'èpsilon i la seva disminució. Un decaïment més ràpid d'èpsilon en SARSA pot limitar l'exploració, mentre que un decaïment més lent pot allargar el procés d'aprenentatge.\n",
    "\n",
    "Q-learning pot ser més robust a aquests canvis perquè sempre utilitza la millor acció en el següent estat.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMobYk_5XLyO"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 5.2 - Influència del factor de descompte (discount factor)</strong> (0,25 punts)\n",
    "\n",
    "Tots els agents s'han entrenat amb recompenses sense descomptar (factor de descompte = 1). A què creus que es deu aquesta elecció? Creus que millorarien els resultats si s'utilitza un factor de descompte diferent? Per què? En cas afirmatiu, selecciona un nou factor de descompte i testeja'l a algun dels algortimes (per exemple a Q-learning).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS2P1vMYXLyP"
   },
   "outputs": [],
   "source": [
    "#SOLUCIÓ\n",
    "#New Q-learning parameters\n",
    "num_episodes = 5000\n",
    "learning_rate = 0.4\n",
    "gamma = 0.9  #discount factor\n",
    "init_epsilon = 0.5\n",
    "epsilon_decay = 0.9\n",
    "epsilon_min = 0.05\n",
    "#init epsilon\n",
    "init_epsilon = epsilon \n",
    "\n",
    "#Init env\n",
    "env = gym.make(\"Gym-Gridworlds/Ex2-4x4-v0\", render_mode=None)\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))\n",
    "\n",
    "Q_qlearning_C, deltas_C = qLearning(env, num_episodes, learning_rate, discount_factor, epsilon, epsilon_decay, epsilon_min)\n",
    "\n",
    "print(\"\\nOptimal Policy based on Q-learning with gamma=0.9:\")\n",
    "print_policy(Q_qlearning, width=4, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlNW-nJYXLyP"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONS:</strong>\n",
    "    \n",
    "El factor de descompte és un paràmetre important en els algoritmes d'aprenentatge per reforç. En aquest cas, s'ha optat per un factor de descompte d'1, la qual cosa implica que l'agent considera totes les recompenses futures com igual d'importants que les recompenses immediates. Això pot ser adequat per a problemes a llarg termini, on l'objectiu és aconseguir el millor retorn possible en tots els passos de l'episodi, independentment de quan es produeixin les recompenses.\n",
    "\n",
    "L'elecció d'un factor de descompte més petit podria comportar canvis importants en el comportament de l'agent, ja que faria que aquest prioritzés més les recompenses immediates, en lloc de considerar només les recompenses futures. A la vegada, un factor de descompte molt més petit podria fer que l'agent focalitzi massa les recompenses properes deixant de banda els resultats a llarg termini.\n",
    "\n",
    "Segons sembla, fer servir un factor de descompte de ```0.9``` es troba bastant balancejat entre les recompenses immediates i futures, tot i això, tampoc sembla que acabi retornant una política òptima.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Diferència de polítiques entre SARSA i Q-Learning (1.5 punts)\n",
    "\n",
    "En aquest darrer apartat dissenyarem un entorn una mica més complicat i compararem les diferents polítiques que assoleixen els mètodes SARSA i Q-learning.\n",
    "\n",
    "Per a això es demana:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 6.1</strong> (0,5 punts)\n",
    "\n",
    "Crear un entorn nou amb una graella 5x5 com la que s'observa a continuació (utilitzeu <code>\"5X5_Ex6\"</code> com a clau de l'entrada al diccionari <code>GRIDS</code>.):\n",
    "<br><br>\n",
    "<img src=\"images/Gym-Gridworld_Ex6_5x5.png\" alt=\"Mi imagen\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "<br>\n",
    "Fixeu-vos que la casella inicial no es troba a dalt a l'esquerra si no a la meitat de la primera columna. Per a aconseguir això heu de crear una sub-classe de la classe <code>Gridworld</code> i afegir-la al final de l'arxiu <code>\\gym_gridworlds\\gridworld.py</code> (en el mateix arxiu teniu algun exemple).\n",
    "\n",
    "Enrecordeu-vos de registrar l'entorn a l'arxiu <code>\\gym_gridworlds\\\\\\_\\_init\\_\\_.py</code> amb els paràmetres pertinents (fixeu-vos en altres registres com es crida a una sub-classe), reiniciar el kernel del Notebook i tornar a importar els paquets.\n",
    "\n",
    "L'entorn s'ha de poder cridar amb el següent codi:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(5) \n",
      "Observation space is Discrete(25) \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Ex6-5x5-v0\", render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 6.2 - Entrenament i comparativa dels agents</strong> (1 punt)\n",
    "\n",
    "<ol>\n",
    "  <li>Entrena els 2 agents TD (SARSA i Q-learning) en el nou entorn. Ajusta els diferents paràmetres fins que els algoritmes convergeixin.</li>\n",
    "  <li>Imprimeix per pantalla la política assolida per cada agent.</li>\n",
    "  <li>Executa un episodi amb cada política i mostra els resultats (accions, recorregut i recompensa total).</li>\n",
    "  <li>Comenta els resultats. Són els resultats esperats?</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(5) \n",
      "Observation space is Discrete(25) \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Gym-Gridworlds/Ex6-5x5-v0\", render_mode=None)\n",
    "\n",
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/5000 completed.\n",
      "Episode 2000/5000 completed.\n",
      "Episode 3000/5000 completed.\n",
      "Episode 4000/5000 completed.\n",
      "Episode 5000/5000 completed.\n",
      "\n",
      "Optimal policy based on SARSA:\n",
      "State 10: Action 3\n",
      "State 5: Action 3\n",
      "State 15: Action 3\n",
      "State 21: Action 0\n",
      "State 20: Action 0\n",
      "State 17: Action 3\n",
      "State 16: Action 0\n",
      "State 0: Action 4\n",
      "State 1: Action 0\n",
      "State 22: Action 2\n",
      "State 2: Action 3\n",
      "State 3: Action 0\n",
      "State 4: Action 0\n",
      "State 9: Action 0\n",
      "State 14: Action 3\n",
      "State 13: Action 2\n",
      "State 12: Action 4\n",
      "State 19: Action 3\n",
      "State 18: Action 1\n",
      "State 23: Action 0\n",
      "State 24: Action 0\n",
      "CPU times: total: 3.11 s\n",
      "Wall time: 5.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "######################## SOLUCIÓ ###########################\n",
    "#Entrenament SARSA\n",
    "Q_sarsa, deltas = Q_sarsa, deltas_sarsa = sarsaAlgorithm(env, num_episodes, learning_rate, gamma, epsilon, epsilon_decay, epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/5000 completed.\n",
      "Episode 2000/5000 completed.\n",
      "Episode 3000/5000 completed.\n",
      "Episode 4000/5000 completed.\n",
      "Episode 5000/5000 completed.\n",
      "\n",
      "Optimal policy based on Q-learning:\n",
      "State 10: Action 1\n",
      "State 15: Action 1\n",
      "State 20: Action 2\n",
      "State 21: Action 2\n",
      "State 22: Action 2\n",
      "State 16: Action 1\n",
      "State 17: Action 3\n",
      "State 23: Action 3\n",
      "State 18: Action 3\n",
      "State 24: Action 0\n",
      "State 5: Action 1\n",
      "State 0: Action 2\n",
      "State 12: Action 4\n",
      "State 19: Action 3\n",
      "State 13: Action 0\n",
      "State 1: Action 2\n",
      "State 2: Action 2\n",
      "State 3: Action 2\n",
      "State 4: Action 1\n",
      "State 9: Action 1\n",
      "State 14: Action 0\n",
      "CPU times: total: 547 ms\n",
      "Wall time: 790 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "######################## SOLUCIÓ ###########################\n",
    "#Entrenament Q-learning\n",
    "Q_qlearning, deltas_qlearning = qLearning(env, num_episodes, learning_rate, gamma, epsilon, epsilon_decay, epsilon_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy based on SARSA:\n",
      "Optimal Policy based on Q-values\n",
      "\n",
      "Stay Left Up Left Left\n",
      "Up None None None Left\n",
      "Up None Stay Right Up\n",
      "Up Left Up Down Up\n",
      "Left Left Right Left Left\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "#Política SARSA\n",
    "print(\"\\nOptimal Policy based on SARSA:\")\n",
    "print_policy(Q_sarsa, width=5, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing episode with SARSA policy:\n",
      "\n",
      "Agent's trajectory:\n",
      "State: 10, Action: 3, Reward: -0.1\n",
      "State: 5, Action: 3, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "State: 0, Action: 4, Reward: -0.1\n",
      "\n",
      "Total return for the episode: -9.99999999999998\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "#Execució episodi SARSA\n",
    "print(\"\\nExecuting episode with SARSA policy:\")\n",
    "execute_episode(Q_sarsa, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy based on Q-learning:\n",
      "Optimal Policy based on Q-values\n",
      "\n",
      "Right Right Right Right Down\n",
      "Down None None None Down\n",
      "Down None Stay Left Left\n",
      "Down Down Up Up Up\n",
      "Right Right Right Up Left\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "#Política O-learning\n",
    "print(\"\\nOptimal Policy based on Q-learning:\")\n",
    "print_policy(Q_qlearning, width=5, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing episode with Q-learning policy:\n",
      "\n",
      "Agent's trajectory:\n",
      "State: 10, Action: 1, Reward: -0.1\n",
      "State: 15, Action: 1, Reward: -0.1\n",
      "State: 20, Action: 2, Reward: -0.1\n",
      "State: 21, Action: 2, Reward: -0.1\n",
      "State: 22, Action: 2, Reward: -0.1\n",
      "State: 23, Action: 3, Reward: -0.1\n",
      "State: 18, Action: 3, Reward: -0.1\n",
      "State: 13, Action: 0, Reward: -0.1\n",
      "State: 12, Action: 4, Reward: 1.0\n",
      "\n",
      "Total return for the episode: 0.20000000000000007\n"
     ]
    }
   ],
   "source": [
    "######################## SOLUCIÓ ###########################\n",
    "#Execució episodi Q-learning\n",
    "print(\"\\nExecuting episode with Q-learning policy:\")\n",
    "execute_episode(Q_qlearning, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>CONCLUSIONS:</strong>\n",
    "<br><br>\n",
    "\n",
    "Després d'executar ambdós algorismes en un mateix entorn i amb els mateixos paràmetres d'entrada, podem extreure les següents conclusions comparatives.\n",
    "\n",
    "##### Convergència i exploració\n",
    "\n",
    "L'algorisme Q-learning tendeix a convergir més ràpidament i de manera més eficient, ja que actualitza la ```Q-value``` per les millors accions possibles, independentment de les accions que prengui l'agent durant l'entrenament. SARSA, per altra banda, segueix una estratègia més lenta i conservadora, actualitzant la ```Q-value``` d'acord amb les accions realment preses durant l'episodi, fet que provoca que l'agent es quedi atrapat amb certes accions subòptimes. \n",
    "\n",
    "##### Recompenses\n",
    "\n",
    "L'agent de Q-learning ha obtingut una recompensa total positiva de ```(0.2)```, indicant que ha trobat un camí parcialment eficient. L'agent de SARSA ha quedat atrapat en un patró de recompenses negatives, inidicant d'aquesta manera una falta d'exploració efectiva.\n",
    "\n",
    "##### Comportament esperat\n",
    "\n",
    "Els resultats de SARSA no són els esperats donat que l'agent queda presoner de les accions 'Stay' i no aconsegueix una recompensa significativa. Els resultats de Q-learning són més propers als esperats, explorant l'entorn de manera més efectiva i rebent una recompensa positiva. \n",
    " \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
