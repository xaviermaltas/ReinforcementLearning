{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATAcq7bQyNxe"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"https://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/llibre-estil/logo-UOC-masterbrand-vertical.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.983 · Aprenentatge per reforç</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2024-1 · Màster universitari en Ciència de dades (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis d'informàtica, multimèdia i telecomunicació</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "# PAC2: Deep Reinforcement Learning\n",
    "\n",
    "\n",
    "En aquesta pràctica s'implementaran models de DRL en dos entorns diferents, amb l'objectiu d'analitzar diferents formes d'aprenentatge d'un agent i estudiar-ne el rendiment. L'agent serà entrenat amb els mètodes:\n",
    "<ol>\n",
    "    <li>DQN</li>\n",
    "    <li>Dueling DQN</li>\n",
    "</ol>\n",
    "  \n",
    "**Important: El lliurament s'ha de fer en format notebook i en format html on es vegi el codi, els resultats i comentaris de cada exercici. Per exportar el notebook a html es pot fer des del menú File → Download as → HTML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZXQSvJSyNxk"
   },
   "source": [
    "## 0. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zan-tCv_zJXF"
   },
   "source": [
    "L'aprenentatge per reforç és un camp de la intel·ligència artificial que cerca desenvolupar sistemes capaços d'aprendre i prendre decisions autònomes a través de la interacció amb el seu entorn. Al llarg dels anys aquest enfocament ha demostrat la seva capacitat per abordar una àmplia gamma d'aplicacions, des de jocs de taula fins a robòtica i gestió de recursos. Tot i això, una de les qüestions més desafiadores en l'aprenentatge per reforç és la creació d'entorns de simulació adequats que reflecteixin fidelment el context de l'aplicació desitjada.\n",
    "\n",
    "\n",
    "En aquest context, aquesta PAC té com a objectiu desenvolupar un nou entorn de simulació que permeti la investigació i l'experimentació amb diferents agents de trading. Aquest entorn estarà dissenyat específicament per abordar un problema fictici d'inversió i gestió d'un portafoli al mercat de valors, en el que un agent ha d'aprendre a prendre decisions òptimes de compra, venda o manteniment d'accions. L'objectiu de l'agent serà maximitzar els guanys al llarg del temps mitjançant estratègies basades en l'aprenentatge per reforç.\n",
    "\n",
    "Per a portar-ho a terme, s'utilitzarà un entorn adaptat a les especificacions de Gymnasium (https://gymnasium.farama.org/index.html), que permet la creació d'entorns personalitzats per a l'aprenentatge per reforç. Aquest entorn simularà el comportament dinàmic d'un mercat financer, amb fluctuacions en els preus de les accions i esdeveniments del mercat que afecten les decisions de l'agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzjteFjb1JS1"
   },
   "source": [
    "## 1. Creació d'un entorn en Gym (3 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1J3ZQQ2zJXh"
   },
   "source": [
    "En aquest exercici dissenyem un entorn senzill seguint l'esquema dels entorns de <code>Gymnasium</code>, i tractarem de resoldre'l.\n",
    "\n",
    "Els entorns de <code>Gymnasium</code> acostumen a tenir la següent estructura:\n",
    "\n",
    "```\n",
    "class FooEnv(gym.Env):\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self):\n",
    "    ...\n",
    "  def step(self, action):\n",
    "    ...\n",
    "    return new_state, reward, terminated, truncated, info\n",
    "\n",
    "  def reset(self):\n",
    "    ...\n",
    "    return observation, info\n",
    "\n",
    "  def render(self, mode='human', close=False):\n",
    "    ...\n",
    "\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U1O0HWGzJXh"
   },
   "source": [
    "El primer pas serà instal·lar les llibreries necessàries per abordar la PAC:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TYp6OAfE1JS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\xavim\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: torch in c:\\users\\xavim\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\xavim\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\xavim\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\xavim\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (5.29.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: tdqm in c:\\users\\xavim\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tdqm) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from tqdm->tdqm) (0.4.6)\n",
      "Requirement already satisfied: tabulate in c:\\users\\xavim\\anaconda3\\lib\\site-packages (0.8.10)\n",
      "Requirement already satisfied: yfinance==0.2.50 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (0.2.50)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (2.31.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (4.9.3)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (2023.3.post1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (3.17.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (4.12.2)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from yfinance==0.2.50) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance==0.2.50) (2.4)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance==0.2.50) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from html5lib>=1.1->yfinance==0.2.50) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance==0.2.50) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from pandas>=1.3.0->yfinance==0.2.50) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance==0.2.50) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance==0.2.50) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance==0.2.50) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from requests>=2.31->yfinance==0.2.50) (2023.11.17)\n",
      "Requirement already satisfied: pandas in c:\\users\\xavim\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xavim\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install torch\n",
    "\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install tensorboard\n",
    "!pip install tdqm\n",
    "!pip install tabulate\n",
    "!pip install yfinance==0.2.50\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsmRtf2Z1JS3"
   },
   "source": [
    "y las importamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Version: 1.0.0\n",
      "Torch Version: 2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"Gym Version:\", gym.__version__)  # 1.0.0\n",
    "print(\"Torch Version:\", torch.__version__)  # 2.5.1+cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVVI0tZnp9v2"
   },
   "source": [
    "### 1.1 Entorn de Simulació per al Trading Automàtic al Mercat de Valors\n",
    "\n",
    "**Enhorabona!** Una firma d'inversió ha decidit contractar-vos per desenvolupar un sistema de trading automàtic per a les seves operacions al mercat de valors. Per a això, us demanen que dissenyeu un entorn de simulació que permeti entrenar un agent capaç de prendre decisions de compra, venda o mantenir posicions sobre una acció determinada, maximitzant els guanys al llarg del temps. L'entorn ha de complir les següents especificacions:\n",
    "\n",
    "- L'entorn s'ha d'anomenar `StockMarketEnv`.\n",
    "- L'entorn ha d'heretar de la classe `gym.Env`.\n",
    "- El preu inicial de l'acció estarà basat en dades històriques, obtingudes a partir d'una consulta a Yahoo Finance.\n",
    "- El balanç inicial de l'agent serà de 10.000 dòlars, els quals es podran utilitzar per comprar accions.\n",
    "- L'agent pot realitzar les següents accions:\n",
    "  - `0` -> Mantenir (no es realitzen operacions).\n",
    "  - `1` -> Comprar (es compren totes les accions possibles al preu actual).\n",
    "  - `2` -> Vendre (es venen totes les accions disponibles al preu actual).\n",
    "- El sistema de recompenses serà el següent:\n",
    "  - L'agent rep una recompensa de +1 si el valor net del seu portafoli (balanç_actual + balanç_anterior) augmenta respecte al pas anterior.\n",
    "  - L'agent rep una recompensa de +1 si el valor net del seu portafoli (balanç_actual) es manté igual, no posseeix cap acció i el valor de les accions disminueix. Aquesta comprovació no es realitza el primer dia de trading.\n",
    "  - L'agent rep una recompensa de -1 si el valor net del seu portafoli (balanç_actual) es manté igual, no posseeix cap acció i el valor de les accions augmenta respecte al dia anterior. Aquesta comprovació no es realitza el primer dia de trading.\n",
    "  - Si el valor net disminueix respecte al dia anterior, l'agent rep una recompensa de -1.\n",
    "  - En altres casos, rep una puntuació de 0.\n",
    "- L'entorn tindrà una durada per defecte per a l'entrenament del `2019-01-01` al `2021-01-01`.\n",
    "- L'entorn finalitzarà si el valor net del portafoli cau per sota del 85% del balanç inicial (és a dir, 8.500 dòlars).\n",
    "\n",
    "L'objectiu d'aquest entorn és que l'agent aprengui a prendre decisions òptimes de compra i venda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yuHxDz3ZXawz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym Version: 1.0.0\n",
      "Torch Version: 2.5.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"Gym Version:\", gym.__version__)  # 1.0.0\n",
    "print(\"Torch Version:\", torch.__version__)  # 2.5.1+cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EulU8U8F1JS4"
   },
   "source": [
    "\n",
    "![Imagen de Stock Market](https://media1.tenor.com/m/wWvt6qEQB8EAAAAd/kah.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWhwescU1JS4"
   },
   "source": [
    "#### 1.1.1 Implementació dels indicadors econòmics\n",
    "\n",
    "El primer pas és implementar dues funcions anomenadaes `calculate_rsi` i `calculate_ema` que calcularan diferents indicadors tècnics utilitzats en l'anàlisi de mercats financers. Aquests indicadors ajudaran els agents de trading a prendre decisions basades en patrons i tendències del mercat.\n",
    "\n",
    "A continuació, s'explica en què consisteixen aquestes mètriques:\n",
    "\n",
    "- **RSI (Índex de Força Relativa)**: Calcula l'RSI utilitzant el canvi de preu durant una finestra de temps especificada. Aquest indicador mostra si un actiu està sobrecomprat o sobrevenut. Podeu veure una explicació més detallada a https://es.wikipedia.org/wiki/%C3%8Dndice_de_fuerza_relativa\n",
    "- **EMA (Mitjana Mòbil Exponencial)**: Calcula l'EMA, que és una versió ponderada de la mitjana mòbil que dóna més pes als preus recents. Podeu veure una explicació més de tallada a https://es.tradingview.com/support/solutions/43000592270/\n",
    "\n",
    "Les funcions prenen alguns dels següents arguments:\n",
    "\n",
    "- `data`: Les dades històriques de preus de les accions, generalment en format de sèries temporals. En aquest exemple, utilitzarem els preus de tancament diaris.\n",
    "- `window` (opcional): El nombre de períodes a utilitzar per als càlculs dels indicadors. Per defecte, s'assumeix un valor de 14 per al RSI i l'EMA.\n",
    "\n",
    "A continuació, es mostren les funcions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KxrX5J9U1JS4"
   },
   "outputs": [],
   "source": [
    "def calculate_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi.fillna(50).squeeze()\n",
    "\n",
    "def calculate_ema(data, window=14):\n",
    "    return data.ewm(span=window, adjust=False).mean().squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfJKeOH61JS5"
   },
   "source": [
    "Amb aquestes funcions, els agents podran utilitzar informació tècnica clau sobre les accions al mercat per prendre decisions de trading més informades.\n",
    "\n",
    "Ara bé, necessitem el valor de les accions. Per a aquest projecte, utilitzem la llibreria yfinance, que permet obtenir dades històriques d'actius financers de manera senzilla. El següent fragment de codi descarrega les dades de l'ETF SPY (un fons que segueix l'índex S&P 500) des de l'1 de gener de 2021 fins a l'1 de gener de 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zviO1kyU1JS5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price        Adj Close       Close        High         Low        Open  \\\n",
      "Ticker             SPY         SPY         SPY         SPY         SPY   \n",
      "Date                                                                     \n",
      "2021-01-04  348.299408  368.790009  375.450012  364.820007  375.309998   \n",
      "2021-01-05  350.698303  371.329987  372.500000  368.049988  368.100006   \n",
      "2021-01-06  352.794891  373.549988  376.980011  369.119995  369.709991   \n",
      "2021-01-07  358.036499  379.100006  379.899994  375.910004  376.100006   \n",
      "2021-01-08  360.076538  381.260010  381.489990  377.100006  380.589996   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2021-12-27  456.751007  477.260010  477.309998  472.010010  472.059998   \n",
      "2021-12-28  456.377686  476.869995  478.809998  476.059998  477.720001   \n",
      "2021-12-29  456.961517  477.480011  478.559998  475.920013  476.980011   \n",
      "2021-12-30  455.698242  476.160004  479.000000  475.670013  477.929993   \n",
      "2021-12-31  454.549713  474.959991  476.859985  474.670013  475.640015   \n",
      "\n",
      "Price          Volume  \n",
      "Ticker            SPY  \n",
      "Date                   \n",
      "2021-01-04  110210800  \n",
      "2021-01-05   66426200  \n",
      "2021-01-06  107997700  \n",
      "2021-01-07   68766800  \n",
      "2021-01-08   71677200  \n",
      "...               ...  \n",
      "2021-12-27   56808600  \n",
      "2021-12-28   47274600  \n",
      "2021-12-29   54503000  \n",
      "2021-12-30   55329000  \n",
      "2021-12-31   65237400  \n",
      "\n",
      "[252 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = yf.download(\"SPY\", start=\"2021-01-01\", end=\"2022-01-01\")\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agW2zE6Xp9v4"
   },
   "source": [
    "El resultat és un **DataFrame** que conté la següent informació per a cada dia del rang de dates:\n",
    "\n",
    "- **Open**: Preu d'obertura de l'actiu.\n",
    "- **High**: Preu màxim de l'actiu durant el dia.\n",
    "- **Low**: Preu mínim de l'actiu durant el dia.\n",
    "- **Close**: Preu de tancament de l'actiu.\n",
    "- **Adj Close**: Preu ajustat que té en compte dividends i splits.\n",
    "- **Volume**: Nombre d'accions negociades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxDFUMS2p9v5"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 1.1 (0.25 pts):</strong>\n",
    "    Utilitza les dades històriques del mercat financer, descarregades mitjançant la funció <code>yfinance.download</code>, i aplica els indicadors tècnics proporcionats: <code>calculate_rsi</code> i <code>calculate_ema</code>.\n",
    "    A continuació, realitza les següents tasques:\n",
    "    <ul>\n",
    "        <li>Descarrega les dades històriques de SPY per a l'any 2021.</li>\n",
    "        <li>Calcula l'RSI per als preus de tancament durant el període.</li>\n",
    "        <li>Calcula la mitjana mòbil exponencial (EMA) per al mateix període amb els preus de tancament.</li>\n",
    "        <li>Imprimeix l'últim valor dels càlculs de cada indicador (RSI i EMA) per verificar que s'han generat correctament sense errors. Aquests valors haurien de ser RSI: 53.765164 i EMA: 470.690088 (el nombre de decimals pot variar).</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gjYAIycip9v5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last RSI value: 53.76516415158352\n",
      "Last EMA value: 470.6900882953319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download historical data for SPY (2021)\n",
    "data = yf.download(\"SPY\", start=\"2021-01-01\", end=\"2022-01-01\")\n",
    "\n",
    "# Extract closing prices\n",
    "close_prices = data[\"Close\"]\n",
    "\n",
    "# RSI and EMA\n",
    "rsi = calculate_rsi(close_prices)\n",
    "ema = calculate_ema(close_prices)\n",
    "\n",
    "# Print the last value of RSI and EMA for verification\n",
    "print(\"Last RSI value:\", rsi.iloc[-1])\n",
    "print(\"Last EMA value:\", ema.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLrYAbIBzJXi"
   },
   "source": [
    "#### 1.1.2 Implementació de StockMarketEnv\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 1.2 (2.25 pts):</strong> Defineix l'entorn <code>StockMarketEnv</code> seguint les indicacions aportades anteriorment.\n",
    "    A més, a banda de les típiques funcions de qualsevol entorn (<code>reset</code>, <code>step</code> i <code>render</code>), s'han d'implementar dos funcions més (<code>save_to_csv_file</code> i <code>_normalize</code>) que s'expliquen a continuació:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3IO8btTNMg2"
   },
   "source": [
    "##### Crear la funció <code>save_to_csv_file</code> en Python\n",
    "Implementa una funció <code>save_to_csv_file</code> que guardi les dades actuals d'una classe en un fitxer CSV. La funció calcularà el benefici (<code>profit</code>) com la diferència entre el valor net actual (<code>net_worth</code> -> balanç efectiu + valor de les accions en possessió) i el balanç inicial (<code>initial_balance</code>), i escriurà una nova fila amb els valors de <code>current_step</code>, <code>balance</code>, <code>shares_held</code>, <code>net_worth</code> i <code>profit</code> en el fitxer CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoGzAsUuHQJo"
   },
   "source": [
    "##### Crear la funció de normalització en Python\n",
    "\n",
    "La funció `_normalize` s'utilitza per ajustar un valor a un rang estàndard, comunament entre 0 i 1, en relació amb un valor mínim i màxim especificats. Aquest procés, conegut com a **normalització**, és útil per transformar dades, mantenint les seves proporcions relatives, dins d'un interval més manejable. La funció pren tres paràmetres:\n",
    "\n",
    "- `value`: el valor a normalitzar.\n",
    "- `min_val`: el límit inferior del rang de normalització.\n",
    "- `max_val`: el límit superior del rang de normalització.\n",
    "\n",
    "La normalització es realitza mitjançant la següent fórmula:\n",
    "\n",
    "$$\n",
    "\\text{normalized_value} = \\frac{\\text{value} - \\text{min_val}}{\\text{max_val} - \\text{min_val}}\n",
    "$$\n",
    "\n",
    "Aquest càlcul ajusta `value` al rang definit entre `min_val` i `max_val`. A més, si `max_val` i `min_val` són iguals, la funció retorna `0` per evitar una **divisió per zero**.\n",
    "\n",
    "##### Beneficis en el Context de l'Aprenentatge per Reforç (RL)\n",
    "\n",
    "En un entorn d'**aprenentatge per reforç (RL)**, la normalització és fonamental per diverses raons:\n",
    "\n",
    "1. **Estabilitat d'Entrenament**: Al normalitzar les recompenses, les observacions o les accions a un rang estàndard, s'evita que valors grans desestabilitzin el procés d'aprenentatge. Els models de RL, com les xarxes neuronals, tendeixen a aprendre millor amb dades en intervals limitats.\n",
    "\n",
    "2. **Facilita la Comparació**: La normalització permet comparar dades provinents de diferents fonts o escales, com recompenses de diferents entorns, cosa que millora la generalització del model.\n",
    "\n",
    "3. **Accelera la Convergència**: Dades escalades de manera uniforme ajuden que els algoritmes de RL convergeixin més ràpidament, ja que es redueix la variabilitat de les entrades.\n",
    "\n",
    "4. **Prevé Errors de Càlcul**: En gestionar entrades normalitzades i amb límits definits, s'eviten errors de càlcul o inestabilitats degudes a diferències numèriques extremes.\n",
    "----\n",
    "<b>Nota</b>: se us proporciona el codi preimplementat. La implementació que es demana en l'enunciat està indicada en els blocs <i>#TODO</i> i/o amb variables igualades a <i>None</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fe9EFgu9zJXi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "start = \"2019-01-01\"\n",
    "end = \"2021-01-01\"\n",
    "ticker = \"SPY\"\n",
    "initial_balance = 10000\n",
    "\n",
    "class StockMarketEnv(gym.Env):\n",
    "    def __init__(self, ticker=ticker, initial_balance=initial_balance, is_eval=False,\n",
    "                 start = start, end = end, save_to_csv=False,\n",
    "                 csv_filename=\"stock_trading_log.csv\"):\n",
    "        super(StockMarketEnv, self).__init__()\n",
    "\n",
    "        # Descarregar les dades històriques de l'acció\n",
    "        self.df = yf.download(ticker, start=start, end=end)\n",
    "        self.num_trading_days = len(self.df)\n",
    "        self.prices = self.df['Close'].values\n",
    "        self.n_steps = len(self.prices) - 1\n",
    "\n",
    "        # Paràmetres de l'entorn\n",
    "        self.initial_balance = initial_balance\n",
    "        self.current_step = 0\n",
    "        self.balance = float(initial_balance)\n",
    "        self.shares_held = 0\n",
    "        self.net_worth = initial_balance\n",
    "        self.previous_net_worth = initial_balance\n",
    "\n",
    "        # Espai d'accions: 0 -> mantenir, 1 -> comprar, 2 -> vendre\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "        # Calculem els indicadors tècnics\n",
    "        self.rsi = calculate_rsi(self.df['Close']).values\n",
    "        self.ema = calculate_ema(self.df['Close']).values\n",
    "\n",
    "        # Espai d'observacions: [preu_actual, balanç, accions, rsi, ema]\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(5,), dtype=np.float32\n",
    "        )\n",
    "        self.is_eval = is_eval\n",
    "\n",
    "        # Valores para normalización (obtenemos mínimos y máximos)\n",
    "        self.min_price = self.prices.min()\n",
    "        self.max_price = self.prices.max()\n",
    "        self.min_rsi = self.rsi.min()\n",
    "        self.max_rsi = self.rsi.max()\n",
    "        self.min_ema = self.ema.min()\n",
    "        self.max_ema = self.ema.max()\n",
    "\n",
    "        # Paràmetres addicionals per al fitxer CSV\n",
    "        self.save_to_csv = save_to_csv\n",
    "        self.csv_filename = csv_filename\n",
    "\n",
    "        # Si l'opció de desar en CSV està activada, crea o sobreescriu el fitxer\n",
    "        if self.save_to_csv:\n",
    "            with open(self.csv_filename, 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Step', 'Balance', 'Shares Held', 'Net Worth', 'Profit'])\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.previous_net_worth = self.initial_balance\n",
    "        return self._next_observation(), {}\n",
    "\n",
    "    def _normalize(self, value, min_val, max_val):\n",
    "        \"\"\"Normalize a value between 0 and 1.\"\"\"\n",
    "        if max_val == min_val:\n",
    "            return 0.0\n",
    "        normalized_value = (value - min_val) / (max_val - min_val)\n",
    "        return float(normalized_value)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        if self.current_step >= self.n_steps:\n",
    "            return np.array([\n",
    "                self._normalize(self.prices[-1], self.min_price, self.max_price),\n",
    "                self._normalize(self.balance, self.initial_balance * 0.85, self.initial_balance * 1.25),\n",
    "                self._normalize(self.shares_held, 0, self.initial_balance / self.prices[-1]),\n",
    "                self._normalize(self.rsi[-1], self.min_rsi, self.max_rsi),\n",
    "                self._normalize(self.ema[-1], self.min_ema, self.max_ema)\n",
    "            ])\n",
    "        \n",
    "        # Normalitzem els valors\n",
    "        norm_price = self._normalize(self.prices[self.current_step], self.min_price, self.max_price)\n",
    "        norm_balance = self._normalize(self.balance, self.initial_balance * 0.85, self.initial_balance * 1.25)\n",
    "        max_shares = self.initial_balance / self.prices[self.current_step]\n",
    "        norm_shares_held = self._normalize(self.shares_held, 0, max_shares)\n",
    "        norm_rsi = self._normalize(self.rsi[self.current_step], self.min_rsi, self.max_rsi)\n",
    "        norm_ema = self._normalize(self.ema[self.current_step], self.min_ema, self.max_ema)\n",
    "\n",
    "        return np.array([norm_price, norm_balance, norm_shares_held, norm_rsi, norm_ema])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one time step within the environment.\"\"\"\n",
    "        if self.current_step >= self.n_steps:\n",
    "            terminated = True\n",
    "            truncated = False\n",
    "            return self._next_observation(), 0, terminated, truncated, {}\n",
    "\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "\n",
    "        # Acció: 0 -> mantenir, 1 -> comprar, 2 -> vendre\n",
    "        if action == 1:  # Buy\n",
    "            shares_bought = int(self.balance // current_price)\n",
    "            self.balance = float(self.balance - shares_bought * current_price)\n",
    "            self.shares_held = int(self.shares_held + shares_bought)\n",
    "        elif action == 2:  # Sell\n",
    "            self.balance = float(self.balance + self.shares_held * current_price)\n",
    "            self.shares_held = 0\n",
    "\n",
    "        # Actualitzar el preu anterior\n",
    "        self.previous_net_worth = self.net_worth\n",
    "        self.net_worth = float(self.balance + (self.shares_held * current_price))\n",
    "\n",
    "        # Calcular la recompensa\n",
    "        reward = self._calculate_reward()\n",
    "\n",
    "        # Avançar al següent pas\n",
    "        self.current_step += 1\n",
    "        terminated = self.net_worth < (0.85 * self.initial_balance)\n",
    "        truncated = self.current_step >= self.n_steps\n",
    "\n",
    "        if self.save_to_csv:\n",
    "            self.save_to_csv_file()\n",
    "\n",
    "        # Retorna l'observació, la recompensa, si està complet, i altra informació addicional\n",
    "        return self._next_observation(), reward, terminated, truncated, {}\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        reward = 0\n",
    "        # primera etapa no computada\n",
    "        if self.current_step == 0:\n",
    "            return 0\n",
    "        \n",
    "        current_price = self.prices[self.current_step]\n",
    "        previous_price = self.prices[self.current_step - 1]\n",
    "\n",
    "        # net worth increased\n",
    "        if self.net_worth > self.previous_net_worth:\n",
    "            return 1\n",
    "        # net worth decreased\n",
    "        if self.net_worth < self.previous_net_worth:\n",
    "            return -1\n",
    "        if self.net_worth == self.previous_net_worth:\n",
    "            # shares worth less than before\n",
    "            if self.shares_held == 0 and current_price < previous_price:\n",
    "                return 1\n",
    "            # shares worth more than before\n",
    "            if self.shares_held == 0 and current_price > previous_price:\n",
    "                return -1\n",
    "        return reward\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment state.\"\"\"\n",
    "        profit = float(self.net_worth - self.initial_balance)\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Balance: {self.balance}\")\n",
    "        print(f\"Shares held: {self.shares_held}\")\n",
    "        print(f\"Net worth: {self.net_worth}\")\n",
    "        print(f\"Profit: {profit}\")\n",
    "\n",
    "    def save_to_csv_file(self):\n",
    "        \"\"\"Desa les dades actuals al fitxer CSV.\"\"\"\n",
    "        profit = self.net_worth - self.initial_balance\n",
    "        with open(self.csv_filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([self.current_step, self.balance, int(self.shares_held), self.net_worth, profit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlH3wQoIPqYv"
   },
   "source": [
    "La següent cel·la és de **comprovació** i ha de generar la sortida que es mostra a continuació. Aquesta sortida serveix per verificar que tot s'ha implementat correctament. En executar la cel·la, la sortida ha de coincidir exactament amb el resultat següent:\n",
    "\n",
    "\n",
    "```\n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.14086006, 0.375     , 0.        , 0.45140651, 0.        ]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.19505732, -2.06710007,  0.4       ,  0.45140651,  0.00333123]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.20824227, -2.06710007,  0.4       ,  0.45140651,  0.00842359]), 1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.22407732, 0.47669998, 0.        , 0.45140651, 0.01548553]), 1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 4  \n",
    "Balance: 10406.799926757812  \n",
    "Shares held: 0  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "(array([0.23202811, 0.47669998, 0.        , 0.45140651, 0.02293572]), -1, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 5  \n",
    "Balance: 10406.799926757812  \n",
    "Shares held: 0  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "(array([ 0.23805742, -2.10300003,  0.4       ,  0.45140651,  0.03040101]), 0, False, False, {})\n",
    "------------------------------------------------------------------------------------\n",
    "Step: 6  \n",
    "Balance: 87.9998779296875  \n",
    "Shares held: 40  \n",
    "Net worth: 10406.799926757812  \n",
    "Profit: 406.7999267578125  \n",
    "None  \n",
    "------------------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_c2GrpVHNMg3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "(array([0.14086006, 0.375     , 0.        , 0.45140651, 0.        ]), 0, False, False, {})\n",
      "------------------------------------------------------------------------------------\n",
      "(array([ 0.19505732, -2.06710007,  1.00956   ,  0.45140651,  0.00333123]), 0, False, False, {})\n",
      "------------------------------------------------------------------------------------\n",
      "(array([ 0.20824227, -2.06710007,  1.01752002,  0.45140651,  0.00842359]), 1, False, False, {})\n",
      "------------------------------------------------------------------------------------\n",
      "(array([0.22407732, 0.47669998, 0.        , 0.45140651, 0.01548553]), 1, False, False, {})\n",
      "------------------------------------------------------------------------------------\n",
      "Step: 4\n",
      "Balance: 10406.799926757812\n",
      "Shares held: 0\n",
      "Net worth: 10406.799926757812\n",
      "Profit: 406.7999267578125\n",
      "None\n",
      "------------------------------------------------------------------------------------\n",
      "(array([0.23202811, 0.47669998, 0.        , 0.45140651, 0.02293572]), -1, False, False, {})\n",
      "------------------------------------------------------------------------------------\n",
      "Step: 5\n",
      "Balance: 10406.799926757812\n",
      "Shares held: 0\n",
      "Net worth: 10406.799926757812\n",
      "Profit: 406.7999267578125\n",
      "None\n",
      "------------------------------------------------------------------------------------\n",
      "(array([ 0.23805742, -2.10300003,  1.03552002,  0.45140651,  0.03040101]), 0, False, False, {})\n",
      "------------------------------------------------------------------------------------\n",
      "Step: 6\n",
      "Balance: 87.9998779296875\n",
      "Shares held: 40\n",
      "Net worth: 10406.799926757812\n",
      "Profit: 406.7999267578125\n",
      "None\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = StockMarketEnv()\n",
    "env.reset()\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(0))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(2))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(0))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.step(1))\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print(env.render())\n",
    "print('------------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmlDKL_WzJXi"
   },
   "source": [
    "#### 1.1.3 Interacció amb l'Entorn StockMarketEnv\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 1.3 (0.5 pts):</strong> Carregar l'entorn <code>StockMarketEnv</code> i realitzar les següents tasques:\n",
    "    <ul>\n",
    "        <li>Mostrar l'espai d'accions i l'espai d'observacions.</li>\n",
    "        <li>Executar 100 episodis amb accions aleatòries, mostrant la recompensa mitjana obtinguda entre totes les partides.</li>\n",
    "        <li>Executar una partida aleatòria i mostrar el render en finalitzar-la.</li>\n",
    "        <li>Provar la funció <code>save_to_csv_file</code> i mostrar el resultat de les darreres 5 files.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Pt0BhGGJzJXi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "env = StockMarketEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tldZjjBlzJXi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(3)\n",
      "Observation Space: Box(0.0, 1.0, (5,), float32)\n"
     ]
    }
   ],
   "source": [
    "# Display the action space and observation space\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"Observation Space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UxM9K02yNMg3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean reward over 100 episodes: 8.81\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "episodes = 100\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Select a random action\n",
    "        _, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "mean_reward = total_reward / episodes\n",
    "print(f'\\nMean reward over {episodes} episodes: {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EK54MX40NMg3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 308\n",
      "Balance: 93.75483703613281\n",
      "Shares held: 37\n",
      "Net worth: 8342.904724121094\n",
      "Profit: -1657.0952758789062\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    #env.render() #Uncomment to see all steps\n",
    "    #time.sleep(0.1)\n",
    "    action = env.action_space.sample()  #Random action\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HqRtVBGI1RVs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Step       Balance  Shares Held     Net Worth       Profit\n",
      "751   752    359.930603           27  13066.130768  3066.130768\n",
      "752   753    359.930603           27  13245.950867  3245.950867\n",
      "753   754  13235.420471            0  13235.420471  3235.420471\n",
      "754   755    343.460175           27  13235.420471  3235.420471\n",
      "755   756    343.460175           27  13199.780273  3199.780273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def provar_save_to_csv(env, rows):\n",
    "    \"\"\"\n",
    "    Provar la funció save_to_csv_file executant diverses accions\n",
    "    a l'entorn i després mostrant les últimes 5 files del fitxer CSV.\n",
    "\n",
    "    Paràmetres:\n",
    "        env: L'entorn StockMarketEnv.\n",
    "        rows: El número de files que es volen mostrar del fitxer CSV.\n",
    "    \"\"\"\n",
    "    # Executar passos a l'entorn amb accions aleatòries\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # Llegir i mostrar les últimes x files del fitxer CSV\n",
    "    if env.save_to_csv:\n",
    "        # Llegir el fitxer CSV en un DataFrame\n",
    "        df = pd.read_csv(env.csv_filename)\n",
    "        print(df.tail(rows))\n",
    "\n",
    "env = StockMarketEnv(ticker=\"SPY\", start=\"2019-01-01\", end=\"2022-01-01\", save_to_csv=True, csv_filename=\"stock_trading_log.csv\")\n",
    "provar_save_to_csv(env, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_ZXX9atzJXj"
   },
   "source": [
    "## 2. Agent DQN inversor en borsa (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CgCRw341JS7"
   },
   "source": [
    "En aquest apartat implementarem un DQN tenint en compte l'exploració-explotació (**epsilon-greedy**), la xarxa objectiu i el **buffer de repetició d'experiències**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90l7Ba721JS7"
   },
   "source": [
    "Definirem el buffer de la manera següent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPdwD1G81JS7"
   },
   "outputs": [],
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer',\n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size,\n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque\n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvY3iwTP1JS7"
   },
   "source": [
    "### 2.1 Implementació de la Classe NeuralNetStockMarket\n",
    "\n",
    "Primerament implementarem la xarxa neuronal, utilitzant un model Seqüencial amb la següent configuració:\n",
    "\n",
    "- Tres capes completament connectades (representades en PyTorch per `nn.Linear`) amb 256, 128 i 64 neurones cadascuna, `bias=True`, i activació ReLU.\n",
    "- Una capa de sortida completament connectada i `bias=True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISy3TJ5s1JS7"
   },
   "source": [
    "Farem servir l'optimitzador Adam per entrenar la xarxa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-FNzJNO1JS8"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 2.1 (0.5 pts):</strong> Implementar la classe <code>NeuralNetStockMarket()</code>. Inicialitzar les variables necessàries i definir el model Seqüencial de xarxa neuronal indicat.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi preimplementat. La implementació que es demana en l'enunciat està indicada en els blocs <i>#TODO</i> i/o amb variables igualades a <i>None</i>.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ql0g4IKE1JS8"
   },
   "outputs": [],
   "source": [
    "class NeuralNetStockMarket(torch.nn.Module):\n",
    "\n",
    "    ###################################\n",
    "    ### Inicialització i model ###\n",
    "    def __init__(self, env, learning_rate=1e-3, optimizer=None, device=None):\n",
    "        \"\"\"\n",
    "        Paràmetres\n",
    "        ==========\n",
    "        n_inputs: mida de l'espai d'estats\n",
    "        n_outputs: mida de l'espai d'accions\n",
    "        actions: array d'accions possibles\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        ##Inicialitzar paràmetres\n",
    "        super(NeuralNetStockMarket, self).__init__()\n",
    "        self.n_inputs = env.observation_space.shape[0]  #assignar la mida de l'espai d'estats\n",
    "        self.n_outputs = env.action_space.n  #assignar la mida de l'espai d'accions\n",
    "        self.actions = list(range(self.n_outputs))  #assignar l'array d'accions possibles\n",
    "        self.learning_rate = learning_rate  #assignar el valor de la taxa d'aprenentatge\n",
    "\n",
    "        # Definir el dispositiu (CPU o GPU)\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        #######################################\n",
    "        ## Construcció de la xarxa neuronal\n",
    "        #construir el model Seqüencial\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 256, bias=True), #primera capa - 256 neurones\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128, bias=True), #segona capa - 128 neurones\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64, bias=True), #tercera capa - 64 neurones\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.n_outputs, bias=True) #capa de sortida\n",
    "        ).to(self.device)\n",
    "\n",
    "        #######################################\n",
    "        ## Inicialitzar l'optimitzador\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "    ### Mètode e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            #random action (exploració)\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            #action based on qvalues\n",
    "            qvals = self.get_qvals(state)\n",
    "            action = torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        return self.model(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEfgOTll1JS8"
   },
   "source": [
    "### 2.2 Implementació de l'Agent DQN amb Exploració/Explotació i Sincronització de Xarxes\n",
    "\n",
    "A continuació, implementarem una classe que defineixi el comportament de l'agent DQN tenint en compte:\n",
    "\n",
    "- **Exploració/Explotació**: Decaïment de **epsilon** per equilibrar l'exploració i l'explotació.\n",
    "- **Actualització i Sincronització de Xarxes**: Sincronització periòdica de la xarxa principal i la xarxa objectiu, i actualització basada en la pèrdua.\n",
    "\n",
    "#### Criteri d'aprenentatge:\n",
    "Es considera que l'agent ha après a realitzar la tasca (és a dir, el \"joc\" acaba) quan obté una mitjana mínima de **8700 $** durant **100 episodis consecutius**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQVGVM6T1JS8"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.2 (0.5 pts):</strong> Implementar els següents punts de la classe <code>DQNAgent()</code>:\n",
    "<ol>\n",
    "    <li>Declarar les variables de la classe</li>\n",
    "    <li>Inicialitzar les variables necessàries</li>\n",
    "    <li>Implementar l'acció a prendre</li>\n",
    "    <li>Actualitzar la xarxa principal segons la freqüència establerta als hiperparàmetres</li>\n",
    "    <li>Calcular la pèrdua (equació de Bellman, etc.)</li>\n",
    "    <li>Sincronitzar la xarxa objectiu segons la freqüència establerta als hiperparàmetres</li>\n",
    "    <li>Calcular la mitjana de recompenses dels últims 100 episodis</li>\n",
    "    <li>Comprovar el límit d'episodis</li>\n",
    "    <li>Actualitzar epsilon segons:\n",
    "    $$ \\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, 0.01) $$ </li>\n",
    "</ol>\n",
    "A més, durant el procés s'han d'emmagatzemar (*):\n",
    "<ol>\n",
    "    <li>Les recompenses obtingudes en cada pas de l'entrenament</li>\n",
    "    <li>Les recompenses mitjanes cada 100 episodis</li>\n",
    "    <li>La pèrdua durant l'entrenament</li>\n",
    "    <li>L'evolució de epsilon al llarg de l'entrenament</li>\n",
    "    <li>Emmagatzemar la quantitat d'episodis necessaris per dur a terme l'entrenament a la variable <code>episodes_train_dqn</code></li>\n",
    "</ol>\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi preimplementat. La implementació que es demana a l'enunciat està indicada als blocs <i>#TODO</i> i/o amb variables igualades a <i>None</i>, excepte (*) que indica en quin moment emmagatzemar les variables esmentades.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQcTPvAE1JS8"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, main_network, buffer, epsilon=0.1, eps_decay=0.99, batch_size=32, min_episodes=300, device=None):\n",
    "        ######################################\n",
    "        ## TODO 1: Declarar variables\n",
    "        self.env = env  #assignar l'entorn\n",
    "        self.main_network = main_network  #assignar la xarxa principal\n",
    "        self.target_network = copy.deepcopy(main_network)  #xarxa objectiu (còpia de la principal)\n",
    "        self.buffer = buffer  #assignar el buffer de repetició d'experiències\n",
    "        self.epsilon = epsilon  #assignar el valor inicial de epsilon\n",
    "        self.eps_decay = eps_decay  #assignar la velocitat de decaïment de epsilon\n",
    "        self.batch_size = batch_size  #assignar la mida del batch\n",
    "        self.nblock = 100  # bloc dels X últims episodis per calcular la mitjana de recompenses\n",
    "        self.initialize()\n",
    "        self.min_episodes = min_episodes  #assignar el nombre mínim d'episodis\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Configurar el dispositiu (CPU o GPU)\n",
    "\n",
    "    def initialize(self):\n",
    "        ######################################\n",
    "        ## TODO 3: Inicialitzar el necessari\n",
    "        self.state0 = None  # Estat inicial\n",
    "        self.update_loss = []\n",
    "        self.total_reward = 0  # Recompensa total d'un episodi\n",
    "        self.step_count = 0  # Comptador de passos\n",
    "        self.rewards_history = []  # Llista de recompenses per episodi\n",
    "        self.episode_losses = []  # Llista de pèrdues\n",
    "        self.episode_epsilon = []  # Evolució d'epsilon\n",
    "        self.mean_rewards_history = []  # Mitjanes de recompenses\n",
    "\n",
    "    ## Prendre una nova acció\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            # acció aleatòria durant el burn-in o en la fase d'exploració (epsilon)\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            # acció basada en el valor de Q (elecció de l'acció amb millor Q)\n",
    "            if np.random.random() < eps:\n",
    "                action = self.env.action_space.sample()\n",
    "            else: \n",
    "                qvals = self.main_network.get_qvals(self.state0)  #qvalues for current state\n",
    "                action = torch.argmax(qvals).item()  #better qvalues action select\n",
    "            self.step_count += 1\n",
    "        # TODO: prendre un 'step', obtenir un nou estat i recompensa. Desar l'experiència al buffer\n",
    "\n",
    "        next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        self.buffer.append(self.state0, action, reward, done, next_state)\n",
    "        \n",
    "        # TODO: reiniciar l'entorn si s'ha completat l'episodi ('if done')\n",
    "        if done or truncated:\n",
    "            self.state0 = self.env.reset()#[0]\n",
    "            return True\n",
    "        else:\n",
    "            self.state0 = next_state\n",
    "            return False\n",
    "\n",
    "    ## Entrenament\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, REWARD_THRESHOLD=9000):\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Omplir el buffer amb N experiències aleatòries (burn-in)\n",
    "        print(\"Omplint el buffer de repetició d'experiències...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Entrenament...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while not gamedone:\n",
    "                # L'agent pren una acció\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "                ##################################################################################\n",
    "                ##### TODO 4: Actualitzar la xarxa principal segons la freqüència establerta #######\n",
    "                if self.step_count % dnn_update_frequency == 0 and len(self.buffer.replay_memory) > batch_size:\n",
    "                    self.update()\n",
    "                ########################################################################################\n",
    "                ### TODO 6: Sincronitzar la xarxa principal i la xarxa objectiu segons la freqüència establerta #####\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    #######################################################################################\n",
    "                    ### TODO 7: calcular la mitjana de recompenses dels últims X episodis i emmagatzemar #####\n",
    "                    self.rewards_history.append(self.total_reward)\n",
    "                    mean_rewards = np.mean(self.rewards_history[-self.nblock:])\n",
    "                    self.mean_rewards_history.append(mean_rewards)\n",
    "                    self.episode_epsilon.append(self.epsilon)\n",
    "                    self.update_loss = []\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ### TODO 8: Comprovar que encara queden episodis. Parar l'aprenentatge si s'arriba al límit\n",
    "\n",
    "                    print(\"\\rEpisodi {:d} Recompenses Mitjanes {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "\n",
    "                    # Comprovar si s'ha arribat al límit d'episodis\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nLímit d\\'episodis assolit.')\n",
    "                        print('\\nEntorn resolt en {} episodis!'.format(episode))\n",
    "                        break\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ### TODO 9: Afegir el mínim d'episodis requerits\n",
    "                    if mean_rewards >= REWARD_THRESHOLD and self.min_episodes < episode:\n",
    "                        training = False\n",
    "                        print('\\nLímit d\\'episodis assolit.')\n",
    "                        print('\\nEntorn resolt en {} episodis!'.format(episode))\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ###### TODO 9: Actualitzar epsilon segons la velocitat de decaïment fixada ########\n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "                    self.epsilon_history.append(self.epsilon)\n",
    "\n",
    "    ## Càlcul de la pèrdua\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separem les variables de l'experiència i les convertim a tensors\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(self.device)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1, 1).to(self.device)\n",
    "        dones_t = torch.tensor(dones, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        # Obtenim els valors de Q de la xarxa principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals).to(self.device)\n",
    "        # Obtenim els valors de Q objectiu. El paràmetre detach() evita que aquests valors actualitzin la xarxa objectiu\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                               dim=-1)[0].detach().to(self.device)\n",
    "        qvals_next[dones_t.bool()] = 0\n",
    "\n",
    "        #################################################################################\n",
    "        ### Calcular equació de Bellman\n",
    "        expected_qvals = rewards_vals + self.gamma * qvals_next\n",
    "\n",
    "        # Assegurem que les dimensions de qvals i expected_qvals siguin les mateixes\n",
    "        expected_qvals = expected_qvals.unsqueeze(1)\n",
    "\n",
    "        #################################################################################\n",
    "        ### Calcular la pèrdua (MSE)\n",
    "        loss = torch.nn.functional.mse_loss(qvals, expected_qvals)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminem qualsevol gradient passat\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size)  # seleccionem un conjunt del buffer\n",
    "        loss = self.calculate_loss(batch)  # calculem la pèrdua\n",
    "        loss.backward()  # fem la diferència per obtenir els gradients\n",
    "        self.main_network.optimizer.step()  # apliquem els gradients a la xarxa neuronal\n",
    "        # Guardem els valors de pèrdua\n",
    "        self.update_loss.append(loss.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70dAg7v71JS9"
   },
   "source": [
    "### 2.3 Entrenament del Model\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.3 (0.5 pts):</strong> A continuació, entrenarem el model amb els següents hiperparàmetres:\n",
    "<ul>\n",
    "    <li>Velocitat d'aprenentatge: 0.0005</li>\n",
    "    <li>Mida del batch: 128</li>\n",
    "    <li>Nombre d'episodis: 4000</li>\n",
    "    <li>Nombre d'episodis per omplir el buffer (**BURN_IN**): 1000</li>\n",
    "    <li>Freqüència d'actualització de la xarxa neuronal: 6</li>\n",
    "    <li>Freqüència de sincronització amb la xarxa objectiu: 15</li>\n",
    "    <li>Capacitat màxima del buffer (**MEMORY_SIZE**): 50000</li>\n",
    "    <li>Factor de descompte: 0.99</li>\n",
    "    <li>Epsilon: 1, amb decaïment de 0.995</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBFZNwKA1JS9"
   },
   "outputs": [],
   "source": [
    "# TODO: Definició de variables.\n",
    "# Define variables\n",
    "learning_rate = 0.0005\n",
    "batch_size = 128\n",
    "max_episodes = 4000\n",
    "BURN_IN = 1000\n",
    "dnn_update_frequency = 6\n",
    "dnn_sync_frequency = 15\n",
    "MEMORY_SIZE = 50000\n",
    "gamma = 0.99\n",
    "epsilon = 1.0  # inicialitzar epsilon\n",
    "eps_decay = 0.995\n",
    "ticker = 'SPY'\n",
    "start = '2023-01-01' \n",
    "end = '2024-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kU6ANy1NMg8"
   },
   "outputs": [],
   "source": [
    "# Convertir les dates a `datetime`\n",
    "from datetime import datetime\n",
    "start_date = datetime.strptime(start, '%Y-%m-%d')\n",
    "end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "\n",
    "# Calcular el nombre de dies de trading entre les dates\n",
    "num_days = (end_date - start_date).days\n",
    "\n",
    "print(f\"Nombre de dies de trading per {ticker} des de {start} fins a {end}: {num_days}\")\n",
    "print(f\"El nostre objectiu és guanyar el 50% dels dies: {round(num_days / 2)}\")\n",
    "\n",
    "# Calcular el REWARD_THRESHOLD\n",
    "REWARD_THRESHOLD = round(num_days / 2)\n",
    "print(f\"REWARD_THRESHOLD calculat: {REWARD_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBl792X11JS9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Entrenament\n",
    "# Temps d'entrenament a Google Colab amb GPU: 42.53 minuts\n",
    "# De mitjana obté entre 170-190 de puntuació i arriba als 4000 episodis.\n",
    "\n",
    "# Importar les llibreries necessàries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear l'entorn i l'agent\n",
    "env = StockMarketEnv(ticker, initial_balance=10000.0, is_eval=False, start=start_date, end=end_date, save_to_csv=False, csv_filename='')  # assegura't que l'entorn estigui correctament definit\n",
    "network = NeuralNetStockMarket(env, learning_rate=learning_rate)\n",
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)\n",
    "agent = DQNAgent(env, network, buffer, epsilon=epsilon, eps_decay=eps_decay, batch_size=batch_size, min_episodes=300)\n",
    "\n",
    "# Iniciar el procés d'entrenament\n",
    "agent.train(gamma=gamma, max_episodes=max_episodes, batch_size=batch_size, dnn_update_frequency=dnn_update_frequency, dnn_sync_frequency=dnn_sync_frequency, REWARD_THRESHOLD=REWARD_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff6qg_fV1JS9"
   },
   "source": [
    "### 2.4 Anàlisi de l'Entrenament\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.4 (0.25 pts):</strong> Representar:\n",
    "<ol>\n",
    "    <li>Gràfic amb les recompenses obtingudes al llarg de l'entrenament, l'evolució de les recompenses mitjanes cada 100 episodis, i el llindar de recompensa establert per l'entorn.</li>\n",
    "    <li>Gràfic amb l'evolució de la pèrdua al llarg de l'entrenament.</li>\n",
    "    <li>Gràfic amb l'evolució de epsilon al llarg de l'entrenament.</li>\n",
    "</ol>\n",
    "\n",
    "Comentar els resultats obtinguts.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6tyd9Z11JS-"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(tr_rewards, mean_tr_rewards, th):\n",
    "    #TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwBsmpUz1JS-"
   },
   "outputs": [],
   "source": [
    "def plot_loss(tr_loss):\n",
    "    #TODO\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1os-TdE1JS-"
   },
   "outputs": [],
   "source": [
    "def plot_epsilon(eps_evolution):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ccu9YyPX1JS-"
   },
   "outputs": [],
   "source": [
    "#TODO mostrar gráficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRc9q-LB1JS-"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "#TODO: Afegeix aquí els teus comentaris sobre l'entrenament, l'evolució de recompenses, pèrdues i epsilon.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_QDcyMtHHpA"
   },
   "source": [
    "Un cop entrenat l'agent, ens interessa comprovar com de bé ha après i si és capaç de superar l'entorn. Per fer-ho, recuperem el model entrenat i deixem que l'agent prengui accions aleatòries segons aquest model per observar el seu comportament.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcbEPSqp1JS-"
   },
   "source": [
    "### 2.5 Test de l'agent\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 2.5 (0.25 pts):</strong> Carregar el model entrenat i executar l'agent entrenat durant 505 episodis consecutius en diferents períodes aleatoris des de l'any 2015 fins al 2024. Calcula la suma de recompenses per cada execució. Per aconseguir aquest punt, executa:\n",
    "<ul>\n",
    "    <li>Un gràfic amb la suma de les recompenses respecte dels episodis, incloent-hi el llindar de recompensa establert.</li>\n",
    "    <li>Emmagatzema la recompensa mitjana obtinguda en les 100 partides a la variable <code>mean_reward_dqn_test</code> i l'última recompensa obtinguda en l'entrenament a <code>mean_reward_dqn_last</code>. També calcula en quants escenaris s'ha obtingut més de 252 dies positius en el trading.</li>\n",
    "</ul>\n",
    "\n",
    "A més, realitza l'anàlisi següent amb el model per a l'entorn utilitzat en l'entrenament:\n",
    "<ul>\n",
    "    <li>Reproduir una partida completa de l'agent entrenat i mostrar el resultat final, incloent-hi el valor total del portafoli al final de l'episodi.</li>\n",
    "    <li>Generar un fitxer CSV que registri els resultats de les interaccions de l'agent amb el mercat en cada episodi i mostra per pantalla les últimes 30 accions.</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Comenta tots els resultats obtinguts en aquest apartat. A quines conclusions podem arribar? Com podríem millorar l'entrenament i quines implicacions tindria?</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijUTlkn91JS_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Generar un fitxer CSV que registri els resultats de les interaccions\n",
    "# de l'agent amb el mercat en cada episodi i mostra per pantalla les últimes 30 accions.\n",
    "file_path = 'stock_trading_agent_dqn.csv'\n",
    "\n",
    "# Reproduir una partida completa de l'agent entrenat i mostrar el resultat final,\n",
    "# incloent-hi el valor total del portafoli al final de l'episodi.\n",
    "env = ''  # TODO: Assignar l'entorn entrenat\n",
    "\n",
    "def read_csv_and_show_last_30(file_path):\n",
    "    try:\n",
    "        # Llegir el fitxer CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Mostrar les últimes 30 accions\n",
    "        print(\"Últimes 30 accions del fitxer CSV:\")\n",
    "        print(df.tail(30))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"El fitxer {file_path} no s'ha trobat.\")\n",
    "    except Exception as e:\n",
    "        print(f\"S'ha produït un error en llegir el fitxer: {e}\")\n",
    "\n",
    "# Exemple d'ús\n",
    "read_csv_and_show_last_30(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df3VnU3i1JS_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generar calendario de trading usando días hábiles\n",
    "def generate_random_trading_dates(start_range, end_range, trading_days_target=505):\n",
    "    \"\"\"\n",
    "    Genera un par de fechas (start, end) que tengan exactamente trading_days_target días hábiles.\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_range)\n",
    "    end_date = pd.to_datetime(end_range)\n",
    "\n",
    "    while True:\n",
    "        # Seleccionar una fecha de inicio aleatoria\n",
    "        random_start = start_date + pd.DateOffset(days=random.randint(0, (end_date - start_date).days - trading_days_target))\n",
    "\n",
    "        # Generar un rango de fechas de trading usando solo los días hábiles\n",
    "        trading_days = pd.bdate_range(random_start, random_start + pd.DateOffset(days=2 * trading_days_target)).tolist()\n",
    "\n",
    "        # Filtrar las fechas para obtener exactamente el número de días objetivo\n",
    "        if len(trading_days) >= trading_days_target:\n",
    "            random_end = trading_days[trading_days_target - 1]  # Último día de trading en el rango deseado\n",
    "            return random_start.strftime(\"%Y-%m-%d\"), random_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def test_model(agent, base_env, start_range, end_range, trading_days_target=505, win_days_target=252):\n",
    "    \"\"\"\n",
    "    Testea el modelo entrenado en 100 episodios con fechas aleatorias de trading.\n",
    "\n",
    "    Parámetros:\n",
    "    - agent: Agente entrenado\n",
    "    - base_env: Entorno base\n",
    "    - start_range, end_range: Rangos de fechas para generar las fechas de trading\n",
    "    - trading_days_target: Días de trading por episodio\n",
    "    - win_days_target: Días positivos requeridos para considerar éxito en un episodio\n",
    "\n",
    "    Retorna:\n",
    "    - all_rewards: Lista con las recompensas totales de cada episodio\n",
    "    - success_rate: Porcentaje de episodios exitosos\n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    success_count = 0\n",
    "\n",
    "    for i_episode in range(100):\n",
    "        # Generar nuevas fechas de inicio y fin aleatorias que cumplan con los días de trading deseados\n",
    "        start_date, end_date = generate_random_trading_dates(start_range, end_range, trading_days_target)\n",
    "\n",
    "        # Actualizar el entorno con las nuevas fechas\n",
    "        env = base_env  # TODO: Inicializar entorno con las fechas generadas\n",
    "\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        win_days = 0\n",
    "\n",
    "        while True:\n",
    "            # El agente toma una acción\n",
    "            action = agent.get_action(state)  # TODO: Obtener la acción del agente\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Actualizar recompensas y días positivos\n",
    "            total_reward += reward\n",
    "            if reward > 0:\n",
    "                win_days += 1\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(total_reward)\n",
    "        if win_days >= win_days_target:\n",
    "            success_count += 1\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    success_rate = success_count / 100\n",
    "    return all_rewards, success_rate\n",
    "\n",
    "\n",
    "def plot_test(rewards, th):\n",
    "    \"\"\"\n",
    "    Grafica los resultados del test.\n",
    "    - rewards: Lista de recompensas totales por episodio\n",
    "    - th: Umbral de recompensa establecido\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label=\"Recompensas Totales\")\n",
    "    plt.axhline(y=th, color='r', linestyle='-', label=\"Umbral de Recompensa\")\n",
    "    plt.xlabel(\"Episodios\")\n",
    "    plt.ylabel(\"Recompensas\")\n",
    "    plt.title(\"Resultados del Test del Modelo\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nf-dQTJSNMg-"
   },
   "outputs": [],
   "source": [
    "# TODO: Calcular la recompensa mitjana de les 100 partides de test\n",
    "mean_reward_dqn = np.mean(all_rewards)  # Calcular la mitjana de recompenses obtingudes en el test\n",
    "\n",
    "# TODO: Assignar la Mean Reward de la última iteració de l'entrenament\n",
    "mean_reward_dqn_last = mean_rewards[-1]  # Última recompensa mitjana obtinguda durant l'entrenament\n",
    "\n",
    "# TODO: Calcular el percentatge d'episodis exitosos\n",
    "success_rate = success_count / 100  # Proporció d'episodis amb més de 252 dies positius\n",
    "\n",
    "# Resultats\n",
    "print(f\"La recompensa mitjana obtinguda per l'agent DQN en les 100 partides de test és: {mean_reward_dqn:.2f} punts.\")\n",
    "print(f\"Percentatge d'episodis que van aconseguir guanyar almenys 252 dies: {success_rate * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zivDev7J1JS_"
   },
   "source": [
    "\n",
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentarios:</strong>\n",
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVL98bV1JS_"
   },
   "source": [
    "## 3. Agent Dueling DQN (1.5 pts)\n",
    "\n",
    "En aquest apartat resoldrem el mateix entorn amb les mateixes característiques per a l'agent, però utilitzant una dueling DQN. Com en el cas anterior, primer definirem el model de xarxa neuronal, després descriurem el comportament de l'agent, l'entrenarem i, finalment, testejarem el funcionament de l'agent entrenat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puwBgnP41JS_"
   },
   "source": [
    "### 3.1 Definició de l'arquitectura de la xarxa neuronal\n",
    "\n",
    "L'objectiu principal de les dueling DQN és \"estalviar-se\" el càlcul del valor de Q en aquells estats on l'acció que es prengui sigui irrellevant. Per això, es descompon la funció Q en dos components:\n",
    "\n",
    "$$Q(s, a) = A(s, a) + V(s)$$\n",
    "\n",
    "Aquesta descomposició es realitza a nivell de l'arquitectura de la xarxa neuronal. Les primeres capes que teníem a la DQN seran comunes, i després la xarxa es dividirà en dues parts separades definides per la resta de capes.\n",
    "\n",
    "La descomposició en subxarxes del model de la DQN implementada a l'apartat anterior serà:\n",
    "\n",
    "<ol>\n",
    "  <li> Bloc comú: </li>\n",
    "  <ul>\n",
    "    <li>Una primera capa completament connectada de 256 neurones i <code>bias = True</code>, amb activació ReLU.</li>\n",
    "    <li>Una segona capa completament connectada de 128 neurones i <code>bias = True</code>, amb activació ReLU.</li>\n",
    "  </ul>\n",
    "  <li>Per a cadascuna de les subxarxes d'avantatge \\(A(s,a)\\) i valor \\(V(s)\\):</li>\n",
    "  <ul>\n",
    "    <li>Una capa completament connectada de 64 neurones i <code>bias = True</code>, amb activació ReLU.</li>\n",
    "    <li>Una última capa completament connectada i <code>bias = True</code>. Aquesta serà la nostra capa de sortida, i per tant, el nombre de neurones de sortida dependrà de si es tracta de la xarxa \\(A(s,a)\\), que tindrà tantes neurones com dimensions tingui l'espai d'accions, o si es tracta de la xarxa \\(V(s)\\), amb un valor per estat.</li>\n",
    "  </ul>\n",
    "</ol>\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGxOz5G21JS_"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.1 (0.5 pts):</strong> Implementar la classe <code>duelingDQN()</code>. Inicialitzar les variables necessàries i definir el model de xarxa neuronal indicat.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi preimplementat. La implementació que es demana a l'enunciat està indicada als blocs <i>#TODO</i> i/o amb variables igualades a <i>None</i>.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltuTJg4w1JTA"
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class duelingDQN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, device=None, learning_rate=1e-3):\n",
    "\n",
    "        \"\"\"\n",
    "        Paràmetres\n",
    "        ==========\n",
    "        n_inputs: mida de l'espai d'estats\n",
    "        n_outputs: mida de l'espai d'accions\n",
    "        actions: array d'accions possibles\n",
    "        \"\"\"\n",
    "\n",
    "        ###################################\n",
    "        #### TODO: Inicialitzar variables ####\n",
    "        super(duelingDQN, self).__init__()\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.n_inputs = #TODO\n",
    "        self.n_outputs = #TODO\n",
    "        self.actions = #TODO\n",
    "\n",
    "        ######\n",
    "\n",
    "        #######################################\n",
    "        ## TODO: Construcció de la xarxa neuronal\n",
    "        # Xarxa comuna\n",
    "        ## Construcció de la xarxa neuronal\n",
    "\n",
    "        self.model_common = #TODO\n",
    "\n",
    "        # Subxarxa de la funció de Valor\n",
    "        self.fc_layer_inputs = self.feature_size()\n",
    "\n",
    "\n",
    "        self.advantage  = #TODO\n",
    "\n",
    "        # Recordeu adaptar-les a CPU o GPU\n",
    "\n",
    "        # Subxarxa de l'Avantatge A(s,a)\n",
    "        self.value = #TODO\n",
    "\n",
    "        #######\n",
    "        #######################################\n",
    "        ## TODO: Inicialitzar l'optimitzador\n",
    "        self.optimizer = #TODO\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    ##### TODO: Funció forward #############\n",
    "    def forward(self, state):\n",
    "        # Connexió entre capes de la xarxa comuna\n",
    "        common_out = #TODO\n",
    "\n",
    "        # Connexió entre capes de la Subxarxa de Valor\n",
    "        advantage = #TODO\n",
    "\n",
    "        # Connexió entre capes de la Subxarxa d'Avantatge\n",
    "        value = #TODO\n",
    "\n",
    "\n",
    "        ## Agregar les dues subxarxes:\n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        action = #TODO\n",
    "\n",
    "        return action\n",
    "    #######\n",
    "\n",
    "\n",
    "\n",
    "    ### Mètode e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)\n",
    "            action = torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "\n",
    "\n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(self.device)\n",
    "        return self.forward(state_t)\n",
    "\n",
    "    def feature_size(self):\n",
    "        dummy_input = torch.zeros(1, *env.observation_space.shape).to(self.device)\n",
    "        return self.model_common(autograd.Variable(dummy_input)).view(1, -1).size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-QAliFGBBy7"
   },
   "source": [
    "Per al buffer de repetició d'experiències, podem utilitzar exactament la mateixa classe `experienceReplayBuffer` descrita a l'apartat anterior de la DQN.\n",
    "\n",
    "### 3.2 Definició de l'agent\n",
    "\n",
    "La diferència entre la DQN i la dueling DQN es centra, com hem vist, en la definició de l'arquitectura de la xarxa. Però el procés d'aprenentatge i actualització és exactament el mateix. Així, podem recuperar la classe implementada a l'apartat anterior, `DQNAgent()`, i reutilitzar-la aquí sota el nom de `duelingDQNAgent()`. L'únic que haurem de fer és afegir l'optimitzador entre les variables a declarar i adaptar la funció de pèrdua al format funcional de PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXnacQ_e1JTA"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.2 (0.25 pts):</strong> Implementar la classe <code>duelingDQNAgent()</code> com la <code>DQNAgent()</code>\n",
    "<p>\n",
    "</p>\n",
    "De nou, durant el procés s'han d'emmagatzemar (*):\n",
    "<ul>\n",
    "    <li>Les recompenses obtingudes a cada pas de l'entrenament</li>\n",
    "    <li>Les recompenses mitjanes dels 100 episodis anteriors</li>\n",
    "    <li>La pèrdua durant l'entrenament</li>\n",
    "    <li>L'evolució de epsilon al llarg de l'entrenament</li>\n",
    "</ul>\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se us proporciona el codi preimplementat. La implementació que es demana a l'enunciat està indicada als blocs <i>#TODO</i> i/o amb variables igualades a <i>None</i>, excepte (*) que indica en quin moment emmagatzemar les variables esmentades.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0FuC3kM1JTA"
   },
   "outputs": [],
   "source": [
    "class duelingDQNAgent:\n",
    "\n",
    "    def __init__(self, env, main_network, buffer, reward_threshold, epsilon=0.1, eps_decay=0.99, batch_size=32, device=None):\n",
    "        \"\"\"\"\"\n",
    "        Paràmetres\n",
    "        ==========\n",
    "        env: entorn\n",
    "        target_network: classe amb la xarxa neuronal dissenyada\n",
    "        target_network: xarxa objectiu\n",
    "        buffer: classe amb el buffer de repetició d'experiències\n",
    "        epsilon: epsilon\n",
    "        eps_decay: decaïment d'epsilon\n",
    "        batch_size: mida del batch\n",
    "        nblock: bloc dels X últims episodis dels quals es calcularà la mitjana de recompensa\n",
    "        reward_threshold: llindar de recompensa definit en l'entorn\n",
    "        \"\"\"\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        ###############################################################\n",
    "        ##### TODO 1: inicialitzar variables ######\n",
    "        self.env = None  # TODO\n",
    "        self.main_network = None  # TODO\n",
    "        self.target_network = None  # TODO Xarxa objectiu (còpia de la principal)\n",
    "        self.buffer = None  # TODO\n",
    "        self.epsilon = None  # TODO\n",
    "        self.eps_decay = None  # TODO\n",
    "        self.batch_size = None  # TODO\n",
    "        self.nblock = None  # TODO Bloc dels X últims episodis dels quals es calcularà la mitjana de recompensa\n",
    "        self.reward_threshold = reward_threshold  # Llindar de recompensa definit en l'entorn\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    ###############################################################\n",
    "    ##### TODO 2: inicialitzar variables extres que es necessiten ######\n",
    "    def initialize(self):\n",
    "        pass\n",
    "        # TODO\n",
    "\n",
    "    #################################################################################\n",
    "    ###### TODO 3: Prendre nova acció ###############################################\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = None  # TODO Acció aleatòria durant el burn-in\n",
    "        else:\n",
    "           action = None  # TODO Acció basada en el valor de Q (elecció de l'acció amb millor Q)\n",
    "           self.step_count += 1\n",
    "\n",
    "        # TODO: Realització de l'acció i obtenció del nou estat i la recompensa\n",
    "\n",
    "        # TODO: reiniciar entorn 'if done'\n",
    "        if done:\n",
    "            pass  # TODO\n",
    "        return done\n",
    "\n",
    "    ## Entrenament\n",
    "    def train(self, gamma=0.99, max_episodes=50000,\n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000, min_episodios=250):\n",
    "        self.gamma = gamma\n",
    "        # Omplim el buffer amb N experiències aleatòries\n",
    "        print(\"Omplint el buffer de repetició d'experiències...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Entrenant...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while not gamedone:\n",
    "                # L'agent pren una acció\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "\n",
    "                #################################################################################\n",
    "                ##### TODO 4: Actualitzar la xarxa principal segons la freqüència establerta #######\n",
    "\n",
    "                ########################################################################################\n",
    "                ### TODO 6: Sincronitzar xarxa principal i xarxa objectiu segons la freqüència establerta #####\n",
    "\n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    ##################################################################\n",
    "                    ######## TODO: Emmagatzemar epsilon, training rewards i loss #######\n",
    "\n",
    "                    ####\n",
    "                    self.update_loss = []\n",
    "\n",
    "                    #######################################################################################\n",
    "                    ### TODO 7: Calcular la mitjana de recompensa dels últims X episodis i emmagatzemar #####\n",
    "                    mean_rewards = None\n",
    "                    ###\n",
    "\n",
    "                    print(\"\\rEpisodi {:d} Recompenses Mitjanes {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "\n",
    "                    # Comprovar si s'ha arribat al màxim d'episodis\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nLímit d\\'episodis assolit.')\n",
    "                        break\n",
    "\n",
    "                    # Finalitza si la mitjana de recompenses arriba al llindar fixat\n",
    "                    if mean_rewards >= self.reward_threshold and min_episodios < episode:\n",
    "                        training = False\n",
    "                        print('\\nEntorn resolt en {} episodis!'.format(episode))\n",
    "                        break\n",
    "\n",
    "                    #################################################################################\n",
    "                    ###### TODO 8: Actualitzar epsilon ########\n",
    "                    self.epsilon = None\n",
    "\n",
    "    ## Càlcul de la pèrdua\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separem les variables de l'experiència i les convertim a tensors\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_vals = torch.FloatTensor(rewards).to(self.device).reshape(-1, 1)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1, 1).to(self.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(self.device)\n",
    "\n",
    "        # Obtenim els valors de Q de la xarxa principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states).to(self.device), 1, actions_vals)\n",
    "\n",
    "        # Obtenim els valors de Q de la xarxa objectiu\n",
    "        next_actions = torch.max(self.main_network.get_qvals(next_states).to(self.device), dim=-1)[1]\n",
    "        next_actions_vals = next_actions.reshape(-1, 1).to(self.device)\n",
    "        target_qvals = self.target_network.get_qvals(next_states).to(self.device)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_vals).detach()\n",
    "\n",
    "        qvals_next[dones_t.bool()] = 0\n",
    "\n",
    "        # Calculem equació de Bellman\n",
    "        expected_qvals = None\n",
    "        # Funció de pèrdua\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1, 1))\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # Eliminem qualsevol gradient passat\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size)  # Seleccionem un conjunt del buffer\n",
    "        loss = self.calculate_loss(batch)  # Calculem la pèrdua\n",
    "        loss.backward()  # Obtenim els gradients\n",
    "        self.main_network.optimizer.step()  # Apliquem els gradients a la xarxa neuronal\n",
    "        # Emmagatzemem els valors de pèrdua\n",
    "        if self.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhxbHDHLBYYU"
   },
   "source": [
    "### 3.3 Entrenament del Model\n",
    "\n",
    "A continuació, entrenarem el model **dueling DQN** amb els mateixos hiperparàmetres amb què vam entrenar la DQN.\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.3 (0.25):</strong> Carregar el model de xarxa neuronal i entrenar l'agent amb els mateixos hiperparàmetres utilitzats per a la DQN.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzDK0jVGBnyC"
   },
   "outputs": [],
   "source": [
    "# TODO: Temps d'execució 69 minuts a Google Colaboratory amb GPU.\n",
    "# Resultat esperat al voltant de 180-200 punts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlGKaJwyB5ci"
   },
   "source": [
    "### 3.4 Anàlisi de l'Entrenament\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.4 (0.25 pts):</strong> Mostrar els mateixos gràfics que amb la DQN:\n",
    "<ol>\n",
    "    <li>Recompenses obtingudes al llarg de l'entrenament i l'evolució de les recompenses mitjanes cada 100 episodis, juntament amb el llindar de recompensa establert per l'entorn</li>\n",
    "    <li>Pèrdua durant l'entrenament</li>\n",
    "    <li>Evolució de epsilon al llarg de l'entrenament</li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHoR0XY71JTA"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DyLBp4p1JTB"
   },
   "source": [
    "### 3.5 Test de l'Agent\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 3.5 (0.25 pts):</strong> Carregar el model entrenat i executar l'agent entrenat durant 505 episodis consecutius en diferents períodes aleatoris des de l'any 2015 fins al 2024. Calcula la suma de recompenses per cada execució. Per aconseguir aquest punt, executa:\n",
    "<ul>\n",
    "    <li>Un gràfic amb la suma de les recompenses respecte dels episodis, incloent-hi el llindar de recompensa establert.</li>\n",
    "    <li>Emmagatzema la recompensa mitjana obtinguda en les 100 partides a la variable <code>mean_reward_agentduelingDQN</code> i l'última recompensa obtinguda en l'entrenament a <code>mean_reward_agentduelingDQN_last</code>.</li>\n",
    "</ul>\n",
    "A més, realitza l'anàlisi següent amb el model per a l'entorn utilitzat durant l'entrenament:\n",
    "<ul>\n",
    "    <li>Reproduir una partida completa de l'agent entrenat i mostrar el resultat final, incloent-hi el valor total del portafoli al final de l'episodi.</li>\n",
    "    <li>Generar un fitxer CSV que registri els resultats de les interaccions de l'agent amb el mercat en cada episodi i mostrar per pantalla les últimes 30 accions.</li>\n",
    "</ul>\n",
    "<strong>Comenta tots els resultats obtinguts en aquest apartat. A quines conclusions podem arribar? Com podríem millorar l'entrenament i quines implicacions tindria?</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQSAsiD81JTB"
   },
   "outputs": [],
   "source": [
    "file_path = \"stock_trading_agent_ddqn.csv\"\n",
    "env = None  # TODO: Assignar l'entorn entrenat\n",
    "\n",
    "mean_reward_agentduelingDQN = 0  # TODO: Calcular la recompensa mitjana en les 100 partides de test\n",
    "mean_reward_agentduelingDQN_last = 0  # TODO: Assignar la recompensa mitjana de l'última iteració de l'entrenament\n",
    "\n",
    "print(f\"La recompensa mitjana obtinguda per l'agent DQN en les 100 partides de test és: {mean_reward_agentduelingDQN:.2f} punts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETW8_5TL1JTD"
   },
   "source": [
    "\n",
    "## 4. Comparació dels resultats (1.5 pts)\n",
    "\n",
    "Ara compararem els resultats. Si has seguit totes les indicacions, hauràs emmagatzemat mètriques bastant interessants que et permetran interpretar els resultats obtinguts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf3qjvYV1JTD"
   },
   "outputs": [],
   "source": [
    "# Defineix les dades de la taula\n",
    "data = [\n",
    "    [\"DQN\", mean_reward_dqn_last, mean_reward_dqn, time_dqn],\n",
    "    [\"Dueling DQN\", mean_reward_agentduelingDQN_last, mean_reward_agentduelingDQN, time_ddqn],\n",
    "]\n",
    "\n",
    "# Defineix els encapçalaments de la taula\n",
    "headers = [\"Agent\", \"Mitjana Reward d'Entrenament\", \"Mitjana test amb 100 Partides Aleatòries\", \"Temps d'entrenament\"]\n",
    "\n",
    "# Imprimeix la taula\n",
    "table = tabulate(data, headers, tablefmt=\"pipe\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo0jZp-h1JTD"
   },
   "source": [
    "**texto en negrita**<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 4 (1.5 pts):</strong>\n",
    "\n",
    "Comenta els resultats obtinguts. Quin agent ha obtingut millors resultats? Justifica'l\n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsEGnkks1JTD"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentaris:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI1eaU4a1JTD"
   },
   "source": [
    "## 5. Compara els agents en un altre entorn (2 pts)\n",
    "\n",
    "En aquesta part de la PAC, compararem com es desenvolupa cadascun dels agents en un altre entorn diferent implementat per un tercer.\n",
    "\n",
    "Un dels beneficis d'haver utilitzat Gymnasium és que podem utilitzar ràpidament els nostres algorismes en qualsevol entorn que comparteixi la mateixa interfície. Un exemple pot ser [Cart Pole](https://gymnasium.farama.org/environments/classic_control/cart_pole/), un entorn clàssic que consisteix en un carretó sobre el qual s'aguanta una barra vertical. L'objectiu és mantenir la barra en equilibri evitant que caigui, aplicant petites forces al carretó cap a la dreta o cap a l'esquerra. Les úniques accions possibles són aquestes forces, que permeten a l'algorisme aprendre a mantenir l'equilibri de la barra a la posició correcta.\n",
    "\n",
    "![cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
    "\n",
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercici 5 (2 pts):</strong>\n",
    "\n",
    "Executa l'agent DQN i Dueling DQN en el nou entorn. Un cop ho hagis fet, implementa una taula com la mostrada a l'exercici anterior i analitza els resultats. Continua sent el mateix agent el que ha obtingut millor resultat?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Tu93R01JTD"
   },
   "outputs": [],
   "source": [
    "# Configuració d'hiperparàmetres per a DQN.\n",
    "lr = None             # Velocitat d'aprenentatge ajustada per a una millor convergència\n",
    "MEMORY_SIZE = None     # Capacitat de memòria reduïda, suficient per a un entorn senzill com CartPole\n",
    "MAX_EPISODES = 5000     # Nombre màxim d'episodis reduït, ja que CartPole és un problema més simple\n",
    "EPSILON = None             # Valor inicial d'epsilon (alta exploració inicial)\n",
    "EPSILON_DECAY = None    # Decaïment d'epsilon ajustat per a un descens més gradual\n",
    "GAMMA = None           # Factor de descompte gamma\n",
    "BATCH_SIZE = None         # Mida del lot per a l'entrenament\n",
    "BURN_IN = None           # Episodis inicials per omplir el buffer d'experiència abans d'entrenar\n",
    "DNN_UPD = 1             # Freqüència d'actualització de la xarxa neuronal (cada pas)\n",
    "DNN_SYNC = 1000         # Freqüència de sincronització de pesos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFeqn26fpm3g"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(ag, env):\n",
    "    all_rewards = []\n",
    "    # Fem servir tqdm per al bucle d'episodis\n",
    "    for i_episode in tqdm(range(100), desc=\"Progrés d'episodis\"):\n",
    "        # TODO: Implementar el test de l'agent\n",
    "        pass\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "def plot_test(rewards, th):\n",
    "    # TODO: Implementar la visualització dels resultats\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RANwmV4WpuM6"
   },
   "outputs": [],
   "source": [
    "# Utilitza les funcions anteriors per imprimir l'evolució dels agents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMW_Jfvi1JTE"
   },
   "outputs": [],
   "source": [
    "# Mostra les dades dels entrenaments de cada agent.\n",
    "data = [\n",
    "    [\"DQN\", mean_reward_dqn_last, mean_reward_dqn, time_dqn],\n",
    "    [\"Dueling DQN\", mean_reward_agentduelingDQN_last, mean_reward_agentduelingDQN, time_ddqn],\n",
    "]\n",
    "\n",
    "# Defineix els encapçalaments de la taula\n",
    "headers = [\"Agent\", \"Mitjana Reward d'Entrenament\", \"Mitjana test amb 100 Partides Aleatòries\", \"Temps d'entrenament\"]\n",
    "\n",
    "# Imprimeix la taula\n",
    "table = tabulate(data, headers, tablefmt=\"pipe\")\n",
    "print(table)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
