{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80446480",
   "metadata": {
    "id": "80446480"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.983 · Aprenentatge per reforç</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2024-1 · Màster universitari en Ciència de dades (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-bottom: 100px;\">Estudis d'Informàtica, Multimèdia i Telecomunicació</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "# PEC3: Deep Reinforcement Learning (II)\n",
    "\n",
    "L'objectiu d'aquesta pràctica és utilitzar la llibreria [stable baselines 3](https://stable-baselines3.readthedocs.io/en/master/), que ja te implementats algoritmes d'aprenentatge per reforç, per solucionar una versió modificada del cartpole, anomenada cartpole target. Els objectius d'aquesta pràctica són:\n",
    "<ol>\n",
    "    <li>Desenvolupar una capacitat analítica sobre com està funcionant el model.</li>\n",
    "    <li>Familiaritzar-se amb la llibreria stable baselines 3.</li>\n",
    "    <li>Analitzar l'efecte de modificar la funció de recompensa durant l'entrenament de l'agent.</li>\n",
    "    <li>Analitzar l'efecte de modificar l'espai d'accions.</li>\n",
    "    <li>Ser capaços de realitzar una cerca d'hiperparàmetres.</li>\n",
    "    <li>(Opcional) Familiaritzar-se amb l'anàlisi mitjançant tensorboard.</li>\n",
    "</ol>\n",
    "\n",
    "**Important: El lliurament s'ha de fer en format notebook i en format html on es vegi el codi i els resultats i comentaris de cada exercici. Per exportar el notebook a html es pot fer des del menú File → Download as → HTML.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566988c0",
   "metadata": {
    "id": "566988c0"
   },
   "source": [
    "# 0. Cartpole target (2 punts)\n",
    "\n",
    "En aquesta pràctica utilitzarem un entorn anomenat cartpole random target. Aquest entorn està fora de la llibreria gymnasium i és una modificació de l'entorn [cartpole](https://gymnasium.farama.org/environments/classic_control/cart_pole/). En aquest entorn, l'objectiu no només és mantenir el pal estable, sinó que el carro ha d'estar el més a prop possible d'un punt objectiu, el qual es pot observar marcat en verd quan es renderitza l'entorn:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Hi ha alguns paràmetres en l'entorn que permeten regular com de rellevant volem que sigui el punt objectiu, fins al punt que podem aconseguir que aquesta modificació es comporti com el cartpole original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b65a1",
   "metadata": {
    "id": "a53b65a1"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 0.1 (0.75 punts):</strong> Llegeix el codi de l'entorn que es proporciona a continuació detingudament. Ignoreu tots els TODOS, són coses que haurem d'implementar a mesura que la pràctica avanci. Després, responeu les següents preguntes:\n",
    "    <ul>\n",
    "      <li>Quin és l'espai d'observacions? Com es distingeix respecte a la versió bàsica de cartpole?</li>\n",
    "      <li>Quines dues versions de la reward existeixen en l'entorn?</li>\n",
    "      <li>Per al cas en que la reward és custom, què fa el paràmetre target_desire_factor?</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc977cd",
   "metadata": {
    "id": "9bc977cd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "\n",
    "\n",
    "class CartPoleEnvRandomTarget(gym.Env[np.ndarray, Union[int, np.ndarray]]):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    This environment corresponds to a modification of the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "     The modification consists on a target appearing into scene. The cart must be as close to the target as posible\n",
    "\n",
    "    For more details about original cartpole look for source code in gymnasium.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_desire_factor: float = 0.5,\n",
    "        reward_function: str = \"default\",\n",
    "        is_eval=False,\n",
    "        increased_actions=False,\n",
    "        render_mode: Optional[str] = None,\n",
    "    ):\n",
    "\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "\n",
    "        self.max_steps = 500\n",
    "        self.steps = 0\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "        self.target_threshold = 2\n",
    "\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.inf,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.inf,\n",
    "                self.target_threshold * 2,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.increased_actions = increased_actions\n",
    "        if self.increased_actions:\n",
    "            #TODO 3.1: Ampliar l'espai d'observacions\n",
    "            raise NotImplementedError(\"Increased actions should be implemented\")\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state: np.ndarray | None = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        #TODO 1.3: Tenir en compte si l'entorn és d'avaluació\n",
    "        self.is_eval = is_eval\n",
    "        self.target_position = self.generate_random_target_position()\n",
    "\n",
    "        self.target_desire_factor = min(\n",
    "            max(target_desire_factor, 0), 1\n",
    "        )  # between 0 and 1\n",
    "        if reward_function in [\"default\", \"custom\"]:\n",
    "            self.reward_function = reward_function\n",
    "        else:\n",
    "            raise AttributeError(\"reward function must be either default or custom\")\n",
    "\n",
    "    def generate_random_target_position(\n",
    "        self,\n",
    "    ):\n",
    "        if self.is_eval:\n",
    "            #TODO 1.3:Tingueu en compte si l'entorn és d'avaluació per situar el target en unes posicions concretes.\n",
    "            raise NotImplementedError(\"You should implement is_eval casuistic\")\n",
    "            return target_position\n",
    "        else:\n",
    "            return np.random.uniform(-self.x_threshold, self.x_threshold)\n",
    "\n",
    "    def custom_reward(self, target_position, current_position, angle, terminated):\n",
    "        if self.reward_function == \"default\":\n",
    "            #Reward default del cartpole\n",
    "            return 1 if not terminated else 0\n",
    "        else:\n",
    "            angle_reward = (\n",
    "                -abs(angle) / (2.0 * self.x_threshold) / self.theta_threshold_radians\n",
    "            )\n",
    "            target_reward = -(abs(target_position - current_position) ** 2) / (\n",
    "                (2 * self.x_threshold) ** 2\n",
    "            )\n",
    "            return (\n",
    "                1\n",
    "                + self.target_desire_factor * target_reward\n",
    "                + (1 - self.target_desire_factor) * angle_reward\n",
    "            )\n",
    "        #TODO 2.2: Implementar 2 funcions de reward extra\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(\n",
    "            action\n",
    "        ), f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        x, x_dot, theta, theta_dot, _ = self.state\n",
    "        if self.increased_actions:\n",
    "            force_factor = action - 3\n",
    "            force = (\n",
    "                force_factor / 3 * self.force_mag\n",
    "                if force_factor > 0\n",
    "                else (force_factor - 1) / 3 * self.force_mag\n",
    "            )\n",
    "        else:\n",
    "            force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * np.square(theta_dot) * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length\n",
    "            * (4.0 / 3.0 - self.masspole * np.square(costheta) / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "\n",
    "        self.state = np.array(\n",
    "            (x, x_dot, theta, theta_dot, self.target_position), dtype=np.float64\n",
    "        ) #Aquest estat es diferent respecte al del cartpole original\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        reward = self.custom_reward(self.target_position, x, theta, terminated)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, truncated, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = utils.maybe_parse_reset_bounds(\n",
    "            options, -0.05, 0.05  # default low\n",
    "        )  # default high\n",
    "\n",
    "        self.state = self.np_random.uniform(low=low, high=high, size=(4,)).tolist()\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        #Generar posició aleatòria del target i guardar en l'estat\n",
    "        self.target_position = self.generate_random_target_position()\n",
    "        self.state.append(self.target_position)\n",
    "\n",
    "        self.state = np.array(self.state)\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            assert self.spec is not None\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                'pygame is not installed, run `pip install \"gymnasium[classic-control]\"`'\n",
    "            ) from e\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode == \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "        targetwidth = 10\n",
    "        targetheight = 10\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -targetwidth / 2,\n",
    "            targetwidth / 2,\n",
    "            targetheight / 2,\n",
    "            -targetheight / 2,\n",
    "        )\n",
    "        axleoffset = targetheight / 4.0\n",
    "        targetx = x[-1] * scale + self.screen_width / 2.0  # MIDDLE OF target\n",
    "        targety = 90  # TOP OF target\n",
    "        target_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        target_coords = [(c[0] + targetx, c[1] + targety) for c in target_coords]\n",
    "        gfxdraw.aapolygon(self.surf, target_coords, (10, 255, 10))\n",
    "        gfxdraw.filled_polygon(self.surf, target_coords, (10, 255, 10))\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1e339",
   "metadata": {
    "id": "cdd1e339"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "    \n",
    "1.**Espai d'observacions**: \n",
    "   \n",
    "L'espai d'observacions es tracta d'una caixa (Box) amb els límits següents:\n",
    "    \n",
    "* Posició del carro: entre -4.8 i 4.8.\n",
    "* Velocitat del carro: entre -infinit i infinit.\n",
    "* Angle del pal: entre -24º i 24º (radians).\n",
    "* Velocitat angular del pal: entre -infinit i infinit.\n",
    "* Posició del target: entre -4.8 i 4.8.\n",
    "    \n",
    "Definició, creació i assiganció dels límits de l'espai d'observació:\n",
    "    \n",
    "```\n",
    "#Definition    \n",
    "high = np.array(\n",
    "    [\n",
    "        self.x_threshold * 2,  # Límit per la posició del carro\n",
    "        np.inf,                # Límit per la velocitat del carro\n",
    "        self.theta_threshold_radians * 2,  # Límit per l'angle del pal\n",
    "        np.inf,                # Límit per la velocitat angular del pal\n",
    "        self.target_threshold * 2,  # Límit per la posició del target\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "#Creation\n",
    "self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "    \n",
    "#Assignment\n",
    "self.state = np.array(\n",
    "    (x, x_dot, theta, theta_dot, self.target_position), dtype=np.float64\n",
    ")\n",
    "    \n",
    "```\n",
    "    \n",
    "Aquest espai d'observacions es distingeix de la versió bàsica de 'cartpole' perquè inclou la posició del target com a una dimensió addicional. La versió original inclou 4 elements: posició del carro, velocitat del carro, angle del pal i velocitat del pal. \n",
    "    \n",
    "2.**Reward**:\n",
    "    \n",
    "Hi ha dues versions de la funció de recompensa en aquest entorn:\n",
    "    \n",
    "* _Default_ : Recompensa similar a la de la versió bàsica del 'cartpole'. La recompensa és 1 per a cada pas que el pal es manté dret i no ha acabat l'episodi.\n",
    "    \n",
    "```\n",
    "if self.reward_function == \"default\":\n",
    "    return 1 if not terminated else 0\n",
    "```\n",
    "\n",
    "* _Custom_ : La recompensa es calcula considerant la proximitat al target i l'angle del pal. Es calcula de la següent manera: \n",
    "\n",
    "```\n",
    "angle_reward = (\n",
    "    -abs(angle) / (2.0 * self.x_threshold) / self.theta_threshold_radians\n",
    ")\n",
    "target_reward = -(abs(target_position - current_position) ** 2) / (\n",
    "    (2 * self.x_threshold) ** 2\n",
    ")\n",
    "return (\n",
    "    1\n",
    "    + self.target_desire_factor * target_reward\n",
    "    + (1 - self.target_desire_factor) * angle_reward\n",
    ")\n",
    "```    \n",
    "    \n",
    "Quan ens trobem en el cas de la recompensa custom, el paràmetre 'target_desire_factor' determina el pes relatiu entre la proximitat al target i l'angle del pal. Un valor més alt fa que la recompensa depengui més de la proximitat al target, mentre que un valor més baix fa que la recompensa depengui més de l'angle del pal. \n",
    "  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c5639",
   "metadata": {
    "id": "e01c5639"
   },
   "source": [
    "<strong>IMPORTANT: PER ALS APARTATS 1 I 2 FAREM SERVIR LA REWARD DEFAULT.</strong> Això implica que el nostre entorn serà el mateix que el del cartpole clàssic sense target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb1a87",
   "metadata": {
    "id": "bccb1a87"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 0.2 (0.75 punts):</strong> Executa l'entorn cartpole target utilitzant una policy random. Desa les observacions en una llista i posteriorment crea una funció per analitzar els resultats on es vegi:\n",
    "    <ul>\n",
    "    <li>Un scatter plot dels angles del pal així com la posició relativa del carro respecte al target. Heu de marcar el punt inicial i final. De manera opcional, podeu marcar cada punt de forma diferent segons l'acció presa (empujar a l'esquerra o a la dreta). Per exemple, una creu si es prem a la dreta i un cercle si es fa cap a l'esquerra).</li>\n",
    "    <li>L'evolució de la reward.</li>\n",
    "    <li>Imprimir el número d'episodis realitzats.</li>\n",
    "    </ul>\n",
    "Comenta els resultats trobats.\n",
    "\n",
    "<strong>NOTA: Per poder veure com l'agent va movent el Pal de manera interactiva, podeu utilitzar render_mode = 'human'. Això NO s'ha de fer quan s'entrenin els models, ja que seran molt lents d'entrenar.\n",
    "    \n",
    "No és obligatori utilitzar render_mode = 'human' per a les avaluacions, però pot ser interessant. En cas de voler-ho fer, recordeu que cal executar-ho localment.</strong>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277ea62",
   "metadata": {
    "id": "7277ea62"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9a2fe",
   "metadata": {
    "id": "a6f9a2fe"
   },
   "outputs": [],
   "source": [
    "#TODO: Completar codi\n",
    "env = CartPoleEnvRandomTarget(render_mode='human',reward_function = 'default')\n",
    "\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # TODO: Escollir accio aleatoria\n",
    "\n",
    "    # TODO: Executar accio i esperar resposta de l'entorn\n",
    "\n",
    "    # TODO: Guardar informacio necesaria per a poder fer les grafiques despres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0d97e",
   "metadata": {
    "id": "4cb0d97e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "X_THRESHOLD = env.unwrapped.x_threshold*1.25 #Limit de posicio del carro\n",
    "ANGLE_THRESHOLD = env.unwrapped.theta_threshold_radians*1.25 #Limit de l'angle del pal\n",
    "def analyze_results(**kwargs):\n",
    "    #TODO: Completar codi\n",
    "\n",
    "    plt.plot()\n",
    "    plt.xlabel('Distance to target')\n",
    "    plt.ylabel('Angle')\n",
    "    plt.ylim(-ANGLE_THRESHOLD,ANGLE_THRESHOLD)\n",
    "    plt.xlim(-X_THRESHOLD,X_THRESHOLD)\n",
    "    plt.title('Distance-angle trajectory')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot()\n",
    "    plt.ylabel('Angle')\n",
    "    plt.ylabel('Episode')\n",
    "    plt.show()\n",
    "\n",
    "    print (f'Episode length: {}')\n",
    "\n",
    "    return\n",
    "\n",
    "#TODO: passar inputs necessaris\n",
    "analyze_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8b2e0",
   "metadata": {
    "id": "a0c8b2e0"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d930e7",
   "metadata": {
    "id": "37d930e7"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 0.3 (0.5 punts):</strong> Respecte al gràfic realitzat a l'apartat anterior, quina seria la trajectòria que realitzaria un agent que es comportés de manera òptima?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0daee17",
   "metadata": {
    "id": "c0daee17"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869af4e",
   "metadata": {
    "id": "5869af4e"
   },
   "source": [
    "# 1. Stable baselines 3 (2 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c5fd9",
   "metadata": {
    "id": "725c5fd9"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    Comencem a entrenar un model per solucionar l'entorn. Recordeu que per als apartats 1 i 2 utilitzarem la reward default.\n",
    "    Per solucionar l'entorn, utilitzarem el model A2C de la llibreria stable baselines.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61b2d8",
   "metadata": {
    "id": "fc61b2d8"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "STEPS = 20_000\n",
    "EVAL_FREQ = 2000\n",
    "EVAL_EPISODES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436a89c",
   "metadata": {
    "id": "9436a89c"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 1.1 (0.25 punts):</strong> Entrena un model A2C amb paràmetres default i utilitzant una MlpPolicy per entrenar un agent que solucioni el cartpole amb reward default.\n",
    "    Afegeix un callback per avaluar el model 5 vegades cada 2 mil iteracions.\n",
    "    A més, fes que cada vegada que es trobi un model millor, es guardi el model. Inicialment, abans de començar l'entrenament, utilitza la funció evaluate_policy per avaluar el model sense entrenar. Utilitza també la funció evaluate_policy just abans d'entrenar el model per tenir una idea de com funciona el model abans de ser entrenat.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16626c6b",
   "metadata": {
    "id": "16626c6b"
   },
   "source": [
    "Opcional: Per a aquells que vulgueu aprofundir una mica més en l'assignatura, també existeix una eina anomenada TensorBoard, molt útil per analitzar el procés d'entrenament de qualsevol xarxa neuronal en general. Stable Baselines 3 té automatitzada la captura de logs amb TensorBoard. Per fer-ho, només cal que passeu al constructor del model el paràmetre tensorboard_log=path on vulgueu guardar el log. Podeu utilitzar aquests logs per veure no només les rewards d'evaluació, sinó també per veure els gradients, la loss i altres paràmetres durant l'entrenament.\n",
    "\n",
    "Podeu utilitzar-lo en aquest apartat i en tots els que vulgueu durant la pràctica. No puntuïarà ni es tindrà en compte, però permet conèixer més en detall com s'està entrenant el model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f6e293",
   "metadata": {
    "id": "35f6e293"
   },
   "outputs": [],
   "source": [
    "env = CartPoleEnvRandomTarget(render_mode=None, reward_function = 'default')\n",
    "\n",
    "callback = #carregar callback\n",
    "\n",
    "model = #carregar model\n",
    "\n",
    "initial_eval = #avaluació inicial\n",
    "\n",
    "#TODO: entrenar model\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c3bf6",
   "metadata": {
    "id": "932c3bf6"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 1.2 (0.25 punts):</strong> Carrega el millor model trobat i executa la funció evaluate_policy. Compara les rewards inicials i finals.\n",
    "    Executa un episodi i analitza els resultats utilitzant la funció prèviament creada per visualitzar el comportament de l'agent en l'espai d'angle-posició.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ed115",
   "metadata": {
    "id": "2f9ed115"
   },
   "outputs": [],
   "source": [
    "model = #carregar el millor model\n",
    "final_eval = #evaluacio final\n",
    "print (f'Initial evaluation: {initial_eval}')\n",
    "print (f'Best model evaluation: {final_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301433a",
   "metadata": {
    "id": "1301433a"
   },
   "outputs": [],
   "source": [
    "#TODO: Completar funcio d'avaluació del model entrenat\n",
    "def evaluate_trained_model(env,model):\n",
    "    #Primer executem un cop l'entorn\n",
    "    obs,_ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # TODO: Escollir accio en base al model\n",
    "\n",
    "        # TODO: Executar accio i esperar resposta de l'entorn\n",
    "\n",
    "        # TODO: Guardar informacio necesaria per a poder fer les grafiques despres\n",
    "\n",
    "    #Analitzem resultats\n",
    "    analyze_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca3dee",
   "metadata": {
    "id": "99ca3dee"
   },
   "outputs": [],
   "source": [
    "env = CartPoleEnvRandomTarget(render_mode='human',reward_function = 'default') #podeis hacer render mode = None\n",
    "evaluate_trained_model(env,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2bce55",
   "metadata": {
    "id": "2a2bce55"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd105f1d",
   "metadata": {
    "id": "dd105f1d"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 1.3 (0.5 punts):</strong> Un dels problemes que tenim amb aquest entorn és que estem avaluant el model sobre un entorn random. Això genera que durant el callback puguem guardar un model no perquè sigui millor que els anteriors, sinó perquè els entorns sobre els quals s'ha avaluat són més senzills. Per exemple, si en un dels callbacks el target es situa als extrems, tot i que l'agent es comporti millor que les seves versions anteriors, potser no ho sabrem perquè la reward total serà més petita que, per exemple, una versió amb un comportament pitjor, però en què el target s'ha situat al centre.\n",
    "    Modifica l'entorn de cartpole (veure TODOS dins del codi) perquè en avaluació el target es trobi en 9 posicions diferents:\n",
    "    <ul>\n",
    "    <li> Als extrems</li>\n",
    "    <li> A 3/4, 1/2 i 1/4 de la distància entre el centre i els extrems</li>\n",
    "    <li> Just al centre</li>\n",
    "    </ul>\n",
    "    Setejarem a partir d'ara el número d'episodis d'avaluació a 18 (EVAL_EPISODES) perquè s'executi 2 vegades cada casuística.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335fc2bd",
   "metadata": {
    "id": "335fc2bd"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 1.4.1 (0.5 punts):</strong> Modifica la funció analyze_results perquè creï els mateixos gràfics que es generaven abans però per a cada casuística. Suposa que a la funció li arribaran 9 episodis diferents, 1 per a cada casuística. Podeu utilitzar subplots de matplotlib. Us hauria de quedar alguna cosa així:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86996fbf",
   "metadata": {
    "id": "86996fbf"
   },
   "source": [
    "![image-3.png](exemple_sortida.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb549f",
   "metadata": {
    "id": "89cb549f"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 1.4.2 (0.5 punts):</strong> Repeteix els exercicis 1.1 i 1.2 però avaluant amb un entorn d'avaluació i utilitzant la nova funció d'avaluació. Recorda que tot i que els EVAL_EPISODES són 18, per avaluar l'agent i fer els gràfics executarem 1 vegada cada casuística, fent un total de 9 execucions.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dc446",
   "metadata": {
    "id": "dc5dc446"
   },
   "outputs": [],
   "source": [
    "#Modificar primer els TODOS de l'entorn\n",
    "EVAL_EPISODES = 18\n",
    "env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'default')\n",
    "eval_env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'default',is_eval = True)\n",
    "\n",
    "#TODO: Completar codi\n",
    "#Creem callback i model. Per al callback farem servir l'entorn d'evaluacio, per al model el normal\n",
    "callback = ?\n",
    "\n",
    "model = ?\n",
    "\n",
    "#Executem evaluacio inicial\n",
    "initial_eval = ?\n",
    "\n",
    "#Entrenem model\n",
    "model.\n",
    "\n",
    "#Carreguem millor model i executem validacio final\n",
    "model = ?\n",
    "final_eval = ?\n",
    "print (f'Initial evaluation: {initial_eval}')\n",
    "print (f'Best model evaluation: {final_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a521ba0",
   "metadata": {
    "id": "5a521ba0"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_results():\n",
    "    #TODO: Completar nova funció que genera un subplot amb una gràfica per test case\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_trained_model(env,model):\n",
    "    #TODO: Completar funció\n",
    "\n",
    "    obs,_ = env.reset()\n",
    "    #Fem un for loop per tots els tests cases i guardem resultats\n",
    "    n_eval_cases = 9\n",
    "\n",
    "    for _ in tqdm(range(n_eval_cases)):\n",
    "        done = False\n",
    "        #Executar el primer test case i guardar\n",
    "        while not done:\n",
    "            pass #TODO: Completar codi\n",
    "        obs,_ = env.reset()\n",
    "\n",
    "    #Executem la funció analyze results\n",
    "    analyze_results(**kwargs)\n",
    "\n",
    "#Executem la funcio\n",
    "eval_env = CartPoleEnvRandomTarget(render_mode='human',reward_function = 'default',is_eval = True) #podeis modificar el render mode a None\n",
    "evaluate_trained_model(eval_env,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b38ad0",
   "metadata": {
    "id": "31b38ad0"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6e7f5",
   "metadata": {
    "id": "a2f6e7f5"
   },
   "source": [
    "# 2. Efecte del reward (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30c9c7",
   "metadata": {
    "id": "bf30c9c7"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 2.1 (1.25 punts):</strong> Entrena un agent utilitzant diferents valors del paràmetre target_desire_factor:\n",
    "    <ul>\n",
    "        <li>target_desire_factor=0</li>\n",
    "        <li>target_desire_factor=0.5</li>\n",
    "        <li>target_desire_factor=1</li>\n",
    "    </ul>\n",
    "\n",
    "Executa un episodi amb el millor agent (recorda utilitzar els callbacks) i comenta els resultats obtinguts.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b6e3e",
   "metadata": {
    "id": "a30b6e3e"
   },
   "outputs": [],
   "source": [
    "env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom',target_desire_factor=)#Modificar target_desire_factor\n",
    "eval_env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom',target_desire_factor=,is_eval = True)#Modificar target_desire_factor\n",
    "\n",
    "#TODO: Repetir el mateix que en els exercicis anteriors. Callback, model, avaluació inicial, càrrega del millor model i entrenament.\n",
    "#Al final, analitzar els resultats utilitzant la funció evaluate_trained_model.\n",
    "#Repetir per a cada valor de target_desire_factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593dc01",
   "metadata": {
    "id": "b593dc01"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a667e",
   "metadata": {
    "id": "924a667e"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 2.2 (1.25 punts):</strong> Fixant ara target_desire_factor = 1, proposa 2 modificacions de la funció de recompensa únicament de la component que té a veure amb el target i comenta els resultats obtinguts. Utilitza la mateixa estructura que fins ara (callback, model, avaluació inicial...).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0a613",
   "metadata": {
    "id": "35e0a613"
   },
   "outputs": [],
   "source": [
    "env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom_1',target_desire_factor=0)\n",
    "eval_env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom_1',target_desire_factor=0,is_eval = True)\n",
    "\n",
    "#TODO: Repetir el mateix que en els exercicis anteriors. Callback, model, avaluació inicial, càrrega del millor model i entrenament.\n",
    "#Al final, analitzar els resultats.\n",
    "#Repetir per a les dues noves funcions d'error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd70054",
   "metadata": {
    "id": "4bd70054"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea2904d",
   "metadata": {
    "id": "6ea2904d"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 2.3 (0.5 punts):</strong> A nivell teòric i suposant un temps d'entrenament més llarg amb uns hiperparàmetres més òptims, podríem arribar a esperar mai d'un agent amb una alpha = 1 (només es fixa en estar a prop del target) que aprengui a mantenir el pal en equilibri?\n",
    "    Sabries trobar una equivalència amb el cas sense el target?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51304663",
   "metadata": {
    "id": "51304663"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b54ab",
   "metadata": {
    "id": "494b54ab"
   },
   "source": [
    "# 3. Increment de l'espai d'accions (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800e7dc",
   "metadata": {
    "id": "2800e7dc"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 3.1 (0.5 punts):</strong> Modifica el codi de tal manera que ara existeixin 6 accions en comptes de 2:\n",
    "    <ul>\n",
    "    <li>Accions 6,5,4, fer un 100%, 66% i 33% de força positiva respectivament</li>\n",
    "    <li>Accions 1,2,3, fer un 100%, 66% i 33% de força negativa respectivament</li>\n",
    "    </ul>\n",
    "\n",
    "<strong>NOTA: A partir d'ara i fins al final de la pràctica es farà servir la recompensa custom que ja venia implementada amb target_desire_factor = 1</strong>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dfe76b",
   "metadata": {
    "id": "b4dfe76b"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 3.2 (1 punt):</strong> Executa un entrenament i analitza els resultats. Per què han anat millor/pitjor? És el que esperaves?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4007eda",
   "metadata": {
    "id": "c4007eda"
   },
   "outputs": [],
   "source": [
    "#implementar abans els TODO de l'entorn\n",
    "env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom',increased_actions = True,target_desire_factor=1)\n",
    "eval_env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom',increased_actions = True,target_desire_factor=1,is_eval = True)\n",
    "\n",
    "#TODO: Repetir el mateix que en els exercicis anteriors. Callback, model, avaluació inicial, càrrega del millor model i entrenament.\n",
    "#Al final, analitzar els resultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931441a8",
   "metadata": {
    "id": "931441a8"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ed916",
   "metadata": {
    "id": "b78ed916"
   },
   "source": [
    "# 4. Cerca d'hiperparàmetres (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce738cf3",
   "metadata": {
    "id": "ce738cf3"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 4.1 (1 punt):</strong> Utilitzant l'entorn amb desire_factor = 1 i amb les accions per defecte (no ampliades), busca els hiperparàmetres del model perquè aquest aprengui en menys de 10 mil passos. Us donem algunes pistes sobre el rang de valors que val la pena explorar:\n",
    "    <ul>\n",
    "    <li>gamma = (0.9,0.999). Es recomana cerca en escala logarítmica</li>\n",
    "    <li>max_grad_norm = (0.3,5.0). Es recomana cerca en escala logarítmica</li>\n",
    "    <li>n_steps = (8,32)</li>\n",
    "    <li>learning_rate = (1e-5,1e-1). Es recomana cerca en escala logarítmica</li>\n",
    "    <li>ent_coef = (1e-8,1e-3). Es recomana cerca en escala logarítmica</li>\n",
    "    </ul>\n",
    "    <strong>Important:</strong> La cerca d'hiperparàmetres s'ha de poder fer en 100 passos, és a dir, només caldria entrenar el model amb 100 combinacions diferents d'hiperparàmetres, res més.\n",
    "    \n",
    "Explica l'estratègia de cerca que s'ha seguit. Si s'utilitza una estratègia dummy tipus grid search, la puntuació màxima serà de 0.5/1 punts. Per aconseguir la màxima puntuació, podeu utilitzar frameworks com [optuna](https://optuna.org/#code_examples) per fer l'optimització d'hiperparàmetres.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d22f28",
   "metadata": {
    "id": "34d22f28"
   },
   "outputs": [],
   "source": [
    "STEPS = 10_000 #BAIXEM ELS STEPS A 10_000 per fer la cerca més ràpida\n",
    "EVAL_EPISODES = 18\n",
    "EVAL_FREQ = 2_000\n",
    "N_TRIALS = 100  #Només farem 100 trials amb diferents combinacions d'hiperparàmetres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416832b",
   "metadata": {
    "id": "c416832b"
   },
   "outputs": [],
   "source": [
    "eval_env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom',increased_actions = False,target_desire_factor=1,is_eval = True)\n",
    "\n",
    "#TODO: Cerca hiperparàmetres\n",
    "\n",
    "print ('Best hiperparams:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea5189",
   "metadata": {
    "id": "14ea5189"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473f08d",
   "metadata": {
    "id": "f473f08d"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Exercici 4.2 (0.5 punts):</strong> Ara torna a entrenar un model durant 30_000 passos i analitza els resultats obtinguts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd8353",
   "metadata": {
    "id": "16fd8353"
   },
   "outputs": [],
   "source": [
    "STEPS = 30_000\n",
    "env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom',increased_actions = False,target_desire_factor=1)\n",
    "eval_env = CartPoleEnvRandomTarget(render_mode=None,reward_function = 'custom',increased_actions = False,target_desire_factor=1,is_eval = True)\n",
    "\n",
    "#TODO: Repetir el mateix que en els exercicis anteriors. Callback, model, avaluació inicial, càrrega del millor model i entrenament.\n",
    "#Recordeu fer servir els millors hiperparametres trobats\n",
    "#Al final, analitzar els resultats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035eb52",
   "metadata": {
    "id": "c035eb52"
   },
   "source": [
    "<div style=\"background-color: #fcf2f2; border-color: #dfb5b4; border-left: 5px solid #dfb5b4; padding: 0.5em;\">\n",
    "<strong>Comentari:</strong>\n",
    "<br><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1qV4sU9G99WbznjJiP2aac1G-wyu44YDD",
     "timestamp": 1733959058406
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
